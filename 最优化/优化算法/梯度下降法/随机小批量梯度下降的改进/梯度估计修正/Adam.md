Adam 算法（Adaptive Moment Estimation Algorithm）\[Kingma et al., 2015]可以看作 Momentum 和 RMSprop  的结合， 不但使用动量作为参数更新方向， 而且可以自适应调整学习率 	
惯性保持+环境感知:
一阶矩和二阶矩融合. 分别记录梯度的一阶矩(过往梯度与当前梯度的平均, 体现惯性保持)和二阶矩(过往梯度平方与当前梯度平方的平均, 体现环境感知), 为不同参数产生自适应的学习率
类似滑动窗口内求平均, 当前梯度和近一段时间内梯度的平均值, 时间久远的梯度对当前均值的贡献指数衰减
## 计算公式
`m(t)=β₁m(t-1)+(1-β₁)g(t);
v(t)=β₂v(t-1)+(1-β₂)g²(t)`
其中, g(t):  梯度; β₁,β₂:  衰减系数; m(t):  一阶矩, 相当于估计E\[g(t)], 当下梯度g(t)由于是随机采样得到的估计结果, 因此更关注其在统计意义上的期望;v(t):  二阶矩, 相当于估计E\[g²(t)], 与AdaGrad不同, 不是g²(t)从开始的加和
## 更新公式
`θ(t)=θ(t-1)-ηM/√(V+ε);
M=m/(1-β₁);
V=v/(1-β₂)`
## 物理意义
当||m||大且v大时, 梯度大且稳定: 遇到一个明显的大坡前进方向明确; 
当||m||趋于0且v大时, 梯度不稳定: 遇到一个峡谷, 容易引起反弹震荡
当||m||大且v趋于0时: 这种情况不可能出现
当||m||趋于0且v趋于0时, 梯度趋于0: 可能抵达局部最低点, 可能走到一片坡度极缓的平台