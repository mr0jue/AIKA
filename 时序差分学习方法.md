#时序差分学习 （Temporal-Difference Learning）方法是 #蒙特卡罗方法 的一种改进，通过引入 #动态规划算法 来提高学习效率 [Sutton et al.,2018]． #时序差分学习方法 是模拟一段轨迹，每行动一步 (或者几步)，就利用 #贝尔曼方程 来评估行动前状态的价值．
当时序差分学习方法中每次更新的动作数为最大步数时，就等价于蒙特卡罗方法．

首先, 将蒙特卡罗方法中 ${\mathrm{Q}}$ 函数 ${\hat{Q}^{\pi}(s, a)}$ 的估计改为增量计算的方式, 假设第 ${N}$ 次试验后值函数 ${\hat{Q}_{N}^{\pi}(s, a)}$ 的平均为$${\begin{align} 
\hat{Q}_{N}^{\pi}(s, a) 
&=\frac{1}{N} \sum_{n=1}^{N} G\left(\tau_{s_{0}=s, a_{0}=a}^{(n)}\right) \\
&=\frac{1}{N}\left(G\left(\tau_{s_{0}=s, a_{0}=a}^{(N)}+\sum_{n=1}^{N-1} G\left(\tau_{s_{0}=s, a_{0}=a}^{(n)}\right)\right)\right.\\
&=\frac{1}{N}\left(G\left(\tau_{s_{0}=s, a_{0}=a}^{(N)}+(N-1) \hat{Q}_{N-1}^{\pi}(s, a)\right)\right.\\ 
&=\hat{Q}_{N-1}^{\pi}(s, a)+\frac{1}{N}\left(G\left(\tau_{s_{0}=s, a_{0}=a}^{(N)}-\hat{Q}_{N-1}^{\pi}(s, a)\right),\right. 
\end{align}}$$ 其中 ${\tau_{s_{0}=s, a_{0}=a}}$ 表示轨迹 ${\tau}$ 的起始状态和动作为 ${s, a}$. 值函数 ${\hat{Q}^{\pi}(s, a)}$ 在第 ${N}$ 试验后的平均等于第 ${N-1}$ 试验后的平均加上一个增量. 更一般性地, 我们将权重系数 ${\frac{1}{N}}$ 改为一个比较小的正数 ${\alpha}$. 这样每次采用一个新的轨迹 ${\tau_{s_{0}=s, a_{0}=a}}$, 就可以更新 ${\hat{Q}^{\pi}(s, a)}$. $${ \hat{Q}^{\pi}(s, a) \leftarrow \hat{Q}^{\pi}(s, a)+\alpha\left(G\left(\tau_{s_{0}=s, a_{0}=a}\right)-\hat{Q}^{\pi}(s, a)\right), }$$其中, ${G\left(\tau_{s_{0}=s, a_{0}=a}\right)}$ 为一次试验的完整轨迹所得到的总回报. 其中增量$$\delta \triangleq G\left(\tau_{s_{0}=s, a_{0}=a}\right)-\hat{Q}^{\pi}(s, a)$$ 称为 #蒙特卡罗误差, 表示当前轨迹的 #真实回报 ${G\left(\tau_{s_{0}=s, a_{0}=a}\right)}$ 与 #期望回报 ${\hat{Q}^{\pi}(s, a)}$ 之间的差距. 为了提高效率, 可以借助 #动态规划 的方法来计算 ${G\left(\tau_{s_{0}=s, a_{0}=a}\right)}$, 而不需要得到完整的轨迹. 从 ${s, a}$ 开始, 采样下一步的状态和动作 ${\left(s{\prime}, a{\prime}\right)}$, 并得到奖励 ${r\left(s, a, s{\prime}\right)}$, 然后利用 #贝尔曼方程 来近似估计 ${G\left(\tau_{s_{0}=s, a_{0}=a}\right)}$, $${ G\left(\tau_{s_{0}=s, a_{0}=a, s_{1}=s{\prime}, a_{1}=a{\prime}}\right) =r\left(s, a, s{\prime}\right)+\gamma G\left(\tau_{s_{0}=s{\prime}, a_{0}=a{\prime}}\right) \\ \approx r\left(s, a, s{\prime}\right)+\gamma \hat{Q}^{\pi}\left(s{\prime}, a{\prime}\right), }$$ 其中 ${\hat{Q}^{\pi}\left(s{\prime}, a{\prime}\right)}$ 是当前的 ${\mathrm{Q}}$ 函数的近似估计. 结合上述公式, 有 $${ \hat{Q}^{\pi}(s, a) \leftarrow \hat{Q}^{\pi}(s, a)+\alpha\left(r\left(s, a, s{\prime}\right)+\gamma \hat{Q}^{\pi}\left(s{\prime}, a{\prime}\right)-\hat{Q}^{\pi}(s, a)\right), }$$ 因此, 更新 ${\hat{Q}^{\pi}(s, a)}$ 只需要知道当前状态 ${s}$ 和动作 ${a}$ 、奖励 ${r\left(s, a, s{\prime}\right)}$ 、下一步的状 态 ${s{\prime}}$ 和动作 ${a{\prime}}$. 

这种策略学习方法称为 #SARSA 算法 ( State Action Reward State Action, SARSA ) [Rummery et al., 1994].
$$\begin{array}{ll} 
\hline 
& \text{SARSA}\\
\hline 
& \text{输入: 状态空间 } \mathcal{S}, \text{ 动作空间 } \mathcal{A}, \text{ 折扣率 }  {\gamma}, \text{ 学习率 } \alpha \\
1& \quad \forall s, \forall a, text{ 随机初始化 } Q(s, a); \text{ 根据Q函数构建策略 } {\pi}; \\
2& \quad \text{repeat }\\
3& \quad \quad \text{初始化起始状态 } s; \\
4& \quad \quad \text{repeat }\\
5& \quad \quad \quad \text{在状态 } s{\prime}, \text{ 选择动作 } a{\prime}=\pi^{\epsilon}\left(s{\prime}\right) ;\\
6& \quad \quad \quad \text{执行动作 } a,\text{ 得到即时奖励 } r \text{ 和新状态 } s{\prime};\\
7& \quad \quad \quad  Q(s, a) \leftarrow Q(s, a)+\alpha\left(r+\gamma Q\left(s{\prime}, a{\prime}\right)-Q(s, a)\right) ;  \\
8& \quad \quad \quad s \leftarrow s{\prime} ;\\
9& \quad \quad \text{until } s \text{ 为终止状态 } ;\\
10& \quad \text{until } \forall s, a, Q(s, a) \text{ 收敛 } ;\\
& \text{输出: 策略 } \pi(s)=\arg \max _{a \in|\mathcal{A}|} Q(s, a) \\
\hline 
\end{array}
$$
SARSA算法的采样和优化的策略都是 ${\pi^{\epsilon}}$, 因 此是一种同策略算法. 为了提高计算效率, 我们不需要对环境中所有的 ${s, a}$ 组合 进行穷举, 并计算值函数. 只需要将当前的探索 ${\left(s, a, r, s{\prime}, a{\prime}\right)}$ 中 ${s{\prime}, a{\prime}}$ 作为下一次 估计的起始状态和动作.

时序差分学习是 #强化学习 的主要学习方法, 其关键步骤就是在每次迭代中优化Q函数来减少现实 ${r+\gamma Q\left(s{\prime}, a{\prime}\right)}$ 和预期 ${Q(s, a)}$ 的差距. 这和动物学习的机制十分相像．
在大脑神经元中，多巴胺的释放机制和时序差分学习十分吻合．[Schultz, 1998] 的一个实验中，通过监测猴子大脑释放的多巴胺浓度，发现如果猴子获得比预期更多的果汁，或者在没有预想到的时间喝到果汁, 多巴胺释放大增．如果没有喝到本来预期的果汁，多巴胺的释放就会大减．多巴胺的释放, 来自对于实际奖励和预期奖励的差异，而不是奖励本身．

时序差分学习方法和蒙特卡罗方法的主要不同为：蒙特卡罗方法需要一条完整的路径才能知道其总回报，也不依赖马尔可夫性质；而时序差分学习方法只需要一步，其总回报需要通过马尔可夫性质来进行近似估计．
[[Q学习]]（Q-Learning）算法[Watkins et al., 1992]是一种异策略的时序差分学习方法．