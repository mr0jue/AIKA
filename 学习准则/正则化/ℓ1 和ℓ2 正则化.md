 $\ell_{1}$  和  $\ell_{2}$  正则化是机器学习中最常用的正则化方法, 通过约束参数的  $\ell_{1}$ 和  $\ell_{2}$  #范数 来减小模型在训练数据集上的过拟合现象。
通过加入 $\ell_{1}$  和  $\ell_{2}$  正则化, 优化问题可以写为$$\theta^{*}=\underset{\theta}{\arg \min } \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}\left(y^{(n)}, f\left(\mathbf{x}^{(n)}, \theta\right)\right)+\lambda \ell_{p}(\theta)$$其中  $\mathcal{L}(\cdot)$  为损失函数,  $N$  为训练样本数量,  $f(\cdot)$  为待学习的神经网络,  $\theta$  为其参 数,  $\ell_{p}$  为范数函数,  p  的取值通常为  $\{1,2\}$ 代表  $\ell_{1}$  和  $\ell_{2}$  范数,  $\lambda$  为正则化系数。
带正则化的优化问题等价于下面带约束条件的优化问题,$$\begin{array}{l}
\theta^{*}=\underset{\theta}{\arg \min } \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}\left(y^{(n)}, f\left(\mathbf{x}^{(n)}, \theta\right)\right), \\
\quad \text { subject to } \ell_{p}(\theta) \leq 1
\end{array}$$图给出了不同范数约束条件下的最优化问题示例
![[不同范数约束条件下的最优化问题示例.png]]
红线表示函数  $\ell_{p}=1$ $\mathcal{F}$  为函数  $f(\theta)$  的等高线（简单起见, 这里用直线表示)

ℓ1 范数的引入通常会使得参数有一定 #稀疏性 ，因此在很多算法中也经常使用。从图可以看出,  $\ell_{1}$  范数的约束通常会使得最优解位于坐标轴上, 而从使得最终的参数为稀疏性向量。此外,  $\ell_{1}$  范数在零点不可导, 因此经常下式来近似:$$\ell_{1}(\theta)=\sum_{i} \sqrt{\theta_{i}^{2}+\epsilon}$$其中  $\epsilon$  为一个非常小的常数。

一种折中的正则化方法是 #弹性网络正则化 (Elastic Net Regularization) [Zou and Hastie, 2005], 同时加入  $\ell_{1}$  和  $\ell_{2}$  正则化。$$\theta^{*}=\underset{\theta}{\arg \min } \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}\left(y^{(n)}, f\left(\mathbf{x}^{(n)}, \theta\right)\right)+\lambda_{1} \ell_{1}(\theta)+\lambda_{2} \ell_{2}(\theta)$$其中  $\lambda_{1}$ 和  $\lambda_{2}$  分别为两个正则化项的系数。


