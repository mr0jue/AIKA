机器学习模型的关键是 #泛化 问题，即在样本真实分布上的 #期望风险 最小化。而训练数据集上的 #经验风险最小化 和期望风险并不一致。由于神经网络的拟合能力非常强，其在训练数据上的错误率往往都可以降到非常低，甚至可以到0，从而导致 #过拟合 。因此，如何提高神经网络的泛化能力反而成为影响模型能力的最关键因素。

#正则化 （Regularization）是一类通过限制模型复杂度，从而避免过拟合，提高泛化能力的方法，包括引入一些约束规则， #增加先验 、提前停止等。

在传统的机器学习中，提高泛化能力的方法主要是限制模型复杂度，比如采用[[ℓ1 和ℓ2 正则化]]等方式。而在训练深层神经网络时，特别是在 #过度参数 时，ℓ1 和 ℓ2 正则化的效果往往不如浅层机器学习模型中显著。因此训练深度学习模型时，往往还会使用其它的正则化方法，比如[[数据增强]]、[[提前停止]]、[[丢弃法]]、集成法等。