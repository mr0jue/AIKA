损失函数是一个非负实数函数, 用来量化模型预测和真实标签之间的差异. 下面介绍几种常用的损失函数.

## 0-1 损失函数
最直观的损失函数是模型在训练集上的错误率, 即 #0-1损失函数 ( 0-1 Loss Function ): 
$${\begin{align}
\mathcal{L}(y, f(\boldsymbol{x} ; \theta)) &=\left\{\begin{array}{rr}
0 \text{ if } y=f(\boldsymbol{x} ; \theta) \\ 
1 \text{ if } y \neq f(\boldsymbol{x} ; \theta) \end{array}\right.\\ 
&=I(y \neq f(\boldsymbol{x} ; \theta)), \end{align}}$$ 其中 ${I(\cdot)}$ 是指示函数. 虽然 0-1 损失函数能够客观地评价模型的好坏, 但其缺点是数学性质不是很好: 不连续且导数为 0 , 难以优化. 因此经常用连续可微的损失函数替代. 

## 平方损失函数
#平方损失函数 ( Quadratic Loss Function) 经常用在预测标签 ${y}$ 为实数值的任务中, 定义为 $${ \mathcal{L}(y, f(\boldsymbol{x} ; \theta))=\frac{1}{2}(y-f(\boldsymbol{x} ; \theta))^{2} . }$$ 平方损失函数一般不适用于 #分类 问题. 

## 交叉熵损失函数
#交叉熵损失函数 ( Cross-Entropy Loss Function ) 一般用于 #分类 问题. 假设样本的标签 ${y \in\{1, \cdots, C\}}$ 为离散的类别, 模型 ${f(\boldsymbol{x} ; \theta) \in[0,1]^{C}}$ 的输出为类别标签的条件概率分布, 即 $${ p(y=c | \boldsymbol{x} ; \theta)=f_{c}(\boldsymbol{x} ; \theta), }$$ 并满足 $${ f_{c}(\boldsymbol{x} ; \theta) \in[0,1], \sum_{c=1}^{C} f_{c}(\boldsymbol{x} ; \theta)=1 . }$$其中 $f_{c}(\boldsymbol{x} ; \theta)$ 表示 $f(\boldsymbol{x} ; \theta)$的输出向量的第 $c$ 维．

我们可以用一个 ${C}$ 维的 #one-hot 向量 ${\boldsymbol{y}}$ 来表示样本标签. 假设样本的标签为 ${k}$, 那么标签向量 ${\boldsymbol{y}}$ 只有第 ${k}$ 维的值为 1 , 其余元素的值都为 0 . 标签向量 ${\boldsymbol{y}}$ 可以看作样本标签的真实条件概率分布 ${p_{r}(\boldsymbol{y} | \boldsymbol{x})}$, 即第 ${c}$ 维 ( 记为 ${y_{c}, 1 \leq c \leq C}$ ) 是类别为 ${c}$ 的真实条件概率. 假设样本的类别为 ${k}$, 那么它属于第 ${k}$ 类的概率为 1 , 属于其他类的概率为 0 . 

对于两个概率分布, 一般可以用 #交叉熵 来衡量它们的差异. 标签的真实分布 ${\boldsymbol{y}}$ 和模型预测分布 ${f(\boldsymbol{x} ; \theta)}$ 之间的交叉熵为 $${ \mathcal{L}(\boldsymbol{y}, f(\boldsymbol{x} ; \theta)) =-\boldsymbol{y}^{\top} \log f(\boldsymbol{x} ; \theta) \\ =-\sum_{c=1}^{C} y_{c} \log f_{c}(\boldsymbol{x} ; \theta) . }$$比如对于三分类问题, 一个样本的标签向量为 ${\boldsymbol{y}=[0,0,1]^{\top}}$, 模型预测的 标签分布为 ${f(\boldsymbol{x} ; \theta)=[0.3,0.3,0.4]^{\top}}$, 则它们的交叉熵为 ${-(0 \times \log (0.3)+0 \times \log (0.3)+1 \times \log (0.4))=-\log (0.4)}$. 

因为 ${\boldsymbol{y}}$ 为 one-hot 向量, $\mathcal{L}(\boldsymbol{y}, f(\boldsymbol{x} ; \theta))$ 也可以写为 ${ \mathcal{L}(y, f(\boldsymbol{x} ; \theta))=-\log f_{y}(\boldsymbol{x} ; \theta), }$ 其中 ${f_{y}(\boldsymbol{x} ; \theta)}$ 可以看作真实类别 ${y}$ 的似然函数. 因此, 交叉熵损失函数也就是 #负对数似然函数 (Negative Log-Likelihood). 

## Hinge损失函数
对于二分类问题, 假设 ${y}$ 的取值为 ${\{-1,+1\}, f(\boldsymbol{x} ; \theta) \in \mathbb{R}}$. #Hinge损失函数 ( Hinge Loss Function ) 为 $$\begin{aligned} \mathcal{L}(y, f(\boldsymbol{x} ; \theta)) &=\max (0,1-y f(\boldsymbol{x} ; \theta)) \\ & \triangleq[1-y f(\boldsymbol{x} ; \theta)]_{+} \end{aligned}$$ 其中 ${[x]_{+}=\max (0, x)}$.