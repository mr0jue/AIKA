# 概述
深度学习是近年来发展十分迅速的研究领域，并且在 #人工智能 的很多子领域都取得了巨大的成功．从根源来讲，深度学习是 #机器学习 的一个分支，是指一类问题以及解决这类问题的方法．首先，深度学习问题是一个机器学习问题，指从有限样例中通过算法总结出一般性的规律，并可以应用到新的未知数据上．

#深度学习 （Deep Learning， #DL ）通过构建具有一定“深度”的模型，并通过学习算法来让模型自动从数据中自动学习到有效的特征表示．所谓“深度”是指原始数据进行非线性特征转换的次数．经过多层的特征转换，深度学习模型把原始数据变成更高层次、更抽象的表示．从而避免特征工程，以替代人工设计的特征，从而最终提升预测模型的准确率．
目前，深度学习采用的模型主要是[[人工神经网络]]模型，其主要原因是神经网络模型可以使用误差反向传播算法，从而可以比较好地解决 #贡献度分配问题 ．只要是超过一层的神经网络都会存在贡献度分配问题，因此可以将超过一层的神经网络都看作深度学习模型．随着深度学习的快速发展，模型深度也从早期的5 ∼ 10层增加到目前的数百层．随着模型深度的不断增加，其特征表示的能力也越来越强，从而使后续的预测更加容易．
 # 深度学习的难点问题和解决
 
## [[最优化问题]]
网络优化是指寻找一个神经网络模型来使得经验（或结构）风险最小化的过程， 包括模型选择以及参数学习等． 
深度神经网络的优化十分困难． 是一个具有挑战性的问题． 
首先，深度神经网络是一个高度非线性的模型，其风险函数是一个非凸函数， 因此风险最小化是一个 #非凸优化 问题． 神经网络的损失函数是一个非凸函数，找到全局最优解通常比较困难． 
其次， 深度神经网络的参数通常非常多， 训练数据也比较大， 因此也无法使用计算代价很高的二阶优化方法，而一阶优化方法的训练效率通常比较低． 
此外，深度神经网络存在梯度消失或爆炸问题，导致基于梯度的优化方法经常失效．

## 网络正则化方法
正则化（Regularization）是一类通过限制模型复杂度，从而避免过拟合，提高泛化能力的方法，比如引入约束、增加先验、提前停止等．
在传统的机器学习中， 提高泛化能力的方法主要是限制模型复杂度， 比如采用 ℓ1 和 ℓ2 正则化等方式． 而在训练深度神经网络时， 特别是在 #过度参数化 （Over-Parameterization）时，ℓ1 和 ℓ2 正则化的效果往往不如浅层机器学习模型中显著．因此训练深度学习模型时， 往往还会使用其他的正则化方法， 比如数据增强、提前停止、丢弃法、集成法等．
