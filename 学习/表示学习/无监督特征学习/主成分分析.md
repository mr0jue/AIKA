主成份分析（Principal Component Analysis，PCA）一种最常用的数据降维方法，使得在转换后的空间中数据的方差最大。如图9.1所示的两维数据，如果将这些数据投影到一维空间中，选择数据方差最大的方向进行投影，才能最大化数据的差异性，保留更多的原始数据信息。
![[主成分分析.png|400]]

假设有一组 ${d}$ 维的样本 ${\mathbf{x}^{(n)} \in \mathbb{R}^{d}, 1 \leq n \leq N}$, 我们希望将其投影到一维空间中，投影向量为 ${\mathbf{w} \in \mathbb{R}^{d}}$ 。不失一般性，我们限制 ${\mathbf{w}}$ 的模为 1 , 即 ${\mathbf{w}^{\mathrm{T}} \mathbf{w}=1}$ 。每个样本点 ${\mathbf{x}^{(n)}}$ 投影之后的表示为
$${ z^{(n)}=\mathbf{w}^{\mathrm{T}} \mathbf{x}^{(n)} }$$

我们用矩阵 ${X=\left[\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \cdots, \mathbf{x}^{(N)}\right]}$ 表示输入样本, ${\overline{\mathbf{x}}=\frac{1}{N} \sum_{n=1}^{N} \mathbf{x}^{(n)}}$ 为原始样本的中心点, 所有样本投影后的方差为 
$${ \sigma(X ; \mathbf{w}) =\frac{1}{N} \sum_{n=1}^{N}\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}^{(n)}-\mathbf{w}^{\mathrm{T}} \overline{\mathbf{x}}\right)^{2} \\ =\frac{1}{N}\left(\mathbf{w}^{\mathrm{T}} X-\mathbf{w}^{\mathrm{T}} \bar{X}\right)\left(\mathbf{w}^{\mathrm{T}} X-\mathbf{w}^{\mathrm{T}} \bar{X}\right)^{\mathrm{T}} \\ =\mathbf{w}^{\mathrm{T}} S \mathbf{w} }$$
其中 ${\bar{X}=\overline{\mathbf{x}} \mathbf{1}_{d}^{\mathrm{T}}}$ 为 ${d}$ 列 ${\overline{\mathbf{x}}}$ 组成的矩阵, ${S=\frac{1}{N}(X-\bar{X})(X-\bar{X})^{\mathrm{T}}}$ 是原始样本的协方差矩阵。 

最大化投影方差 ${\sigma(X ; w)}$ 并满足 ${\mathbf{w}^{\mathrm{T}} \mathbf{w}=1}$, 利用拉格朗日方法转换为无约束优化问题, 
$${ \max _{\mathbf{w}} \mathbf{w}^{\mathrm{T}} S \mathbf{w}+\lambda\left(1-\mathbf{w}^{\mathrm{T}} \mathbf{w}\right) }$$ 
其中 ${\lambda}$ 为拉格朗日乘子。对上式求导并令导数等于 0 , 可得
$${ S \mathbf{w}=\lambda \mathbf{w} . }$$
从上式可知, ${\mathbf{w}}$ 是协方差矩阵 ${S}$ 的特征向量, ${\lambda}$ 为特征值。同时 $${ \sigma(X ; w)=\mathbf{w}^{\mathrm{T}} S \mathbf{w}=\mathbf{w}^{\mathrm{T}} \lambda \mathbf{w}=\lambda }$$ ${\lambda}$ 也是投影后样本的方差。因此, 主成分分析可以转换成一个矩阵特征值分解问题, 投影向量 ${\mathbf{w}}$ 为矩阵 ${S}$ 的最大特征对应的特征向量。 
如果要通过投影矩阵 ${W \in R^{d \times d{\prime}}}$ 将样本投到 ${d{\prime}}$ 维空间, 投影矩阵满足 ${W^{\mathrm{T}} W=}$ I, 只需要将 ${S}$ 的特征值从大到小排列, 保留前 ${d{\prime}}$ 个特征向量, 其对应的特征向量即使最优的投影矩阵。 
$${ S W=W \operatorname{diag}(\Lambda) }$$
其中 ${\Lambda=\left[\lambda_{1}, \cdots, \lambda_{d{\prime}}\right]}$ 为 ${S}$ 的前 ${d{\prime}}$ 个最大的特征值。

主成分分析是一种无监督学习方法，可以作为监督学习的数据预处理方法，用来去除噪声并减少特征之间的相关性，但是它并不能保证投影后数据的类别可分性更好。提高两类可分性的方法一般为监督学习方法，比如[[线性判别分析]]（Linear Discriminant Analysis，LDA）。
