为了提高机器学习系统的准确率，我们就需要将输入信息转换为有效的 #特征 ，或者更一般性地称为 #表示 （Representation）．如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫作 #表示学习 （Representation Learning），也称为特征学习。

表示学习的关键是解决 #语义鸿沟 问题。如果一个预测模型直接建立在底层特征之上，会导致对预测模型的能力要求过高。如果可以有一个好的表示在某种程度上可以反映出数据的高层语义特征，那么我们就可以相对容易地构建后续的机器学习模型。

## 核心问题
在表示学习中，有两个核心问题：一是“什么是一个好的表示？”；二是“如何学习到好的表示？”
### 什么是一个好的表示？
“好的表示”是一个非常主观的概念，没有一个明确的标准。但一般而言，一个好的表示具有以下几个优点：

• 一个好的表示应该具有很强的表示能力，即同样大小的向量可以表示更多信息。
• 一个好的表示应该使后续的学习任务变得简单，即需要包含更高层的语义信息。
• 一个好的表示应该具有一般性，是任务或领域独立的。虽然目前的大部分表示学习方法还是基于某个任务来学习，但我们期望其学到的表示可以比较容易的迁移到其它任务上。

在传统机器学习中，我们经常使用两种方式来表示特征： #局部表示 （Local Representation）和 #分布式表示 （Distributed Representation）。

### 如何学习到好的表示？
要学习到一种好的高层语义表示（一般为分布式表示），通常需要从底层特征开始，经过多步非线性转换才能得到。一个深层结构的优点是可以增加特征的重用性，从而指数级地增加表示能力。因此，表示学习的关键是构建具有一定深度的多层次特征表示[Bengio et al., 2013]。
#深度学习 虽然早期主要用来进行表示学习，但后来越来越多地用来处理更加复杂的推理、决策等问题。

在传统的机器学习中，也有很多有关 #特征学习 的方法，比如[[主成分分析]]、[[线性判别分析]]、[[独立成分分析]]。但是传统的特征学习一般是通过人为地设计一些准则，然后根据这些准则来选取有效的特征。特征的学习是和最终预测模型的学习分开进行的，因此学习到的特征不一定可以提升最终模型的性能。

