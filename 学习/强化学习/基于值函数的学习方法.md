#值函数 是对 #策略 ${\pi}$ 的评估. 如果策略 ${\pi}$ 有限 (即状态数和动作数都有限), 可以对所有的策略进行评估并选出最优策略 ${\pi^{*}}$. $${ \forall s, \pi^{*}=\underset{\pi}{\arg \max} V^{\pi}(s) .}$$但这种方式在实践中很难实现. 假设状态空间 ${\mathcal{S}}$ 和动作空间 ${\mathcal{A}}$ 都是离散且有限的,策略空间为 ${|\mathcal{A}|^{|\mathcal{S}|}}$, 往往也非常大. 一种可行的方式是通过迭代的方法不断优化策略, 直到选出最优策略. 对于 一个策略 ${\pi(a | s)}$, 其 #Q函数 为 ${Q^{\pi}(s, a)}$, 我们可以设置一个新的策略 ${\pi{\prime}(a | s)}$, $${ \pi{\prime}(a | s)=\left\{\begin{array}{ll} 1 \text{ if } a=\underset{\hat a}{\arg \max }  Q^{\pi}(s, \hat{a}), \\ 0 \text{ otherwise },\end{array}\right. }$$ 即 ${\pi{\prime}(a | s)}$ 为一个确定性的策略, 也可以直接写为 $${ \pi{\prime}(s)=\underset{a}{\arg \max} Q^{\pi}(s, a) . }$$ 如果执行 ${\pi{\prime}}$, 会有 ${ \forall s, V^{\pi{\prime}}(s) \geq V^{\pi}(s) . }$

根据公式，我们可以通过下面方式来学习最优策略：先随机初始化一个策略，计算该策略的值函数，并根据值函数来设置新的策略，然后一直反复迭代直到收敛．
#基于值函数的策略学习方法 中最关键的是如何计算策略 𝜋 的值函数，一般有[[动态规划算法|动态规划]]或[[蒙特卡罗方法|蒙特卡罗]]两种计算方式．
