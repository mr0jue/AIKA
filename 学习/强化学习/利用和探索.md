但在 #蒙特卡罗方法 中，如果采用确定性策略 𝜋，每次试验得到的轨迹是一样的，只能计算出  ${Q^{\pi}(s, a)}$ ，而无法计算其他动作 𝑎′ 的 Q 函数，因此也无法进一步改进策略．这样情况仅仅是对当前策略的 #利用 （exploitation），而缺失了对环境的 #探索 （exploration），即试验的轨迹应该尽可能覆盖所有的状态和动作，以找到更好的策略．

为了平衡利用和探索, 我们可以采用 #𝜖-贪心法 (𝜖-greedy Method). 对于一个目标策略 ${\pi}$, 其对应的 ${\epsilon}$-贪心法策略为 $${ \pi^{\epsilon}(s)=\left\{\begin{array}{cc} \pi(s), & \text{ 按概率 } 1-\epsilon, \\ \text{ 随机选择 } \mathcal{A} \text{ 中的动作 }, &  \text{ 按概率 } \epsilon . \end{array}\right. }$$ 这样, ${\epsilon}$-贪心法将一个仅利用的策略转为带探索的策略. 每次选择动作 ${\pi(s)}$ 的概率为 ${1-\epsilon+\frac{\epsilon}{|\mathcal{A}|}}$, 其他动作的概率为 ${\frac{\epsilon}{|\mathcal{A}|}}$. 

### 同策略 
在蒙特卡罗方法中, 如果采样策略是 ${\pi^{\epsilon}(s)}$, 不断改进策略也是 ${\pi^{\epsilon}(s)}$ 而不是目标策略 ${\pi(s)}$. 这种采样与改进策略相同（即都是 ${\pi^{\epsilon}(s)}$ ) 的强化学习方法叫作 #同策略 ( On-Policy ) 方法.

### 异策略 
如果采样策略是 ${\pi^{\epsilon}(s)}$,，而优化目标是策略  ${\pi}$，可以通过 #重要性采样 ，引入重要性权重来实现对目标策略 ${\pi}$ 的优化．
这种采样与改进分别使用不同策略的强化学习方法叫作 #异策略 （Off-Policy）方法．

