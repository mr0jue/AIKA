# 引言
监督学习一般需要一定数量的带标签的数据．在很多的应用场景中，通过人工标注的方式来给数据打标签的方式往往行不通．比如我们通过监督学习来训练一个模型可以自动下围棋，就需要将当前棋盘的状态作为输入数据，其对应的最佳落子位置（动作）作为标签．训练一个好的模型就需要收集大量的不同棋盘状态以及对应动作．这种做法实践起来比较困难，一是对于每一种棋盘状态，即使是专家也很难给出“正确”的动作，二是获取大量数据的成本往往比较高．对于下棋这类任务，虽然我们很难知道每一步的“正确”动作，但是其最后的结果（即赢输）却很容易判断．因此，如果可以通过大量的模拟数据，通过最后的结果（奖励）来倒推每一步棋的好坏，从而学习出“最佳”的下棋策略，这就是强化学习．

# 强化学习
#强化学习 （Reinforcement Learning， #RL ），也叫 #增强学习 ，是指一类从（与环境）交互中不断学习的问题以及解决这类问题的方法．强化学习问题可以描述为一个智能体从与环境的交互中不断学习以完成特定目标（比如取得最大奖励值）．和深度学习类似，强化学习中的关键问题也是 #贡献度分配问题 [Minsky, 1961]，每一个动作并不能直接得到监督信息，需要通过整个模型的最终监督信息（奖励）得到，并且有一定的延时性．

强化学习（Reinforcement Learning，RL）是一类通过交互来学习的机器学习算法，
/在强化学习中， 智能体根据环境的状态做出一个动作， 并得到即时或延时的奖励． 智能体在和环境的交互中不断学习并调整策略，以取得最大化的期望总回报．

强化学习也是 #机器学习 中的一个重要分支．强化学习和 #监督学习 的不同在于，强化学习问题不需要给出“正确”策略作为监督信息，只需要给出策略的（延迟）回报，并通过调整策略来取得最大化的期望回报．

强化学习广泛应用于很多领域，比如电子游戏、棋类游戏、迷宫类游戏、控制系统、推荐等．这里我们介绍几个比较典型的[[强化学习例子]]．


# 强化学习的定义
我们先描述强化学习的任务定义．
在强化学习中，有两个可以进行交互的对象：智能体和环境．
（1） #智能体 （ #Agent ）可以感知外界环境的状态（State）和反馈的奖励（Reward），并进行学习和决策．智能体的决策功能是指根据外界环境的状态来做出不同的动作（Action），而学习功能是指根据外界环境的奖励来调整策略．
（2） #环境 （Environment）是智能体外部的所有事物，并受智能体动作的影响而改变其状态，并反馈给智能体相应的奖励．

强化学习的基本要素包括
1) #状态 ${s}$ 是对环境的描述, 可以是离散的或连续的, 其状态空间为 ${\mathcal{S}}$. 
2) #动作 ${a}$ 是对智能体行为的描述, 可以是离散的或连续的, 其动作空 间为 ${\mathcal{A}}$. 
3) [[策略]] ${\pi(a | s)}$ 是智能体根据环境状态 ${s}$ 来决定下一步动作 ${a}$ 的函数. 
4) #状态转移概率 ${p\left(s{\prime} | s, a\right)}$ 是在智能体根据当前状态 ${s}$ 做出一个动作 ${a}$ 之 后, 环境在下一个时刻转变为状态 ${s{\prime}}$ 的概率. 
5) #即时奖励 ${r\left(s, a, s{\prime}\right)}$ 是一个标量函数, 即智能体根据当前状态 ${s}$ 做出动 作 ${a}$ 之后, 环境会反馈给智能体一个 #奖励, 这个奖励也经常和下一个时刻的状态 ${s{\prime}}$ 有关. 

为简单起见, 我们将智能体与环境的交互看作离散的时间序列. 智能体从感知到的初始环境 ${s_{0}}$ 开始, 然后决定做一个相应的动作 ${a_{0}}$, 环境相应地发生改变到新的状态 ${s_{1}}$, 并反馈给智能体一个即时奖励 ${r_{1}}$, 然后智能体又根据状态 ${s_{1}}$ 做一 个动作 ${a_{1}}$, 环境相应改变为 ${s_{2}}$, 并反馈奖励 ${r_{2}}$. 这样的交互可以一直进行下去. $${ s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, \cdots, s_{t-1}, r_{t-1}, a_{t-1}, s_{t}, r_{t}, \cdots }$$ 其中 ${r_{t}=r\left(s_{t-1}, a_{t-1}, s_{t}\right)}$ 是第 ${t}$ 时刻的即时奖励. 
图给出了智能体与环境的交互.
![[智能体与环境的交互.png|400]]
智能体与环境的交互过程可以看作一个[[马尔可夫决策过程]]( #MDP)。


# 强化学习的目标
强化学习的目标是学习到一个 #策略 来最大化[[回报|期望回报]] （Expected Return），即希望智能体执行一系列的动作来获得尽可能多的平均回报．在 #持续式任务 中，强化学习的优化目标也可以定义为 #MDP 到达平稳分布时 #即时奖励 的期望．

## 值函数
**值函数 是对 策略 的评估.** 

为了评估策略 ${\pi}$ 的 #期望回报, 我们定义两个[[值函数]]:  #状态值函数 ${V^{\pi}(s)}$和 #状态-动作值函数 ${Q^{\pi}(s, a)}$(也称为 #Q函数 ). 
有了对策略的评估，我们就可以根据值函数来进行优化：假设在状态 ${s}$, 有一个动作 ${a^{*}}$ 使得 ${Q^{\pi}\left(s, a^{*}\right)>V^{\pi}(s)}$, 说明执行动作 ${a^{*}}$ 的回报比当前的策略 ${\pi(a | s)}$ 要高, 我们就可以调整参数使得策略中动作 ${a^{*}}$ 的概率 ${p\left(a^{*} | s\right)}$ 增加.


# 强化学习的学习方法

强化学习的算法非常多，大体上可以分为 #基于值函数的方法 方法 （包括[[动态规划算法|动态规划]]、[[蒙特卡罗方法|蒙特卡罗]]、[[时序差分学习]]等）、 #基于策略函数的方法 （包括[[策略梯度]]等）以及[[Actor-Critic算法|融合两者]]的方法。

## 基于值函数的学习方法

如果策略 ${\pi}$ 有限 (即状态数和动作数都有限), 则可以使用 #值函数 对所有的 #策略 进行评估并选出最优策略 ${\pi^{*}}$. 但这种方式在实践中很难实现：假设 #状态空间 ${\mathcal{S}}$ 和 #动作空间 ${\mathcal{A}}$ 都是离散且有限的,策略空间为 ${|\mathcal{A}|^{|\mathcal{S}|}}$, 往往也非常大。

一种可行的方式是通过迭代的方法不断优化策略, 直到选出最优策略：
先随机初始化一个策略，计算该策略的值函数，并根据值函数来设置新的策略，然后一直反复迭代直到收敛．
基于值函数的策略学习方法中最关键的是如何计算策略 𝜋 的值函数，一般有[[动态规划算法|动态规划]]或[[蒙特卡罗方法|蒙特卡罗]]两种计算方式．而[时序差分学习]]方法是 #蒙特卡罗方法 的一种改进，通过引入 #动态规划算法 来提高学习效率。

整体上, 在基于值函数的学习方法中, 策略一般为确定性策略. 策略优化通常都依赖于值函数, 比如 #贪心策略 ${\pi(s)=\arg \max _{a} Q(s, a)}$. 最优策略一般需要遍历当前状态 ${\mathrm{s}}$ 下的所有动作, 并找出最优的 ${Q(s, a)}$. 当动作空间离散但是很大时, 遍历求最大需要很高的时间复杂度; 当动作空间是连续的并且 ${Q(s, a)}$ 非凸时, 也很难求解出最佳的策略.

## 基于策略函数的学习方法 
强化学习的目标是学习到一个策略 $\pi_{\theta}(a \mid s)$ 来最大化期望回报．一种直接的方法是在策略空间直接搜索来得到最佳策略，称为 #策略搜索 （Policy Search）．

策略搜索本质是一个 #优化问题 ，可以分为 [[策略梯度|基于梯度的优化]] 和 #无梯度优化 ．策略搜索和基于值函数的方法相比，策略搜索可以不需要值函数，直接优化策略．参数化的策略能够处理连续状态和动作，可以直接学出随机性策略．

## 融合两者的学习方法
一般而言，基于值函数的方法策略更新时可能会导致值函数的改变比较大，对收敛性有一定影响，而基于策略函数的方法在策略更新时更加更平稳些。但后者因为策略函数的解空间比较大，难以进行充分的采样，导致方差较大，并容易收敛到局部最优解。[[Actor-Critic算法]]通过融合两种方法，取长补短，有着更好的收敛性。

不同算法之间的关系如图所示。
![[不同强化学习算法之间的关系.png]]

# 强化学习算法的优化步骤
这些不同的强化学习算法的优化步骤都可以分为三步：（1）执行策略，生成样本；（2）估计回报；（3）更新策略。
表给出了四种典型的强化学习算法（SARSA、Q学习、REINFORCE、Actor-Critic算法）的比较。
$$\begin{array}{llll} 
\hline 
算法 & 执行策略, 生成样本 & 估计回报 & 更新策略 \\
\hline 
\\
SARSA 
& s, a, r, s^{\prime}, a^{\prime} 
& Q(s, a) \leftarrow Q(s, a)+\alpha\left(r+\gamma Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right) 
& pi(s)=\arg \max _{a \in|\mathcal{A}|} Q(s, a) \\\\
Q学习
& s, a, r, s^{\prime} 
& Q(s, a) \leftarrow Q(s, a)+\alpha\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right) 
& \pi(s)=\arg \max _{a \in|\mathcal{A}|} Q(s, a) \\\\
REINFORCE
& \tau=s_{0}, a_{0}, s_{1}, a_{1}, \cdots 
& G(\tau)=\sum_{t=0}^{T-1} r_{t+1} 
& \theta \leftarrow \theta+\sum_{t=0}^{T-1}\left(\frac{\partial}{\partial \theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) \gamma^{t} G\left(\tau_{t: T}\right)\right) \\\\
Actor-Critic
& s, a, s^{\prime}, r 
& {\begin{array}{l}
    G(s)=r+\gamma V_{\phi}\left(s^{\prime}\right) \\
    \phi \leftarrow \phi+\beta\left(G(s)-V_{\phi}(s)\right) \frac{\partial}{\partial \phi} V_{\phi}(s) 
\end{array}}
& {\begin{array}{l}
    \lambda \leftarrow \gamma \lambda \\
    \theta \leftarrow \theta+\alpha \lambda\left(G(s)-V_{\phi}(s)\right) \frac{\partial}{\partial \theta} \log \pi_{\theta}(a \mid s) 
\end{array}}\\\\
\hline 
\end{array}$$

# 深度强化学习
在强化学习中, 一般需要建模 #策略 ${\pi(a | s)}$ 和 #值函数 ${V^{\pi}(s)}$, ${Q^{\pi}(s, a)}$. 
#深度强化学习 ( Deep Reinforcement Learning ) 是将强化学习和深度学习结合在一起, 用强化学习来定义问题和优化目标, 用深度学习来解决策略和值函数的建模问题, 然后使用误差反向传播算法来优化目标函数. 深度强化学习在一定程度上具备解决复杂问题的通用智能, 并在很多任务上都取得了很大的成功.

DeepMind的[Mnih et al., 2015]在2013年提出了第一个强化学习和深度学习结合的模型，[[深度Q网络]]（DQN）。虽然 DQN 模型相对比较简单，只是面向有限的动作空间，但依然在Atari游戏上取了很大的成功，取得了超越人类水平的成绩。之后，深度强化学习开始快速发展。一些基于
DQN的改进包括双Q网络[Van Hasselt et al., 2016]、优先级经验回放[Schaul et al., 2015]、决斗网络[Wang et al., 2015]等。

目前，深度强化学习更多是同时使用 #策略网络 和 #值网络 来近似 #策略函数 和 #值函数 。

在actor-critic算法的基础上，[Silver et al. 2014]将策略梯度的思想推广到确定性的策略上，提出了确定性策略梯度（Deterministic Policy Gradient，DPG）算法。策略函数为状态到动作的映射 $a = π_\theta(s)$。采用确定性策略的一个好处是方差会变得很小，提高收敛性。确定性策略的缺点是对环境的探索不足，可以通过异策略的方法解决。

[Lillicrap et al. 2015]进一步在DPG 算法的基础上，利用DQN来估计值函数，提出深度确定性策略梯度（Deep Deterministic Policy Gradient，DDPG）算法。DDPG算法可以适合连续的状态和动作空间。

[Mnih et al. 2016]利用分布式计算的思想提出了异步优势的演员-评论员（Asynchronous Advantage Actor-Critic，A3C）算法。在A3C算法中，有多个并行的环境，每个环境中都有一个智能体执行各自的动作和并计算累计的参数梯度。在一定步数后进行累计，利用累计的参数梯度去更新所有智能体共享的全局参数。因为不同环境中的智能体可以使用不同的探索策略，会导致经验样本之间的相关性较小，可以提高学习效率。


# 强化学习的分类
从 #贝尔曼方程 可知，如果知道 #马尔可夫决策过程 的 #状态转移概率 ${p\left(s{\prime} | s, a\right)}$ 和 #奖励 ${r\left(s, a, s{\prime}\right)}$ ，我们直接可以通过贝尔曼方程来迭代计算其 #值函数 ．这种模型已知的强化学习算法也称为 #基于模型的强化学习 （Model-Based Reinforcement Learning）算法，这里的模型就是指[[马尔可夫决策过程]]．在模型已知时，可以通过 #动态规划 的方法来计算．

在很多应用场景中，马尔可夫决策过程的状态转移概率 𝑝(𝑠′ |𝑠, 𝑎) 和奖励函数 𝑟(𝑠, 𝑎, 𝑠′) 都是未知的 ．
在这种情况下，我们一般需要智能体和环境进行交互，并收集一些样本，然后再根据这些样本来求解马尔可夫决策过程最优策略．这种模型未知，基于采样的学习算法也称为 #模型无关的强化学习 （Model-Free Reinforcement Learning）算法．

*基于模型的强化学习，也叫作 #模型相关的强化学习 ，或 #有模型的强化学习 ．
模型无关的强化学习也叫作 #无模型的强化学习 ．*

# 后记

强化学习通过智能体不断与环境进行交互，并根据经验调整其策略来最大化其长远的所有奖励的累积值。强化学习更接近生物学习的本质，可以应对多种复杂的场景，而从更接近通用人工智能系统的目标。

强化学习和监督学习的区别在于：
1) 强化学习的样本通过不同与环境进行交互产生，即试错学习，而监督学习的样本由人工收集并标注；
2) 强化学习的反馈信息只有奖励，并且是延迟的；而监督学习需要明确的指导信息（每一个状态对应的动作）。

现代强化学习可以追溯到两个来源：一个是心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为；另一个是控制论领域的最优控制问题，即在满足一定约束条件下，寻求最优控制策略，使得性能指标取极大值或极小值。

早期的强化学习算法主要关注状态和动作都是离散且有限的问题, 可以使用表格来记录这些概率. 但在很多实际问题中, 有些任务的状态和动作的数量非常多. 比如围棋的棋局有 ${3^{361} \approx 10^{170}}$ 种状态, 动作 (即落子位置 ) 数量为 361. 还有些任务的状态和动作是连续的. 比如在自动驾驶中, 智能体感知到的环境状态是各种传感器数据, 一般都是连续的. 动作是操作方向盘的方向和速度控制, 也是连续的. 为了有效地解决这些问题, 我们可以设计一个更强的策略函数 (比如深度神经网络 ), 使得智能体可以应对复杂的环境, 学习更优的策略, 并具有更好的泛化能力.

*从某种意义上讲， #深度学习 以看作一种强化学习（Reinforcement Learning，RL），每个内部组件并不能直接得到监督信息，需要通过整个模型的最终监督信息（奖励）得到，并且有一定的延时性．*

# 其他
对于一个策略 ${\pi(a | s)}$, 其 #Q函数 为 ${Q^{\pi}(s, a)}$, 我们可以设置一个新的策略 ${\pi{\prime}(a | s)}$, $${ \pi{\prime}(a | s)=\left\{\begin{array}{ll} 1 \text{ if } a=\underset{\hat a}{\arg \max }  Q^{\pi}(s, \hat{a}), \\ 0 \text{ otherwise },\end{array}\right. }$$ 即 ${\pi{\prime}(a | s)}$ 为一个确定性的策略, 也可以直接写为 $${ \pi{\prime}(s)=\underset{a}{\arg \max} Q^{\pi}(s, a) . }$$ 如果执行 ${\pi{\prime}}$, 会有 ${ \forall s, V^{\pi{\prime}}(s) \geq V^{\pi}(s) . }$



$$\begin{array}{ll} 
\hline 
算法 & 步骤 \\
\hline \\
            &(1) 执行策略, 生成样本: s, a, r, s^{\prime}, a^{\prime} \\
SARSA       &(2) 估计回报: Q(s, a) \leftarrow Q(s, a)+\alpha\left(r+\gamma Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right) \\
            &(3) 更新策略: \pi(s)=\arg \max _{a \in|\mathcal{A}|} Q(s, a) \\\\
            &(1)执行策略, 生成样本: s, a, r, s^{\prime} \\
Q学习       &(2) 估计回报: Q(s, a) \leftarrow Q(s, a)+\alpha\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right) \\
            &(3) 更新策略: \pi(s)=\arg \max _{a \in|\mathcal{A}|} Q(s, a) \\\\
            &(1) 执行策略, 生成样本: \tau=s_{0}, a_{0}, s_{1}, a_{1}, \cdots \\
REINFORCE   &(2) 估计回报: G(\tau)=\sum_{t=0}^{T-1} r_{t+1} \\
            &(3) 更新策略: \theta \leftarrow \theta+\sum_{t=0}^{T-1}\left(\frac{\partial}{\partial \theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right) \gamma^{t} G\left(\tau_{t: T}\right)\right) \\\\
            &(1) 执行策略, 生成样本: s, a, s^{\prime}, r \\
Actor-Critic&(2) 估计回报: G(s)=r+\gamma V_{\phi}\left(s^{\prime}\right) , \phi \leftarrow \phi+\beta\left(G(s)-V_{\phi}(s)\right) \frac{\partial}{\partial \phi} V_{\phi}(s) \\
            &(3)更新策略: \lambda \leftarrow \gamma \lambda ,\theta \leftarrow \theta+\alpha \lambda\left(G(s)-V_{\phi}(s)\right) \frac{\partial}{\partial \theta} \log \pi_{\theta}(a \mid s) \\\\
\hline 
\end{array}$$