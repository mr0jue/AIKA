# 强化学习
监督学习一般需要一定数量的带标签的数据．在很多的应用场景中，通过人工标注的方式来给数据打标签的方式往往行不通．比如我们通过监督学习来训练一个模型可以自动下围棋，就需要将当前棋盘的状态作为输入数据，其对应的最佳落子位置（动作）作为标签．训练一个好的模型就需要收集大量的不同棋盘状态以及对应动作．这种做法实践起来比较困难，一是对于每一种棋盘状态，即使是专家也很难给出“正确”的动作，二是获取大量数据的成本往往比较高．对于下棋这类任务，虽然我们很难知道每一步的“正确”动作，但是其最后的结果（即赢输）却很容易判断．因此，如果可以通过大量的模拟数据，通过最后的结果（奖励）来倒推每一步棋的好坏，从而学习出“最佳”的下棋策略，这就是强化学习．

#强化学习 （Reinforcement Learning， #RL ），也叫 #增强学习 ，是指一类从（与环境）交互中不断学习的问题以及解决这类问题的方法．强化学习问题可以描述为一个智能体从与环境的交互中不断学习以完成特定目标（比如取得最大奖励值）．和深度学习类似，强化学习中的关键问题也是 #贡献度分配问题 [Minsky, 1961]，每一个动作并不能直接得到监督信息，需要通过整个模型的最终监督信息（奖励）得到，并且有一定的延时性．

强化学习（Reinforcement Learning，RL）是一类通过交互来学习的机器学习算法，
/在强化学习中， 智能体根据环境的状态做出一个动作， 并得到即时或延时的奖励． 智能体在和环境的交互中不断学习并调整策略，以取得最大化的期望总回报．

强化学习也是 #机器学习 中的一个重要分支．强化学习和 #监督学习 的不同在于，强化学习问题不需要给出“正确”策略作为监督信息，只需要给出策略的（延迟）回报，并通过调整策略来取得最大化的期望回报．

强化学习广泛应用于很多领域，比如电子游戏、棋类游戏、迷宫类游戏、控制系统、推荐等．这里我们介绍几个比较典型的[[强化学习例子]]．


# 强化学习的定义
我们先描述强化学习的任务定义．
在强化学习中，有两个可以进行交互的对象：智能体和环境．
（1） #智能体 （ #Agent ）可以感知外界环境的状态（State）和反馈的奖励（Reward），并进行学习和决策．智能体的决策功能是指根据外界环境的状态来做出不同的动作（Action），而学习功能是指根据外界环境的奖励来调整策略．
（2） #环境 （Environment）是智能体外部的所有事物，并受智能体动作的影响而改变其状态，并反馈给智能体相应的奖励．

强化学习的基本要素包括
1) #状态 ${s}$ 是对环境的描述, 可以是离散的或连续的, 其状态空间为 ${\mathcal{S}}$. 
2) #动作 ${a}$ 是对智能体行为的描述, 可以是离散的或连续的, 其动作空 间为 ${\mathcal{A}}$. 
3) [[策略]] ${\pi(a | s)}$ 是智能体根据环境状态 ${s}$ 来决定下一步动作 ${a}$ 的函数. 
4) #状态转移概率 ${p\left(s{\prime} | s, a\right)}$ 是在智能体根据当前状态 ${s}$ 做出一个动作 ${a}$ 之 后, 环境在下一个时刻转变为状态 ${s{\prime}}$ 的概率. 
5) #即时奖励 ${r\left(s, a, s{\prime}\right)}$ 是一个标量函数, 即智能体根据当前状态 ${s}$ 做出动 作 ${a}$ 之后, 环境会反馈给智能体一个 #奖励, 这个奖励也经常和下一个时刻的状态 ${s{\prime}}$ 有关. 

为简单起见, 我们将智能体与环境的交互看作离散的时间序列. 智能体从感知到的初始环境 ${s_{0}}$ 开始, 然后决定做一个相应的动作 ${a_{0}}$, 环境相应地发生改变到新的状态 ${s_{1}}$, 并反馈给智能体一个即时奖励 ${r_{1}}$, 然后智能体又根据状态 ${s_{1}}$ 做一 个动作 ${a_{1}}$, 环境相应改变为 ${s_{2}}$, 并反馈奖励 ${r_{2}}$. 这样的交互可以一直进行下去. $${ s_{0}, a_{0}, s_{1}, r_{1}, a_{1}, \cdots, s_{t-1}, r_{t-1}, a_{t-1}, s_{t}, r_{t}, \cdots }$$ 其中 ${r_{t}=r\left(s_{t-1}, a_{t-1}, s_{t}\right)}$ 是第 ${t}$ 时刻的即时奖励. 
图给出了智能体与环境的交互.
![[智能体与环境的交互.png|400]]
智能体与环境的交互过程可以看作一个[[马尔可夫决策过程]]（Markov Decision Process， #MDP ）


# 强化学习的目标函数
## 总回报
给定策略 ${\pi(a | s)}$，智能体和环境一次交互过程的轨迹 𝜏 所收到的累积奖励为 #总回报 （Return）．$$\begin{aligned} G(\tau) &=\sum_{t=0}^{T-1} r_{t+1} \\ &=\sum_{t=0}^{T-1} r\left(s_{t}, a_{t}, s_{t+1}\right) \end{aligned}$$假设环境中有一个或多个特殊的 #终止状态 （Terminal State），当到达终止状态时，一个智能体和环境的交互过程就结束了．这一轮交互的过程称为一个 #回合 （Episode）或试验（Trial）．一般的强化学习任务（比如下棋、游戏）都属于这种 #回合式任务 （Episodic Task）．
如果环境中没有终止状态（比如终身学习的机器人），即 𝑇 = ∞，称为 #持续式任务 （Continuing Task），其总回报也可能是无穷大．为了解决这个问题，我们可以引入一个折扣率来降低远期回报的权重． #折扣回报 （Discounted Return）定义为$$G(\tau)=\sum_{t=0}^{T-1} \gamma^{t} r_{t+1}$$其中 $\gamma \in [0, 1]$ 是折扣率．当 $\gamma$ 接近于0时，智能体更在意短期回报；而当 $\gamma$ 接近于1时，长期回报变得更重要．

## 目标函数
因为策略和状态转移都有一定的随机性，所以每次试验得到的 #轨迹 是一个随机序列，其收获的 #总回报 也不一样．
强化学习的目标是学习到一个策略 ${\pi_\theta(a | s)}$ 来最大化 #期望回报 （Expected Return），即希望智能体执行一系列的动作来获得尽可能多的平均回报．
在 #持续式任务 中，强化学习的优化目标也可以定义为 #MDP 到达平稳分布时 #即时奖励 的期望．

强化学习的目标函数为$$g(\theta)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}[G(\tau)]=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t+1}\right]$$其中 $\theta$ 为策略函数的参数．

## 值函数
为了评估策略 ${\pi}$ 的 #期望回报, 我们定义两个[[值函数]]:  #状态值函数 ${V^{\pi}(s)}$和 #状态-动作值函数 ${Q^{\pi}(s, a)}$.   #值函数 可以看作对策略 $\pi$ 的评估, 因此我们就可以根据值函数来优化策略. 
假设在状态 ${s}$, 有一个动作 ${a^{*}}$ 使得 ${Q^{\pi}\left(s, a^{*}\right)>V^{\pi}(s)}$, 说明执行动作 ${a^{*}}$ 的回报比当前的笨略 ${\pi(a | s)}$ 要高, 我们就可以调整参数使得策略中动作 ${a^{*}}$ 的概率 ${p\left(a^{*} | s\right)}$ 增加.


# 深度强化学习
在强化学习中, 一般需要建模 #策略 ${\pi(a | s)}$ 和 #值函数 ${V^{\pi}(s)}$, ${Q^{\pi}(s, a)}$. 
早期的强化学习算法主要关注状态和动作都是离散且有限的问题, 可以使用表格来记 录这些概率. 但在很多实际问题中, 有些任务的状态和动作的数量非常多. 比如围棋的棋局有 ${3^{361} \approx 10^{170}}$ 种状态, 动作 (即落子位置 ) 数量为 361. 还有些任务的状态和动作是连续的. 比如在自动驾驶中, 智能体感知到的环境状态是各种传感器数据, 一般都是连续的. 动作是操作方向盘的方向和速度控制, 也是连续的. 为了有效地解决这些问题, 我们可以设计一个更强的策略函数 (比如深度神经网络 ), 使得智能体可以应对复杂的环境, 学习更优的策略, 并具有更好的泛化能力.
#深度强化学习 ( Deep Reinforcement Learning ) 是将强化学习和深度学习结合在一起, 用强化学习来定义问题和优化目标, 用深度学习来解决策略和值函数的建模问题, 然后使用误差反向传播算法来优化目标函数. 深度强化学习在一定程度上具备解决复杂问题的通用智能, 并在很多任务上都取得了很大的成功.

*从某种意义上讲， #深度学习 以看作一种强化学习（Reinforcement Learning，RL），每个内部组件并不能直接得到监督信息，需要通过整个模型的最终监督信息（奖励）得到，并且有一定的延时性．*


# 强化学习的学习方法

从 #贝尔曼方程 可知，如果知道 #马尔可夫决策过程 的 #状态转移概率 ${p\left(s{\prime} | s, a\right)}$ 和 #奖励 ${r\left(s, a, s{\prime}\right)}$ ，我们直接可以通过贝尔曼方程来迭代计算其 #值函数 ．这种模型已知的强化学习算法也称为 #基于模型的强化学习 （Model-Based Reinforcement Learning）算法，这里的模型就是指马尔可夫决策过程．
基于模型的强化学习，也叫作 #模型相关的强化学习 ，或 有模型的强化学习 ．
在模型已知时，可以通过[[动态规划算法|动态规划 ]]的方法来计算．

在很多应用场景中，马尔可夫决策过程的状态转移概率 𝑝(𝑠′ |𝑠, 𝑎) 和奖励函数 𝑟(𝑠, 𝑎, 𝑠′) 都是未知的 ．
在这种情况下，我们一般需要智能体和环境进行交互，并收集一些样本，然后再根据这些样本来求解马尔可夫决策过程最优策略．这种模型未知，基于采样的学习算法也称为 #模型无关的强化学习 （Model-Free Reinforcement Learning）算法．

参考 [[基于值函数的学习方法]] 和 [[基于策略函数的学习方法]].


．参见第14.2.2节．
重 要 性 采 样 参 见第11.5.3节．

基于模型的强化学习，也叫作 #模型相关的强化学习 ，或 #有模型的强化学习 ．
模型无关的强化学习也叫作 #无模型的强化学习 ．
