智能体的 #策略 (Policy) 就是智能体如何根据环境 #状态 ${s}$ 来决定下一步的 #动作 ${a}$, 通常可以分为 #确定性策略 ( Deterministic Policy ) 和 #随机性策略 ( Stochastic Policy )两种. 确定性策略是从状态空间到动作空间的映射函数 ${\pi: \mathcal{S} \rightarrow \mathcal{A}}$. 随机性策略表示在给定环境状态时, 智能体选择某个动作的概率分布. $$\begin{aligned} \pi(a \mid s) & \triangleq p(a \mid s) \\ \sum_{a \in \mathcal{A}} \pi(a \mid s) &=1 \end{aligned}$$
通常情况下，强化学习一般使用随机性策略． 随机性策略可以有很多优点：1）在学习时可以通过引入一定随机性更好地探索环境；2）随机性策略的动作具有多样性，这一点在多个智能体博弈时也非常重要．采用确定性策略的智能体总是对同样的环境做出相同的动作，会导致它的策略很容易被对手预测．

## 利用和探索
#利用和探索
如果采用 #确定性策略 𝜋，每次试验得到的轨迹是一样的，只能计算出对于某一状态下固定动作 𝑎 的 #期望回报 ，而无法计算其他动作 𝑎′ 的期望回报，因此也无法进一步改进策略．这样情况仅仅是对当前策略的 #利用 （exploitation），而缺失了对环境的 #探索 （exploration），即试验的轨迹应该尽可能覆盖所有的状态和动作，以找到更好的策略．

## 𝜖-贪心法 
为了平衡利用和探索, 我们可以采用 #𝜖-贪心法 (𝜖-greedy Method). 对于一个目标策略 ${\pi}$, 其对应的 #𝜖-贪心法策略 为 $${ \pi^{\epsilon}(s)=\left\{\begin{array}{cc} \pi(s), & \text{ 按概率 } 1-\epsilon, \\ \text{ 随机选择 } \mathcal{A} \text{ 中的动作 }, &  \text{ 按概率 } \epsilon . \end{array}\right. }$$ 这样, ${\epsilon}$-贪心法将一个仅利用的策略转为带探索的策略. 每次选择动作 ${\pi(s)}$ 的概率为 ${1-\epsilon+\frac{\epsilon}{|\mathcal{A}|}}$, 其他动作的概率为 ${\frac{\epsilon}{|\mathcal{A}|}}$. 

