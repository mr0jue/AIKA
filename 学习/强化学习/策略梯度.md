#策略梯度 （Policy Gradient）是一种基于梯度的强化学习方法．假设$\pi_{\theta}(a \mid s)$是一个关于$\theta$ 的连续可微函数，我们可以用梯度上升的方法来优化参数$\theta$使得目标函数 ${\mathcal{J}(\theta)}$ 最大.
目标函数 ${\mathcal{J}(\theta)}$ 关于策略参数 ${\theta}$ 的导数为 $${\begin{align} 
\frac{\partial \mathcal{J}(\theta)}{\partial \theta} 
&=\frac{\partial}{\partial \theta} \int p_{\theta}(\tau) G(\tau) \mathrm{d} \tau \\ 
&=\int\left(\frac{\partial}{\partial \theta} p_{\theta}(\tau)\right) G(\tau) \mathrm{d} \tau \\ 
&=\int p_{\theta}(\tau)\left(\frac{1}{p_{\theta}(\tau)} \frac{\partial}{\partial \theta} p_{\theta}(\tau)\right) G(\tau) \mathrm{d} \tau \\ 
&=\int p_{\theta}(\tau)\left(\frac{\partial}{\partial \theta} \log p_{\theta}(\tau)\right) G(\tau) \mathrm{d} \tau \\ 
&=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\frac{\partial}{\partial \theta} \log p_{\theta}(\tau) G(\tau)\right] \end{align}}$$ 其中 ${\frac{\partial}{\partial \theta} \log p_{\theta}(\tau)}$ 为函数 ${\log p_{\theta}(\tau)}$ 关于 ${\theta}$ 的偏导数, $G(\tau)=\sum_{t=0}^{T-1} \gamma^{t} r_{t+1}$. 
从公式中可以看出, 参数 ${\theta}$ 优化的方向是使得总回报 ${G(\tau)}$ 越大的轨迹 ${\tau}$ 的概率 ${p_{\theta}(\tau)}$ 也越大. 

${\frac{\partial}{\partial \theta} \log p_{\theta}(\tau)}$ 可以进一步分解为 $${ \begin{align} \frac{\partial}{\partial \theta} \log p_{\theta}(\tau)
&=\frac{\partial}{\partial \theta} \log \left(p\left(s_{0}\right) \prod_{t=0}^{T-1} \pi_{\theta}\left(a_{t} | s_{t}\right) p\left(s_{t+1} | s_{t}, a_{t}\right)\right) \\ 
&=\frac{\partial}{\partial \theta}\left(\log p\left(s_{0}\right)+\sum_{t=0}^{T-1} \log \pi_{\theta}\left(a_{t} | s_{t}\right)+\sum_{t=0}^{T-1} \log p\left(s_{t+1} | s_{t}, a_{t}\right)\right) \\ 
&=\sum_{t=0}^{T-1} \frac{\partial}{\partial \theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right) \end{align} }$$ 可以看出, ${\frac{\partial}{\partial \theta} \log p_{\theta}(\tau)}$ 是和 #状态转移概率 无关, 只和 #策略函数 相关. 因此, 策略梯度 ${\frac{\partial f(\theta)}{\partial \theta}}$ 可写为 $${  \begin{align} 
\frac{\partial \mathcal{J}(\theta)}{\partial \theta} &=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\left(\sum_{t=0}^{T-1} \frac{\partial}{\partial \theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right)\right) G(\tau)\right] \\ 
&=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\left(\sum_{t=0}^{T-1} \frac{\partial}{\partial \theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right)\right)\left(G\left(\tau_{0: t}\right)+\gamma^{t} G\left(\tau_{t: T}\right)\right)\right] \\ 
&=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=0}^{T-1}\left(\frac{\partial}{\partial \theta} \log \pi_{\theta}\left(a_{t} | s_{t}\right) \gamma^{t} G\left(\tau_{t: T}\right)\right)\right],  
\end{align} }$$ 其中 ${G\left(\tau_{t: T}\right)}$ 为从时刻 ${t}$ 作为起始时刻收到的总回报 $${ G\left(\tau_{t: T}\right)=\sum_{t{\prime}=t}^{T-1} \gamma^{t{\prime}-t} r_{t{\prime}+1} }$$