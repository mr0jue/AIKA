## 状态值函数
策略 ${\pi}$ 的 #期望回报 可以分解为 $${ \mathbb{E}_{\tau \sim p(\tau)}[G(\tau)] =\mathbb{E}_{S \sim p\left(s_{0}\right)}\left[\mathbb{E}_{\tau \sim p(\tau)}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t+1} | \tau_{s_{0}}=s\right]\right] \\ =\mathbb{E}_{S \sim p\left(s_{0}\right)}\left[V^{\pi}(s)\right], }$$ 其中 ${V^{\pi}(s)}$ 称为 #状态值函数 ( State Value Function ), 表示从状态 ${s}$ 开始, 执行策 略 ${\pi}$ 得到的 #期望总回报 $${ V^{\pi}(s)=\mathbb{E}_{\tau \sim p(\tau)}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t+1} | \tau_{s_{0}}=s\right], }$$ 其中 ${\tau_{S_{0}}}$ 表示轨迹 ${\tau}$ 的起始状态. 
为了方便起见, 我们用 ${\tau_{0: T}}$ 来表示轨迹 ${s_{0}, a_{0}, s_{1}, \cdots, s_{T}}$, 用 ${\tau_{1: T}}$ 来表示轨迹 ${s_{1}, a_{1}, \cdots, s_{T}}$, 因此有 ${\tau_{0: T}=s_{0}, a_{0}, \tau_{1: T}}$. 根据 #马尔可夫性质, ${V^{\pi}(s)}$ 可展开得到 $${ \begin{array}{l} V^{\pi}(s)&=\mathbb{E}_{\tau_{0: T} \sim p(\tau)}\left[r_{1}+\gamma \sum_{t=1}^{T-1} \gamma^{t-1} r_{t+1} | \tau_{s_{0}}=s\right] \\ &=\mathbb{E}_{a \sim \pi(a | s)} \mathbb{E}_{s{\prime} \sim p\left(s{\prime} | s, a\right)} \mathbb{E}_{\tau_{1: T} \sim p(\tau)}\left[r\left(s, a, s{\prime}\right)+\gamma \sum_{t=1}^{T-1} \gamma^{t-1} r_{t+1} | \tau_{s_{1}}=s{\prime}\right] \\ &=\mathbb{E}_{a \sim \pi(a | s)} \mathbb{E}_{s{\prime} \sim p\left(s{\prime} | s, a\right)}\left[r\left(s, a, s{\prime}\right)+\gamma \mathbb{E}_{\tau_{1: T} \sim p(\tau)}\left[\sum_{t=1}^{T-1} \gamma^{t-1} r_{t+1} | \tau_{s_{1}}=s{\prime}\right]\right] \\ &=\mathbb{E}_{a \sim \pi(a | s)} \mathbb{E}_{s{\prime} \sim p\left(s{\prime} | s, a\right)}\left[r\left(s, a, s{\prime}\right)+\gamma V^{\pi}\left(s{\prime}\right)\right] . \end{array} }$$
其中 $${V^{\pi}(s)}=\mathbb{E}_{a \sim \pi(a | s)} \mathbb{E}_{s{\prime} \sim p\left(s{\prime} | s, a\right)}\left[r\left(s, a, s{\prime}\right)+\gamma V^{\pi}\left(s{\prime}\right)\right]$$称为 #贝尔曼方程 ( Bellman Equation),也叫作 #动态规划方程． 表示当前状态的值函数可以通过下个状态的值函数来计算. 如果给定 #策略 ${\pi(a | s)}$, #状态转移概率 ${p\left(s{\prime} | s, a\right)}$ 和 #奖励 ${r\left(s, a, s{\prime}\right)}$, 我们就可以通过迭代的方式来计算 ${V^{\pi}(s)}$. 由于存在 #折扣率, 迭代一定步数后, 每个状态的值函数就会固定不变. 

## 状态-动作值函数
${V^{\pi}(s)}$ 的贝尔曼方程中的第二个期望 $\mathbb{E}_{s{\prime} \sim p\left(s{\prime} | s, a\right)}$ 是指初始状态为 ${s}$ 并进行动作 ${a}$, 然后执行策略 ${\pi}$ 得到的 #期望总回报, 称为 #状态-动作值函数 ( State-Action Value Function ): $${ Q^{\pi}(s, a)=\mathbb{E}_{s{\prime} \sim p\left(s{\prime} | s, a\right)}\left[r\left(s, a, s{\prime}\right)+\gamma V^{\pi}\left(s{\prime}\right)\right], }$$状态-动作值函数也经常称为 #Q函数 ( Q-Function ). 
状态值函数 ${V^{\pi}(s)}$ 是 ${\mathrm{Q}}$ 函数 ${Q^{\pi}(s, a)}$ 关于动作 ${a}$ 的期望, 即 $${ V^{\pi}(s)=\mathbb{E}_{a \sim \pi(a | s)}\left[Q^{\pi}(s, a)\right] }$$ 结合公式, ${\mathrm{Q}}$ 函数可以写为 $${ Q^{\pi}(s, a)=\mathbb{E}_{s{\prime} \sim p\left(s{\prime} | s, a\right)}\left[r\left(s, a, s{\prime}\right)+\gamma \mathbb{E}_{a{\prime} \sim \pi\left(a{\prime} | s{\prime}\right)}\left[Q^{\pi}\left(s{\prime}, a{\prime}\right)\right]\right] }$$ 这是关于 ${\mathrm{Q}}$ 函数的 #贝尔曼方程.
