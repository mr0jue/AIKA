在模型已知时，可以通过 #动态规划 的方法来计算．常用的方法主要有 #策略迭代算法 和 #值迭代算法 ．

## 策略迭代算法
#策略迭代 （Policy Iteration）算法中，每次迭代可以分为两步：
1) #策略评估 （Policy Evaluation）：计算当前策略下每个状态的值函数，即算法中的 3-6 步．
策略评估可以通过 #贝尔曼方程 进行迭代计算 ${V^{\pi}(s)}$．
如果状态数有限，也可以通过直接求解贝尔曼方程来得到 ${V^{\pi}(s)}$.
2) #策略改进 （Policy Improvement）：根据值函数来更新策略，即算法中的7-8步．
$$
\begin{array}{ll} 
\hline 
& \text{策略迭代算法}\\
\hline 
& \text{输入: MDP 五元组: }{\mathcal{S}, \mathcal{A}, P, r, \gamma}; \\
1& \quad \text{初始化: }{\forall s, \forall a, \pi(a | s)=\frac{1}{|\mathcal{A}|}}; \\
2& \quad \text{repeat }\\
3& \quad \quad \text{repeat }\\
4& \quad \quad \quad \text{计算 } {V^{\pi}(s), \forall s }; \\
5& \quad \quad \text{until } {\forall s, V(s)} \text{ 收敛};\\
6& \quad \quad \text{计算 } {Q(s, a)}; \\
7& \quad \quad {\forall s, \pi(s)=\arg \max _{a} Q(s, a)}; \\
8& \quad \text{until } {\forall s, \pi(s)} \text{收敛 }; \\
& \text{输出: 策略 }{\pi}\\
\hline 
\end{array}
$$

## 值迭代算法
#策略迭代算法 中的策略评估和策略改进是交替轮流进行，其中策略评估也是通过一个内部迭代来进行计算，其计算量比较大．事实上，我们不需要每次计算出每次策略对应的精确的 #值函数 ，也就是说内部迭代不需要执行到完
全收敛．

#值迭代 ( Value Iteration ) 算法将 #策略评估 和 #策略改进 两个过程合并, 来直接计算出最优策略. 最优策略 ${\pi^{*}}$ 对应的值函数称为 #最优值函数 , 其中包括 #最优状态值函数 ${V^{*}(s)}$ 和 #最优状态-动作值函数 ${Q^{*}(s, a)}$, 它们之间的关系为 ${ V^{*}(s)=\max _{a} Q^{*}(s, a) . }$ 根据 #贝尔曼方程, 我们可以通过迭代的方式来计算最优状态值函数 ${V^{*}(s)}$ 和 最优状态-动作值函数 ${Q^{*}(s, a)}$ : $${\begin{array}{l}
V^{*}(s) = \max _{a} \mathbb{E}_{s{\prime} \sim p\left(s{\prime} | s, a\right)}\left[r\left(s, a, s{\prime}\right)+\gamma V^{*}\left(s{\prime}\right)\right], \\
Q^{*}(s, a) = \mathbb{E}_{s{\prime} \sim p\left(s{\prime} | s, a\right)}\left[r\left(s, a, s{\prime}\right)+\gamma \max _{a{\prime}} Q^{*}\left(s{\prime}, a{\prime}\right)\right] \end{array}}$$这两个公式称为 #贝尔曼最优方程 ( Bellman Optimality Equation ). 值迭代算法通过直接优化贝尔曼最优方程, 迭代计算最优值函数. 
$$
\begin{array}{ll} 
\hline 
& \text{值迭代算法}\\
\hline 
& \text{输入: MDP 五元组: }{\mathcal{S}, \mathcal{A}, P, r, \gamma}; \\
1& \quad \text{初始化: }{\forall s \in \mathcal{S}, V(s)=0}; \\
2& \quad \text{repeat }\\
3& \quad \quad {\forall s, V(s) \leftarrow \max _{a} \mathbb{E}_{s{\prime} \sim p\left(s{\prime} | s, a\right)}\left[r\left(s, a, s{\prime}\right)+\gamma V\left(s{\prime}\right)\right]}; \\
4& \quad \text{until } {\forall s, V(s)} \text{ 收敛};\\
5& \quad \text{计算 } {Q(s, a)}; \\
6& \quad {\forall s, \pi(s)=\arg \max _{a} Q(s, a)}; \\
& \text{输出: 策略 }{\pi}\\
\hline 
\end{array}
$$

## 策略迭代算法和值迭代算法的比较
在策略迭代算法中,每次迭代的时间复杂度最大 为 ${O\left(|\mathcal{S}|^{3}|\mathcal{A}|^{3}\right)}$, 最大迭代次数为 ${|\mathcal{A}|^{|\mathcal{S}|}}$. 而在值迭代算法中, 每次迭代的时间复杂度最大为 ${O\left(|\mathcal{S}|^{2}|\mathcal{A}|\right)}$, 但迭代次数要比策略迭代算法更多.

策略迭代算法是根据贝尔曼方程来更新值函数，并根据当前的值函数来改进策略．而值迭代算法是直接使用贝尔曼最优方程来更新值函数，收敛时的值函数就是最优的值函数，其对应的策略也就是最优的策略．

值迭代算法和策略迭代算法都需要经过非常多的迭代次数才能完全收敛．在实际应用中，可以不必等到完全收敛．这样，当状态和动作数量有限时，经过有限次迭代就可以收敛到近似最优策略．

基于模型的强化学习算法实际上是一种 #动态规划 方法．在实际应用中有以下两点限制：
（1）要求模型已知，即要给出马尔可夫决策过程的状态转移概率 𝑝(𝑠′ |𝑠, 𝑎)和奖励函数 𝑟(𝑠, 𝑎, 𝑠′)．但实际应用中这个要求很难满足．如果我们事先不知道模型，那么可以先让智能体与环境交互来估计模型，即估计状态转移概率和奖励函数．一个简单的估计模型的方法为 R-max [Brafman et al., 2002]，通过 #随机游走 的方法来探索环境．每次随机一个策略并执行，然后收集状态转移和奖励的样本．在收集一定的样本后，就可以通过统计或监督学习来重构出 #马尔可夫决策过程 ．但是，这种基于采样的重构过程的复杂度也非常高，只能应用于状态数非常少的场合．
（2）效率问题，即当状态数量较多时，算法效率比较低．但在实际应用中，很多问题的状态数量和动作数量非常多．比如，围棋有 19 × 19 = 361 个位置，每个位置有黑子、白子或无子三种状态，整个棋局有 $3^{361} ≈ 10^{170}$ 种状态．动作（即落子位置）数量为361．不管是值迭代还是策略迭代，以当前计算机的计算能力，根本无法计算．

一种有效的方法是通过一个函数（比如神经网络）来近似计算值函数，以减少复杂度，并提高泛化能力．参见 #深度Q网络 。