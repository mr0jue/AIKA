Q函数 ${Q^{\pi}(s, a)}$ 是初始状态为 ${s}$, 并执行动作 ${a}$ 后所能得到的期望总回报: $${ Q^{\pi}(s, a)=\mathbb{E}_{\tau \sim p(\tau)}\left[G\left(\tau_{s_{0}=s, a_{0}=a}\right)\right], }$$ 其中 ${\tau_{s_{0}=s, a_{0}=a}}$ 表示轨迹 ${\tau}$ 的起始状态和动作为 ${s, a}$. 

如果模型末知, Q函数可以通过采样来进行计算, 这就是 #蒙特卡罗方法. 对于一个策略 ${\pi}$, 智能体从状态 ${s}$, 执行动作 ${a}$ 开始，然后通过 #随机游走 的方法来探索环境, 并计算其得到的总回报. 假设我们进行 ${N}$ 次试验, 得到 ${N}$ 个轨迹 ${\tau^{(1)}, \tau^{(2)}, \cdots, \tau^{(N)}}$, 其总回报分别为 ${G\left(\tau^{(1)}\right), G\left(\tau^{(2)}\right), \cdots, G\left(\tau^{(N)}\right)}$. Q函数可以近似为 $${ Q^{\pi}(s, a) \approx \hat{Q}^{\pi}(s, a)=\frac{1}{N} \sum_{n=1}^{N} G\left(\tau_{s_{0}=s, a_{0}=a}^{(n)}\right) }$$ 当 ${N \rightarrow \infty}$ 时, ${\hat{Q}^{\pi}(s, a) \rightarrow Q^{\pi}(s, a)}$. 在近似估计出Q函数 ${\hat{Q}^{\pi}(s, a)}$ 之后, 就可以进行策略改进. 然后在新的策略下重新通过采样来估计Q函数, 并不断重复, 直至收敛.

蒙特卡罗方法一般需要拿到完整的轨迹，才能对策略进行评估并更新模型，因此效率也比较低．[[时序差分学习方法]]是蒙特卡罗方法的一种改进，