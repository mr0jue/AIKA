Q函数 ${Q^{\pi}(s, a)}$ 是初始状态为 ${s}$, 并执行动作 ${a}$ 后所能得到的期望总回报: $${ Q^{\pi}(s, a)=\mathbb{E}_{\tau \sim p(\tau)}\left[G\left(\tau_{s_{0}=s, a_{0}=a}\right)\right], }$$ 其中 ${\tau_{s_{0}=s, a_{0}=a}}$ 表示轨迹 ${\tau}$ 的起始状态和动作为 ${s, a}$. 

如果模型末知, Q函数可以通过采样来进行计算, 这就是 #蒙特卡罗方法. 对于一个策略 ${\pi}$, 智能体从状态 ${s}$, 执行动作 ${a}$ 开始，然后通过 #随机游走 的方法来探索环境, 并计算其得到的总回报. 
假设我们进行 ${N}$ 次试验, 得到 ${N}$ 个轨迹 ${\tau^{(1)}, \tau^{(2)}, \cdots, \tau^{(N)}}$, 其总回报分别为 ${G\left(\tau^{(1)}\right), G\left(\tau^{(2)}\right), \cdots, G\left(\tau^{(N)}\right)}$. Q函数可以近似为 $${ Q^{\pi}(s, a) \approx \hat{Q}^{\pi}(s, a)=\frac{1}{N} \sum_{n=1}^{N} G\left(\tau_{s_{0}=s, a_{0}=a}^{(n)}\right) }$$ 当 ${N \rightarrow \infty}$ 时, ${\hat{Q}^{\pi}(s, a) \rightarrow Q^{\pi}(s, a)}$. 在近似估计出Q函数 ${\hat{Q}^{\pi}(s, a)}$ 之后, 就可以进行策略改进. 然后在新的策略下重新通过采样来估计Q函数, 并不断重复, 直至收敛.

蒙特卡罗方法一般需要拿到完整的轨迹，才能对策略进行评估并更新模型，因此效率也比较低．

## 同策略 和 异策略 
在蒙特卡罗方法中, 
### 同策略 
如果采样策略是 ${\pi^{\epsilon}(s)}$, 不断改进策略也是 ${\pi^{\epsilon}(s)}$ 而不是目标策略 ${\pi(s)}$. 这种采样与改进策略相同（即都是 ${\pi^{\epsilon}(s)}$ ) 的强化学习方法叫作 #同策略 ( On-Policy ) 方法.

### 异策略 
如果采样策略是 ${\pi^{\epsilon}(s)}$,，而优化目标是策略  ${\pi}$，可以通过 #重要性采样 ，引入重要性权重来实现对目标策略 ${\pi}$ 的优化．这种采样与改进分别使用不同策略的强化学习方法叫作 #异策略 （Off-Policy）方法．

