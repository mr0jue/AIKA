给定 #策略 ${\pi(a | s)}$，智能体和环境一次交互过程的 #轨迹 𝜏 所收到的累积奖励为 #总回报 （Return）．$$\begin{aligned} G(\tau) =\sum_{t=0}^{T-1} r_{t+1}  =\sum_{t=0}^{T-1} r\left(s_{t}, a_{t}, s_{t+1}\right) \end{aligned}$$假设环境中有一个或多个特殊的 #终止状态 （Terminal State），当到达终止状态时，一个智能体和环境的交互过程就结束了．这一轮交互的过程称为一个 #回合 （Episode）或试验（Trial）．一般的强化学习任务（比如下棋、游戏）都属于这种 #回合式任务 （Episodic Task）．
如果环境中没有终止状态（比如终身学习的机器人），即 𝑇 = ∞，称为 #持续式任务 （Continuing Task），其总回报也可能是无穷大．为了解决这个问题，我们可以引入一个折扣率来降低远期回报的权重． #折扣回报 （Discounted Return）定义为$$G(\tau)=\sum_{t=0}^{T-1} \gamma^{t} r_{t+1}$$其中 $\gamma \in [0, 1]$ 是折扣率．当 $\gamma$ 接近于0时，智能体更在意短期回报；而当 $\gamma$ 接近于1时，长期回报变得更重要．

因为策略和 #状态转移概率 都有一定的随机性，所以每次试验得到的 #轨迹 是一个随机序列，其收获的 #总回报 也不一样．

强化学习的 #目标函数 （即 #期望回报 ）为$$g(\theta)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}[G(\tau)]=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t+1}\right]$$其中 $\theta$ 为策略函数的参数，$G(\tau)$为 #折扣回报 ．