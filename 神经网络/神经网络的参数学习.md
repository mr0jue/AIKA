#参数学习
如果采用交叉熵损失函数, 对于样本 ${(\mathbf{x}, y)}$, 其损失函数为 $${ \mathcal{L}(\mathbf{y}, \hat{\mathbf{y}})=-\mathbf{y}^{\mathrm{T}} \log \hat{\mathbf{y}}, }$$ 其中 ${\mathbf{y} \in\{0,1\}^{C}}$ 为标签 ${y}$ 对应的 #one-hot 向量表示。 给定训练集为 ${\mathcal{D}=\left\{\left(\mathbf{x}^{(n)}, y^{(n)}\right)\right\}_{n=1}^{N}}$, 将每个样本 ${\mathbf{x}^{(n)}}$ 输入给前馈神经网络, 得到网络输出为 ${\hat{\mathbf{y}}^{(n)}}$, 其在数据集 ${\mathcal{D}}$ 上的结构化风险函数为: $${ \mathcal{R}(W, \mathbf{b}) =\frac{1}{N} \sum_{n=1}^{N} \mathcal{L}\left(\mathbf{y}^{(n)}, \hat{\mathbf{y}}^{(n)}\right)+\frac{1}{2} \lambda\|W\|_{F}^{2} \ =\frac{1}{N} \sum_{n=1}^{N} \mathcal{L}\left(\mathbf{y}^{(n)}, \hat{\mathbf{y}}^{(n)}\right)+\frac{1}{2} \lambda\|W\|_{F}^{2}, }$$ 其中 ${W}$ 和 ${\mathbf{b}}$ 分别表示网络中所有的权重矩阵和偏置向量; ${\|W\|_{F}^{2}}$ 是正则化项， 用来防止过拟合; ${\lambda}$ 是为正数的超参数 *(注意这里的正则化项只包含权重参数W，而不包含偏置b)*。 ${\lambda}$ 越大, ${W}$ 越接近于 0 。这里的 ${\|W\|_{F}^{2}}$ 一般使用 #Frobenius范数 。
有了学习准则和训练样本, 网络参数可以通过 #梯度下降法 来进行学习。在梯度下降方法的每次迭代中, 第 ${l}$ 层的参数 ${W^{(l)}}$ 和 ${\mathbf{b}^{(l)}}$ 参数更新方式为 $${  
\begin{align} W^{(l)} \leftarrow & W^{(l)}-\alpha \frac{\partial \mathcal{R}(W, \mathbf{b})}{\partial W^{(l)}} \ =W^{(l)}-\alpha\left(\frac{1}{N} \sum_{n=1}^{N}\left(\frac{\partial \mathcal{L}\left(\mathbf{y}^{(n)}, \hat{\mathbf{y}}^{(n)}\right)}{\partial W^{(l)}}\right)+\lambda W^{(l)}\right), \\ \mathbf{b}^{(l)} \leftarrow & \mathbf{b}^{(l)}-\alpha \frac{\partial \mathcal{R}(W, \mathbf{b})}{\partial \mathbf{b}^{(l)}} \ =\mathbf{b}^{(l)}-\alpha\left(\frac{1}{N} \sum_{n=1}^{N} \frac{\partial \mathcal{L}\left(\mathbf{y}^{(n)}, \hat{\mathbf{y}}^{(n)}\right)}{\partial \mathbf{b}^{(l)}}\right), \end{align}}$$ 其中 ${\alpha}$ 为 #学习率 。 
梯度下降法需要计算损失函数对参数的偏导数, 如果通过链式法则逐一对每个参数进行求偏导效率比较低。在神经网络的训练中经常使用 [[反向传播算法]] 来计算高效地梯度。