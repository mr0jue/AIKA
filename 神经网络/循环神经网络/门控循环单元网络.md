#门控循环单元 （Gated Recurrent Unit， #GRU ）网络 [Cho et al., 2014, Chung et al., 2014]是一种比 #LSTM 网络更加简单的 #循环神经网络 。

GRU网络也是在$${ \mathbf{h}_{t}=\mathbf{h}_{t-1}+g\left(\mathbf{x}_{t}, \mathbf{h}_{t-1} ; \theta\right), }$$的基础上，引入 #门机制 来控制信息更新的方式。在LSTM网络中，输入门与和遗忘门是互补关系，用两个门比较冗余。GRU将 #输入门 和  #遗忘门 合并成一个门： #更新门 。同时，GRU也不引入额外的记忆单元，直接在当前状态 ${\mathbf{h}_{t}}$ 和历史状态 ${\mathbf{h}_{t-1}}$ 之间引入线性依赖关系。

在 GRU网络中, 当前时刻的 #候选状态 ${\tilde{\mathbf{h}}_{t}}$ 为 $${ \tilde{\mathbf{h}}_{t}=\tanh \left(W_{h} \mathbf{x}_{t}+U_{h}\left(\mathbf{r}_{t} \odot \mathbf{h}_{t-1}\right)+\mathbf{b}_{h}\right) }$$ 其中 ${\mathbf{r}_{t} \in[0,1]}$ 为 #重置门 (reset gate), 用来控制候选状态 ${\tilde{\mathbf{h}}_{t}}$ 的计算是否依赖 上一时刻的状态 ${\mathbf{h}_{t-1}}$ 。 $${ \mathbf{r}_{t}=\sigma\left(W_{r} \mathbf{x}_{t}+U_{r} \mathbf{h}_{t-1}+\mathbf{b}_{r}\right) }$$公式中的 ${W_{*}, U_{*}, \mathbf{b}_{*}}$ 为可学习的网络参数, 其中 ${* \in\{b, r, z\} 。}$ 

当 ${\mathbf{r}_{t}=0}$ 时, 候选状态 ${\tilde{\mathbf{h}}_{t}=\tanh \left(W_{c} \mathbf{x}_{t}+\mathbf{b}\right)}$只和当前输入 ${\mathbf{x}_{t}}$ 相关, 和历史状态无关。
当 ${\mathbf{r}_{t}=1}$ 时, 候选状态 ${\tilde{\mathbf{h}}_{t}=\tanh \left(W_{h} \mathbf{x}_{t}+U_{h} \mathbf{h}_{t-1}+\mathbf{b}_{h}\right)}$ 和当前输入 ${\mathbf{x}_{t}}$ 和历史状态 ${\mathbf{h}_{t-1}}$ 相关, 和 #简单循环网络 一致。

GRU 网络的隐状态 ${\mathbf{h}_{t}}$ 更新方式为 $${ \mathbf{h}_{t}=\mathbf{z}_{t} \odot \mathbf{h}_{t-1}+\left(\underline{1}-\mathbf{z}_{t}\right) \odot \tilde{\mathbf{h}}_{t} }$$ 其中 ${\mathbf{z} \in[0,1]}$ 为 #更新门 (update gate), 用来控制当前状态需要从历史状态中 保留多少信息 (不经过非线性变换), 以及需要从候选状态中接受多少新信息。 $${ \mathbf{z}_{t}=\sigma\left(\mathbf{W}_{z} \mathbf{x}_{t}+\mathbf{U}_{z} \mathbf{h}_{t-1}+\mathbf{b}_{z}\right) }$$当 ${\mathbf{z}_{t}=0}$ 时, 当前状态 ${\mathbf{h}_{t}}$ 和历史状态 ${\mathbf{h}_{t-1}}$ 之间为非线性函数。
若同时有 ${\mathbf{z}_{t}=0, \mathbf{r}=1}$ 时, GRU网络退化为简单循环网络; 
若同时有 ${\mathbf{z}_{t}=0, \mathbf{r}=0}$ 时, 当前状态 ${\mathbf{h}_{t}}$ 只和当前输入 ${\mathbf{x}_{t}}$ 相关, 和历史状态 ${\mathbf{h}_{t-1}}$ 无关。
当 ${\mathbf{z}_{t}=1}$ 时, 当前状态 ${\mathbf{h}_{t}}$ 等于上一时刻状态 ${\mathbf{h}_{t-1}}$, 和当前输入 ${\mathbf{x}_{t}}$ 无关。 

图给出了 GRU 循环单元结构。
![[GRU循环单元结构.png]]