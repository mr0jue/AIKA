#循环神经网络 在学习过程中的主要问题是长期依赖问题。[Bengio et al., 1994, Hochreiter and Schmidhuber, 1997, Hochreiteret al., 2001]
在 #BPTT 算法中, 将 $${ \delta_{t, k} =\operatorname{diag}\left(f{\prime}\left(\mathbf{z}_{k}\right)\right) U^{\mathrm{T}} \delta_{t, k+1} }$$ 展开得到 $${ \delta_{t, k}=\prod_{i=k}^{t-1}\left(\operatorname{diag}\left(f{\prime}\left(\mathbf{z}_{i}\right)\right) U^{\mathrm{T}}\right) \delta_{t, t} }$$ 如果定义 $${\gamma \cong\left\|\operatorname{diag}\left(f{\prime}\left(\mathbf{z}_{i}\right)\right) U^{\mathrm{T}}\right\|}$$, 则 $${ \delta_{t, k}=\gamma^{t-k} \delta_{t, t} }$$ 

若 ${\gamma>1}$, 当 ${t-k \rightarrow \infty}$ 时, ${\gamma^{t-k} \rightarrow \infty}$, 会造成系统不稳定, 称为 #梯度爆炸 问题 (Gradient Exploding Problem); 
相反, 若 ${\gamma<1}$, 当 ${t-k \rightarrow \infty}$ 时, ${\gamma^{t-k} \rightarrow 0}$, 会出现和深度前馈神经网络类似的 #梯度消失 问题 (gradient vanishing problem)。 

要注意的是, 在循环神经网络中的梯度消失不是说 ${\frac{\partial \mathcal{L}_{t}}{\partial U}}$ 的梯度消失了, 而是 ${\frac{\partial \mathcal{L}_{t}}{\partial \mathbf{h}_{k}}}$ 的梯度消失了（当 ${t-k}$ 比较大时）也就是说, 参数 ${U}$ 的更新主要靠当前时刻 ${k}$ 的几个相邻状态 ${\mathbf{h}_{k}}$ 来更新, 长距离的状态对 ${U}$ 没有影响。 

由于循环神经网络经常使用非线性激活函数为 #Logistic 函数或 #Tanh 函数作 为非线性激活函数, 其导数值都小于 1 ; 并且权重矩阵 ${\|U\|}$ 也不会太大, 因此如果时间间隔 ${t-k}$ 过大, ${\delta_{t, k}}$ 会趋向于 0 , 因此经常会出现梯度消失问题。 虽然简单循环网络理论上可以建立长时间间隔的状态之间的依赖关系, 但是由于梯度爆炸或消失问题, 实际上只能学习到短期的依赖关系。这样, 如果 ${t}$ 时刻的输出 ${y_{t}}$ 依赖于 ${t-k}$ 时刻的输入 ${\mathbf{x}_{t-k}}$, 当间隔 ${k}$ 比较大时, 简单神经网络很难建模这种长距离的依赖关系, 称为 #长期依赖问题 (Long-Term Dependencies Problem）。

为了避免梯度爆炸或消失问题, 一种最直接的方式就是选取合适的参数, 同时使用非饱和的激活函数, 尽量使得 ${\operatorname{diag}\left(f{\prime}\left(\mathbf{z}_{i}\right)\right) U^{\mathrm{T}} \approx 1}$, 这种方式需要足够的人工调参经验, 限制了模型的广泛应用。比较有效的方式是通过 改进模型 或 优化方法 来缓解循环网络的 梯度爆炸 和 梯度消失 问题。 

 ### 梯度爆炸
一般而言, 循环网络的梯度爆炸问题比较容易解决, 一般通过权重衰减或梯度截断来避免。
#权重衰减 是通过给参数增加 ${\ell_{1}}$ 或 ${\ell_{2}}$ 范数的 #正则化 项来限制参数的取值范围, 从而使得 ${\gamma \leq 1}$ 。
#梯度截断 是另一种有效的启发式方法, 当梯度的模大于一定阈值时, 就将它截断成为一个较小的数。 

 ### 梯度消失
梯度消失是循环网络的主要问题。除了使用一些优化技巧外, 更有效的方式就是改变模型, 比如让 ${U=I}$, 同时使用 ${f{\prime}\left(\mathbf{z}_{i}\right)=1}$, 即 $${ \mathbf{h}_{t}=\mathbf{h}_{t-1}+g\left(\mathbf{x}_{t} ; \theta\right) }$$ 其中 ${g(\cdot)}$ 是一个非线性函数, ${\theta}$ 为参数。
其中, ${\mathbf{h}_{t}}$ 和 ${\mathbf{h}_{t-1}}$ 之间为线性依赖关系, 且权重系数为 1, 这样就不存在梯度爆炸或消失问题。但是, 这种改变也丢失了神经元在反馈边上的非线性激活的性质, 因此也降低了模型的表示能力。 为了避免这个缺点, 我们可以采用一个更加有效的改进策略: $${ \mathbf{h}_{t}=\mathbf{h}_{t-1}+g\left(\mathbf{x}_{t}, \mathbf{h}_{t-1} ; \theta\right), }$$ 

这样 ${\mathbf{h}_{t}}$ 和 ${\mathbf{h}_{t-1}}$ 之间为既有线性关系, 也有非线性关系, 在一定程度上可以缓解 梯度消失问题。

 ### 记忆容量问题 
但这种改进依然有一个问题就是 #记忆容量问题 (memory capacity problem) 。随着 ${\mathbf{h}_{t}}$ 不断累积存储新的输入信息, 会发生饱和现象。
假设 ${g(\cdot)}$ 为  logistic 函数, 则随着时间 ${t}$ 的增长, ${\mathbf{h}_{t}}$ 会变得越来越大, 从而导致 ${\mathbf{h}}$ 变得饱和。也就是说, 隐状态 ${\mathbf{h}_{t}}$ 可以存储的信息是有限的, 随着记忆单元存储的内容越来越多, 其丢失的信息也越来越多。 

为了解决记忆容量问题, 可以有两种方法。
一种是增加一些额外的存储单元 ，比如 #记忆增强网络 ;
另一种是非常好的解决方案是引入门控[Hochreiter and Schmidhuber, 1997]来控制信息的累积速度，包括有选择地加入新的信息，并有选择地遗忘之前累积的信息。而这一类网络可以称为 #基于门控的循环神经网络 。

