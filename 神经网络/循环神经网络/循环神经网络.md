在前馈神经网络中，信息的传递是单向的，这种限制虽然使得网络变得更容易学习，但在一定程度上也减弱了神经网络模型的能力。在生物神经网络中，神经元之间的连接关系要复杂的多。前馈神经网络可以看着是一个复杂的函数，每次输入都是独立的，即网络的输出只依赖于当前的输入。但是在很多现实任务中，网络的输入不仅和当前时刻的输入相关，也和其过去一段时间的输出相关。比如一个有限状态自动机，其下一个时刻的状态（输出）不仅仅和当前输入相关，也和当前状态（上一个时刻的输出）相关。此外，前馈网络难以处理时序数据，比如视频、语音、文本等。时序数据的长度一般是不固定的，而前馈神经网络要求输入和输出的维数都是固定的，不能任意改变。因此，为了处理时序数据并利用其历史信息，我们需要让网络具有短期记忆能力。而前馈网络是一个静态网络，不具备这种记忆能力。一般来讲，我们可以通过以方法下来给网络增加短期记忆能力：[[延时神经网络]]、 #有外部输入的非线性自回归模型 和 循环神经网络。和前两者相比，循环神经网络可以更方便地建模长时间间隔的相关性。

#循环神经网络 （Recurrent Neural Network， #RNN ）是一类具有短期记忆能力的神经网络。在循环神经网络中，神经元不但可以接受其它神经元的信息，也可以接受自身的信息，形成具有环路的网络结构。和 #前馈神经网络 相比，循环神经网络更加符合生物神经网络的结构。
图给出了循环神经网络的示例。
![[循环神经网络.png|300]]

给定一个输入序列$x_1:T = (x_1, x_2, . . . , x_t, . . . , x_T)$，循环神经网络通过$h_t = f(h_{t−1}, x_t)$更新带反馈边的隐藏层的活性值$h_t$，其中$h_0 = 0$，$f(·)$为一个非线性函数，也可以是一个前馈网络。
从数学上讲，$h_t = f(h_{t−1}, x_t)$可以看成一个 #动力系统 。因此，隐藏层的活性值$h_t$在很多文献上也称为状态（state）或 #隐状态 （hidden states）。

循环神经网络已经被广泛应用在 #语音识别 、语言模型以及 #自然语言生成 等任务上(参考 [[循环神经网络的应用模式]])。[[循环神经网络的参数学习]]可以通过 #随时间反向传播 算法来学习。
[[简单循环网络]]就是一个非常简单且典型的循环神经网络。理论上，循环神经网络通过使用带自反馈的神经元，能够处理任意长度的时序数据。但是，当输入序列比较长时，会存在 #梯度爆炸 和 #梯度消失 问题，也称为[[长期依赖问题]] 。为了解决这个问题，人们对循环神经网络进行了很多的改进：一种是其中最有效的改进方式为引入 #门机制[Hochreiter and Schmidhuber, 1997]，而这一类网络被称为 #基于门控的循环神经网络 （ #GatedRNN ），其中以[[长短期记忆网络]]（ #LSTM ）和[[门控循环单元网络]]（ #GRU ）最为经典。
当然还有一些其它方法，比如引入 #注意力机制 和 增加 #外部记忆 等。
此外，我们可以将多个循环网络堆叠起来（[[堆叠循环神经网络]]），或者增加一个按照时间的逆序来传递信息的网络层（[[双向循环神经网络]]），来增强网络的能力。

如果将循环神经网络按时间展开，每个时刻的隐状态 $h_t$ 看做一个节点，那么这些节点构成一个链式结构，每个节点$t$都收到其父节点的消息（message），更新自己的状态，并传递给其子节点。而链式结构是一种特殊的图结构，我们可以比较容易地将这种 #消息传递 （message passing）的思想扩展到任意的图结构上，称为 #图网络[Scarselli et al., 2009]。比如[[递归神经网络]]就是一种有向无环图上的图网络。

LSTM网络的 #线性连接 以及 #门机制 是一种十分有效的避免梯度消失问题的方法。这种机制也可以用在深层的前馈网络中，比如 #残差网络[He et al., 2016]和 #高速网络 [Srivastava et al., 2015] 都通过引入 #线性连接 来训练非常深的卷积网络。对于循环神经网格，这种机制也可以用在深度方向上， #GirdLSTM 网络[Kalchbrenner et al., 2015]、 #DepthGatedRNN[Chung et al., 2015]等。

一个完全连接的循环神经网络有着强大的计算和表示能力，可以近似任何非线性动力系统以及图灵机，解决所有的可计算问题：
如果一个完全连接的循环神经网络有足够数量的 sigmoid 型隐藏神经元，它可以以任意的准确率去近似任何一个非线性动力系统。（[[循环神经网络的通用近似定理]]）
所有的 #图灵机 都可以被一个由使用 sigmoid 型激活函数的神经元构成的全连接循环网络来进行模拟（ #图灵完备 ）[Siegelmann and Sontag, 1991]。因此，一个完全连接的循环神经网络可以近似解决所有的可计算问题。

#GatedRNN 使用 #tanh 激活函数是由于其导数有比较大的值域，缓解梯度消失问题。