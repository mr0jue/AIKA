#循环神经网络 的参数可以通过 #梯度下降方法 来进行学习。不失一般性，这里我们以 #同步的序列到序列模式 为例来介绍循环神经网络的参数学习。

以随机梯度下降为例, 给定一个训练样本 ${(\mathbf{x}, \mathbf{y})}$, 其中 ${\mathbf{x}_{1: T}=\left(\mathbf{x}_{1}, \cdots, \mathbf{x}_{T}\right)}$ 为长度是 ${T}$ 的输入序列, ${y_{1: T}=\left(y_{1}, \cdots, y_{T}\right)}$ 是长度为 ${T}$ 的标签序列。即在每个时刻 ${t}$, 都有一个监督信息 ${y_{t}}$, 我们定义时刻 ${t}$ 的损失函数为 $${ \mathcal{L}_{t}=\mathcal{L}\left(y_{t}, g\left(\mathbf{h}_{t}\right)\right) }$$ 其中 ${g\left(\mathbf{h}_{t}\right)}$ 为第 ${t}$ 时刻的输出, ${\mathcal{L}}$ 为可微分的损失函数, 比如交叉熵。那么整个序列上损失函数为 $${ \mathcal{L}=\sum_{t=1}^{T} \mathcal{L}_{t} }$$ 整个序列的损失函数 ${\mathcal{L}}$ 关于参数 ${U}$ 的梯度为 $${ \frac{\partial \mathcal{L}}{\partial U}=\sum_{t=1}^{T} \frac{\partial \mathcal{L}_{t}}{\partial U} }$$ 即每个时刻损失 ${\mathcal{L}_{t}}$ 对参数 ${U}$ 的偏导数之和。


循环神经网络中存在一个递归调用的函数 f(·)，因此其计算参数梯度的方式和前馈神经网络不太相同。在循环神经网络中主要有两种计算梯度的方式：随时间反向传播（BPTT）和实时循环学习（RTRL）算法。

RTRL算法和BPTT算法都是基于梯度下降的算法，分别通过前向模式和反向模式应用链式法则来计算梯度。
在循环神经网络中，一般网络输出维度远低于输入维度，因此BPTT算法的计算量会更小，但是BPTT算法需要保存所有时刻的中间梯度，空间复杂度较高。BPTT计算时间和空间要求会随时间线性增长。为了提高效率，当输入序列的长度比较大时，可以使用带 #截断 （ #truncated ）的BPTT算法[Williams and Peng, 1990]，只计算固定时间间隔内的梯度回传。
RTRL算法不需要梯度回传，因此非常适合用于需要 #在线学习 或无限序列的任务中。



## 随时间反向传播算法

#随时间反向传播 （Backpropagation Through Time， #BPTT ） [Werbos, 1990] 算法的主要思想是通过类似前馈神经网络的 #错误反向传播算法 [Werbos, 1990] 来进行计算梯度。
BPTT算法将循环神经网络看作是一个展开的多层前馈网络，其中“每一层”对应循环网络中的“每个时刻”。
![[按时间展开的循环神经网络.png]]
这样，循环神经网络就可以按按照前馈网络中的反向传播算法进行计算参数梯度。在“展开”的前馈网络中，所有层的参数是共享的，因此参数的真实梯度是将所有“展开层”的参数梯度之和。

### 计算偏导数 ${\frac{\partial \mathcal{L}_{t}}{\partial U}}$ 
先来计算公式 $${ \frac{\partial \mathcal{L}}{\partial U}=\sum_{t=1}^{T} \frac{\partial \mathcal{L}_{t}}{\partial U} }$$中第 ${t}$ 时刻损失对参数 ${U}$ 的偏导数 ${\frac{\partial \mathcal{L}_{t}}{\partial U}}$ 。 因为参数 ${U}$ 和隐藏层在每个时刻 ${k(1 \leq k \leq t)}$ 的净输入 ${\mathbf{z}_{k}=U \mathbf{h}_{k-1}+W \mathbf{x}_{k}+\mathbf{b}}$ 有关, 因此第 ${t}$ 时刻损失的损失函数 ${\mathcal{L}_{t}}$ 关于参数 ${U_{i j}}$ 的梯度为: $${ \frac{\partial \mathcal{L}_{t}}{\partial U_{i j}} =\sum_{k=1}^{t} \operatorname{tr}\left(\left(\frac{\partial \mathcal{L}_{t}}{\partial \mathbf{z}_{k}}\right)^{\mathrm{T}} \frac{\partial^{+} \mathbf{z}_{k}}{\partial U_{i j}}\right) \\ =\sum_{k=1}^{t}\left(\frac{\partial^{+} \mathbf{z}_{k}}{\partial U_{i j}}\right)^{\mathrm{T}} \frac{\partial \mathcal{L}_{t}}{\partial \mathbf{z}_{k}} }$$ 其中 ${\frac{\partial^{+} \mathbf{z}_{k}}{\partial U_{i j}}}$ 表示 “直接” 偏导数, 即公式 ${\mathbf{z}_{k}=U \mathbf{h}_{k-1}+W \mathbf{x}_{k}+\mathbf{b}}$ 中保持 ${\mathbf{h}_{k-1}}$ 不 变, 对 ${U_{i j}}$ 进行求偏导数, 得到 $$
\frac{\partial^{+} \mathbf{z}_{k}}{\partial U_{i j}}=\left[\begin{array}{c}
0 \\
\vdots \\
\left[\mathbf{h}_{k-1}\right]_{j} \\
\vdots \\
0
\end{array}\right] \triangleq \mathbb{h}_{i-1}\left(\left[\mathbf{h}_{k-1}\right]_{j}\right)
$$ 其中 ${\left[\mathbf{h}_{k-1}\right]_{j}}$ 为第 ${k-1}$ 时刻隐状态的第 ${j}$ 维; ${\mathbb{I}_{i}(x)}$ 除了第 ${i}$ 行值为 ${x}$ 外, 其余都 为 0 的向量。 
定义 ${\delta_{t, k}=\frac{\partial \mathcal{L}_{t}}{\partial \mathbf{z}_{k}}}$ 为第 ${t}$ 时刻的损失对第 ${k}$ 时刻隐藏神经层的净输入 ${\mathbf{z}_{k}}$ 的导数, 则 $${ \delta_{t, k} =\frac{\partial \mathcal{L}_{t}}{\partial \mathbf{z}_{k}} \\ =\frac{\partial \mathbf{h}_{k}}{\partial \mathbf{z}_{k}} \frac{\partial \mathbf{z}_{k+1}}{\partial \mathbf{h}_{k}} \frac{\partial \mathcal{L}_{t}}{\partial \mathbf{z}_{k+1}} \\ =\operatorname{diag}\left(f{\prime}\left(\mathbf{z}_{k}\right)\right) U^{\mathrm{T}} \delta_{t, k+1} }$$将 $\delta_{t, k}$ 和 $\frac{\partial^{+} \mathbf{z}_{k}}{\partial U_{i j}}$ 代入 $\frac{\partial \mathcal{L}_{t}}{\partial U_{i j}}$ 得到 $${ \frac{\partial \mathcal{L}_{t}}{\partial U_{i j}}=\sum_{k=1}^{t}\left[\delta_{t, k}\right]_{i}\left[\mathbf{h}_{k-1}\right]_{j} . }$$ 将上式写成矩阵形式为 $${ \frac{\partial \mathcal{L}_{t}}{\partial U}=\sum_{k=1}^{t} \delta_{t, k} \mathbf{h}_{k-1}^{\mathrm{T}} }$$ 
图6.6给出了误差项随时间进行反向传播算法的示例。
![[随时间反向传播算法示例.png]]

### 参数梯度
将 $\frac{\partial \mathcal{L}_{t}}{\partial U}$ 代入到 $\frac{\partial \mathcal{L}}{\partial U}$ 得到整个序列的损失函数 ${\mathcal{L}}$ 关于参 数 ${U}$ 的梯度 $${ \frac{\partial \mathcal{L}}{\partial U}=\sum_{t=1}^{T} \sum_{k=1}^{t} \delta_{t, k} \mathbf{h}_{k-1}^{\mathrm{T}} }$$ 同理可得, ${\mathcal{L}}$ 关于权重 ${W}$ 和偏置 ${\mathbf{b}}$ 的梯度为 $${ \frac{\partial \mathcal{L}}{\partial W} =\sum_{t=1}^{T} \sum_{k=1}^{t} \delta_{t, k} \mathbf{x}_{k}^{\mathrm{T}}, \\ \frac{\partial \mathcal{L}}{\partial \mathbf{b}} =\sum_{t=1}^{T} \sum_{k=1}^{t} \delta_{t, k} . }$$ 
### 计算复杂度
在 BPTT 算法中, 参数的梯度需要在一个完整的 “前向” 计算和 “反向”计算后才能得到并进行参数更新。

## 实时循环学习算法

与反向传播的BPTT算法不同的是， #实时循环学习 （Real-Time Recurrent Learning， #RTRL ）是通过前向传播的方式来计算梯度 [Williams and Zipser, 1995]

假设循环网络网络中第 ${t+1}$ 时刻的状态 ${\mathbf{h}_{t+1}}$ 为 $${ \mathbf{h}_{t+1}=f\left(\mathbf{z}_{t+1}\right)=f\left(U \mathbf{h}_{t}+W \mathbf{x}_{t+1}+\mathbf{b}\right), }$$ 其关于参数 ${U_{i j}}$ 的偏导数为 $$\begin{aligned} \frac{\partial \mathbf{h}_{t+1}}{\partial U_{i j}} &=\frac{\partial \mathbf{h}_{t+1}}{\partial \mathbf{z}_{t+1}}\left(\frac{\partial^{+} \mathbf{z}_{t+1}}{\partial U_{i j}}+U \frac{\partial \mathbf{h}_{t}}{\partial U_{i j}}\right) \\ &=\operatorname{diag}\left(f^{\prime}\left(\mathbf{z}_{t+1}\right)\right)\left(\mathbb{I}_{i}\left(\left[\mathbf{h}_{t}\right]_{j}\right)+U \frac{\partial \mathbf{h}_{t}}{\partial U_{i j}}\right) \\ &=f^{\prime}\left(\mathbf{z}_{t+1}\right) \odot\left(\mathbb{I}_{i}\left(\left[\mathbf{h}_{t}\right]_{j}\right)+U \frac{\partial \mathbf{h}_{t}}{\partial U_{i j}}\right) \end{aligned}$$ 其中 ${\mathbb{I}_{i}(x)}$ 除了第 ${i}$ 行值为 ${x}$ 外, 其余都为 0 的向量。

RTRL 算法从第 1 个时刻开始, 除了计算循环神经网络的隐状态之外, 还利用$f^{\prime}\left(\mathbf{z}_{t+1}\right) \odot\left(\mathbb{I}_{i}\left(\left[\mathbf{h}_{t}\right]_{j}\right)+U \frac{\partial \mathbf{h}_{t}}{\partial U_{i j}}\right)$ 依次前向计算偏导数 ${\frac{\partial \mathbf{h}_{1}}{\partial U_{i j}}, \frac{\partial \mathbf{h}_{2}}{\partial U_{i j}}, \frac{\partial \mathbf{h}_{3}}{\partial U_{i j}}, \cdots}$ 。

这样, 假设第 ${t}$ 个时刻存在一个监督信息, 其损失函数为 ${\mathcal{L}_{t}}$, 就可以同时 计算损失函数对 ${U_{i j}}$ 的偏导数 $${ \frac{\partial \mathcal{L}_{t}}{\partial U_{i j}}=\left(\frac{\partial \mathbf{h}_{t}}{\partial U_{i j}}\right)^{\mathrm{T}} \frac{\partial \mathcal{L}_{t}}{\partial \mathbf{h}_{t}} . }$$ 这样在第 ${t}$ 时刻, 可以实时地计算损失 ${\mathcal{L}_{t}}$ 关于参数 ${U}$ 的梯度, 并更新参数。 数 ${W}$ 和 ${\mathbf{b}}$ 的梯度也可以同样按上述方法实时计算。


