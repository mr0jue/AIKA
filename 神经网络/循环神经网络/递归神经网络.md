#递归神经网络 （Recursive Neural Network， #RecNN ）是循环神经网络在有向无循环图上的扩展 [Pollack, 1990]。递归神经网络的一般结构为树状的层次结构，如图a所示。当递归神经网络的结构退化为线性序列结构（图b）时，递归神经网络就等价于简单循环网络。
![[递归神经网络.png|600]]


以图a中的结构为例, 有三个隐藏层 ${\mathbf{h}_{1} 、 \mathbf{h}_{2}}$ 和 ${\mathbf{h}_{3}}$, 其中 ${\mathbf{h}_{1}}$ 由两个输入 ${\mathbf{x}_{1}}$ 和 ${\mathbf{x}_{2}}$ 计算得到, ${\mathbf{h}_{2}}$ 由另外两个输入层 ${\mathbf{x}_{3}}$ 和 ${\mathbf{x}_{4}}$ 计算得到, ${\mathbf{h}_{3}}$ 由两个隐藏层 ${\mathbf{h}_{1}}$ 和 ${h_{2}}$ 计算得到。 
对于一个节点 ${\mathbf{h}_{i}}$, 它可以接受来自父节点集合 ${\pi_{i}}$ 中所有节点的消息, 并更新自己的状态。 ${ \mathbf{h}_{i}=f\left(\mathbf{h}_{\pi_{i}}\right) }$ 其中 ${\mathbf{h}_{\pi_{i}}}$ 表示集合 ${\pi_{i}}$ 中所有节点状态的拼接, ${f(\cdot)}$ 是一个和节点位置无关的非线性函数, 可以为一个单层的前馈神经网络。比如图a 所示的递归神经网络具体可以写为 $${ \begin{array}{l} \mathbf{h}_{1}=\sigma\left(W\left[\begin{array}{l} \mathbf{x}_{1} \\ \mathbf{x}_{2} \end{array}\right]+\mathbf{b}\right) \\ \mathbf{h}_{2}=\sigma\left(W\left[\begin{array}{l} \mathbf{x}_{3} \\ \mathbf{x}_{4} \end{array}\right]+\mathbf{b}\right) \\ \mathbf{h}_{3}=\sigma\left(W\left[\begin{array}{l} \mathbf{h}_{1} \\ \mathbf{h}_{2} \end{array}\right]+\mathbf{b}\right) \end{array} }$$
其中 ${\sigma(\cdot)}$ 表示非线性激活函数, ${W}$ 和 ${\mathbf{b}}$ 是可学习的参数。同样, 输出层 ${y}$ 可以为一个分类器, 比如 $${ y=g\left(W{\prime}\left[\begin{array}{l} \mathbf{h}_{1} \\ \mathbf{h}_{2} \end{array}\right]+\mathbf{b}{\prime}\right), }$$ 其中 ${g(\cdot)}$ 为分类器, ${W{\prime}}$ 和 ${\mathbf{b}{\prime}}$ 为分类器的参数。



递归神经网络主要用来建模自然语言句子的语义[Socher et al., 2011, 2013]。给定一个句子的语法结构（一般为树状结构），可以使用递归神经网络来按照句法的组合关系来合成一个句子的语义。句子中每个短语成分可以在分成一些子成分，即每个短语的语义都可以由它的子成分语义组合而来，并进而合成整句的语义。

同样，我们也可以用 #门机制 来改进递归神经网络中的 #长距离依赖问题 ，比如 #树结构的长短期记忆模型 （ #Tree-StructuredLSTM）[Tai et al., 2015, Zhu et al., 2015]就是将LSTM模型的思想应用到树结构的网络中，来实现更灵活的组合函数。