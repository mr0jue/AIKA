#长短期记忆 （Long Short-Term Memory， #LSTM ）网络[Gers et al., 2000, Hochreiter and Schmidhuber, 1997]是 #循环神经网络 的一个变体，可以有效地解决简单循环神经网络的梯度爆炸或消失问题。
## 主要改进
在$${ \mathbf{h}_{t}=\mathbf{h}_{t-1}+g\left(\mathbf{x}_{t}, \mathbf{h}_{t-1} ; \theta\right), }$$的基础上，LSTM网络主要改进在 新的内部状态 和 门机制 两个方面

### 新的内部状态
LSTM网络引入一个新的 #内部状态 (internal state) ${\mathbf{c}_{t}}$ 专门进行线性的循环信息传递, 同时 (非线性) 输出信息给隐藏层的外部状态 ${\mathbf{h}_{t}}$ 。 $${ \begin{align} \mathbf{c}_{t} =&\mathbf{f}_{t} \odot \mathbf{c}_{t-1}+\mathbf{i}_{t} \odot \tilde{\mathbf{c}}_{t} \\ \mathbf{h}_{t} =&\mathbf{o}_{t} \odot \tanh \left(\mathbf{c}_{t}\right) \end{align}}$$其中 ${\mathbf{f}_{t}, \mathbf{i}_{t}}$ 和 ${\mathbf{o}_{t}}$ 为三个 门 (gate) 来控制信息传递的路径; ${\odot}$ 为向量元素乘积; ${\mathbf{c}_{t-1}}$ 为上一时刻的记忆单元; ${\tilde{\mathbf{c}}_{t}}$ 是通过非线性函数得到 #候选状态, $${ \tilde{\mathbf{c}}_{t}=\tanh \left(W_{c} \mathbf{x}_{t}+U_{c} \mathbf{h}_{t-1}+\mathbf{b}_{c}\right) }$$在每个时刻 ${t}$ ，LSTM网络的内部状态 ${\mathbf{c}_{t}}$ 记录了到当前时刻为止的历史信息。
公式中的 ${W_{*}, U_{*}, \mathbf{b}_{*}}$ 为可学习的网络参数, 其中 ${* \in\{i, f, o, c\} 。}$ 

### 门机制
LSTM 网络引入 #门机制 (gating mechanism) 来控制信息传递的路径。 公式中三个 “门” 分别为 #输入门 ${\mathbf{i}_{t}}$, #遗忘门 ${\mathbf{f}_{t}}$ 和 #输出门 ${\mathbf{o}_{t}}$,

在数字电路中，门（gate）为一个二值变量{0, 1}，0代表关闭状态，不许任何信息通过；1代表开放状态，允许所有信息通过。LSTM网络中的“门”是一种“软”门，取值在(0, 1)之间，表示以一定的比例运行信息通过。LSTM网络中三个门的作用为：
- 遗忘门 ${\mathbf{f}_{t}}$ 控制上一个时刻的内部状态 ${\mathbf{c}_{t-1}}$ 需要遗忘多少信息。 
- 输入门 ${\mathbf{i}_{t}}$ 控制当前时刻的候选状态 ${\tilde{\mathbf{c}}_{t}}$ 有多少信息需要保存。
- 输出门 ${\mathbf{o}_{t}}$ 控制当前时刻的内部状态 ${\mathbf{c}_{t}}$ 有多少信息需要输出给外部状态 ${\mathbf{h}_{t}}$ 。

当 ${\mathbf{f}_{t}=0, \mathbf{i}_{t}=1}$ 时, 记忆单元将历史信息清空, 并将候选状态向量 ${\tilde{\mathbf{c}}_{t}}$ 写入。但此时记忆单元 ${\mathbf{c}_{t}}$ 依然和上一时刻的历史信息相关。
当 ${\mathbf{f}_{t}=1, \mathbf{i}_{t}=0}$ 时, 记忆单元将复制上一时刻的内容, 不写入新的信息。 
三个门的计算方式为: $${\begin{align} \mathbf{i}_{t} = &\sigma\left(W_{i} \mathbf{x}_{t}+U_{i} \mathbf{h}_{t-1}+\mathbf{b}_{i}\right) \\ \mathbf{f}_{t} = &\sigma\left(W_{f} \mathbf{x}_{t}+U_{f} \mathbf{h}_{t-1}+\mathbf{b}_{f}\right) \\ \mathbf{o}_{t} = &\sigma\left(W_{o} \mathbf{x}_{t}+U_{o} \mathbf{h}_{t-1}+\mathbf{b}_{o}\right) \end{align}}$$ 其中 ${\sigma(\cdot)}$ 为 logistic 函数, 其输出区间为 ${(0,1), \mathbf{x}_{t}}$ 为当前时刻的输入, ${\mathbf{h}_{t-1}}$ 为 上一时刻的外部状态。 图给出了 LSTM网络的循环单元结构, 其计算过程为:
1）首先利用 上一时刻的外部状态 ${\mathbf{h}_{t-1}}$ 和当前时刻的输入 ${\mathbf{x}_{t}}$, 计算出三个门, 以及候选状态 ${\tilde{\mathbf{c}}_{t}}$; 
2) 结合遗忘门 ${\mathbf{f}_{t}}$ 和输入门 ${\mathbf{i}_{t}}$ 来更新记忆单元 ${\mathbf{c}_{t}}$; (3) 结合输出门 ${\mathbf{o}_{t}}$, 将 内部状态的信息传递给外部状态 ${\mathbf{h}_{t}}$ 。
![[LSTM循环单元结构.png]]

通过LSTM循环单元，整个网络可以建立较长距离的时序依赖关系。LSTM公式可以简洁地描述为


$$
\begin{aligned} 
{
\left[
	\begin{array}{c} 
	\tilde{\mathbf{c}}_{t} \\
	\mathbf{o}_{t} \\ 
	\mathbf{i}_{t} \\ 
	\mathbf{f}_{t} 
	\end{array}
\right] } &
=
\left[
	\begin{array}{c} 
	\tanh \\ 
	\sigma \\ 
	\sigma \\ 
	\sigma 
	\end{array}
\right]
\left(
	\mathbf{w}
	\left[
		\begin{array}{c} 
		\mathbf{x}_{t} \\
		\mathbf{h}_{t-1} 
		\end{array}
	\right] 
	+\mathbf{b}
\right), \\ 
\mathbf{c}_{t} &=\mathbf{f}_{t} \odot \mathbf{c}_{t-1}+\mathbf{i}_{t} \odot \tilde{\mathbf{c}}_{t}, \\ 
\mathbf{h}_{t} &=\mathbf{o}_{t} \odot \tanh \left(\mathbf{c}_{t}\right), 
\end{aligned}$$
其中 ${\mathbf{x}_{t} \in \mathbb{R}^{e}}$ 为当前时刻的输入, ${W \in \mathbb{R}^{4 d \times(d+e)}}$ 和 ${\mathbf{b} \in \mathbb{R}^{4 d}}$ 为网络参数。

## 记忆
循环神经网络中的隐状态$h$存储了历史信息，可以看作是一种 #记忆 （memory）。在简单循环网络中，隐状态每个时刻都会被重写，因此可以看作是一种 #短期记忆 （short-term memory）。在神经网络中， #长期记忆 （long-term memory）可以看作是网络参数，隐含了从训练数据中学到的经验，并更新周期要远远慢于短期记忆。而在 LSTM 网络中，记忆单元$c$可以在某个时刻捕捉到某个关键信息，并有能力将此关键信息保存一定的时间间隔。记忆单元$c$中保存信息的生命周期要长于短期记忆$h$，但又远远短于长期记忆，因此称为 #长的短期记忆 （long short-term memory）。

## 参数初始化
一般在深度网络参数学习时，参数初始化的值一般都比较小。但是在训练LSTM网络时，过小的值会使得遗忘门的值比较小。这意味着前一时刻的信息大部分都丢失了，这样网络很难捕捉到长距离的依赖信息。并且相邻时间间隔的梯度会非常小，这会导致 #梯度弥散问题 。因此遗忘的参数初始值一般都设得比较大，其偏置向量$\mathbf{b}_f$ 设为1或2.

## LSTM网络的各种变体
目前主流的LSTM网络用的三个门来动态地控制内部状态的应该遗忘多少历史信息，输入多少新信息，以及输出多少信息。我们可以对门机制进行改进并获得LSTM网络的各种变体。

### 无遗忘门的LSTM 网络
Hochreiter and Schmidhuber [1997]最早提出的LSTM 网络是没有遗忘门的, 其内部状态的更新为 $${ \mathbf{c}_{t}=\mathbf{c}_{t-1}+\mathbf{i}_{t} \odot \tilde{\mathbf{c}}_{t} }$$ 如之前的分析, 记忆单元 ${\mathbf{c}}$ 会不断增大。当输入序列的长度非常大时, 记忆单元的容量会饱和, 从而大大降低 LSTM模型的性能。 
### peephole 连接
另外一种变体是三个门不但依赖于输入 ${\mathrm{x}_{t}}$ 和上一时刻的隐状态 ${\mathbf{h}_{t-1}}$, 也依赖于上一个时刻的记忆单元 ${\mathbf{c}_{t-1}}$ 。 $${\begin{aligned} 
\mathbf{i}_{t} &=\sigma\left(W_{i} \mathbf{x}_{t}+U_{i} \mathbf{h}_{t-1}+V_{i} \mathbf{c}_{t-1}+\mathbf{b}_{i}\right) \\ \mathbf{f}_{t} &=\sigma\left(W_{f} \mathbf{x}_{t}+U_{f} \mathbf{h}_{t-1}+V_{f} \mathbf{c}_{t-1}+\mathbf{b}_{f}\right) \\ \mathbf{o}_{t} &=\sigma\left(W_{o} \mathbf{x}_{t}+U_{o} \mathbf{h}_{t-1}+V_{o} \mathbf{c}_{t}+\mathbf{b}_{o}\right) 
\end{aligned}}$$ 其中 ${V_{i}, V_{f}}$ 和 ${V_{o}}$ 为对角阵形式的参数。 
### 耦合输入门和遗忘门
LSTM网络中的输入门和遗忘门有些互补关系, 因此同时用两个门比较冗余。为了减少 LSTM 网络的计算复杂度, 将这两门合并为一个门。令 ${ \mathbf{f}_{t}=\mathbf{1}-\mathbf{i}_{t} . }$ 这样, 内部状态的更新方式为 $${ \mathbf{c}_{t}=\left(\mathbf{1}-\mathbf{i}_{t}\right) \odot \mathbf{c}_{t-1}+\mathbf{i}_{t} \odot \tilde{\mathbf{c}}_{t} }$$
## 后记
LSTM网络是目前为止最成功的循环神经网络模型，成功应用在很多领域，比如语音识别、机器翻译[Sutskever et al., 2014]、语音模型以及文本生成。LSTM网络通过引入线性连接来缓解长距离依赖问题。虽然LSTM网络取得了很大的成功，其结构的合理性一直受到广泛关注。人们不断有尝试对其进行改进来寻找最优结构，比如减少门的数量、提高并行能力等。关于LSTM网络的分析可以参考文献[Greffff et al., 2017, Jozefowicz et al., 2015, Karpathy et al., 2015]。
