如果将深度定义为网络中信息传递路径长度的话， #循环神经网络 可以看作是既“深”又“浅”的网络。一方面来说，如果我们把循环网络按时间展开，长时间间隔的状态之间的路径很长，循环网络可以看作是一个非常深的网络了。从另一方面来说，如果同一时刻网络输入到输出之间的路径 $\mathbf{x}_{t} \rightarrow \mathbf{h}_{t}$ ，这个网络是非常浅的。
因此，我们可以增加循环神经网络的深度从而增强循环神经网络的能力。增加循环神经网络的深度主要是增加同一时刻网络输入到输出之间的路径 $\mathbf{x}_{t} \rightarrow \mathbf{h}_{t}$，比如增加隐状态到输出 $\mathbf{h}_{t} \rightarrow \mathbf{y}_{t}$，以及输入到隐状态 $\mathbf{x}_{t} \rightarrow \mathbf{h}_{t}$ 之间的路径的深度。

一种常见的做法是将多个循环网络堆叠起来，称为 #堆叠循环神经网络 （Stacked Recurrent Neural Network， #SRNN ）。一个堆叠的简单循环网络（stacked SRN）也称为 #循环网络循环多层感知器 （Recurrent Multi-Layer Perceptron， #RMLP ）[Parlos et al. 1991]

叠循环神经网络 第 ${l}$ 层网络的输入是第 ${l-1}$ 层网络的输出。我们定义 ${\mathbf{h}_{t}^{(l)}}$ 为在时刻 ${t}$ 时第 ${l}$ 层的隐状态 $${ \mathbf{h}_{t}^{(l)}=f\left(U^{(l)} \mathbf{h}_{t-1}^{(l)}+W^{(l)} \mathbf{h}_{t}^{(l-1)}+\mathbf{b}^{(l)}\right) }$$ 其中 ${U^{(l)}, W^{(l)}}$ 和 ${\mathbf{b}^{(l)}}$ 为权重矩阵和偏置向量, ${\mathbf{h}_{t}^{(0)}=\mathbf{x}_{t}}$ 。

图给出了按时间展开的堆叠循环神经网络。
![[按时间展开的堆叠循环神经网络.png]]