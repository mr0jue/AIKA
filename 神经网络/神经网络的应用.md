根据通用近似定理, 神经网络在某种程度上可以作为一个“万能”函数来使用, 可以用来进行复杂的特征转换, 或逼近一个复杂的条件分布。
在机器学习中, 输入样本的特征对分类器的影响很大。以监督学习为例, 好 的特征可以极大提高分类器的性能。因此, 要取得好的分类效果, 需要样本的原始特征向量 ${\mathbf{x}}$ 转换到更有效的特征向量 ${\varphi(\mathbf{x})}$, 这个过程叫做 #特征抽取 。
多层前馈神经网络可以看作是一个非线性复合函数 ${\varphi: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d{\prime}}}$, 将输入 ${\mathbf{x} \in \mathbb{R}^{d}}$ 映射到输出 ${\varphi(\mathbf{x}) \in \mathbb{R}^{d{\prime}}}$ 。因此, 多层前馈神经网络也可以看成是一种特征转换方法, 其输出 ${\varphi(\mathrm{x})}$ 作为分类器的输入进行分类。
给定一个训练样本 ${(\mathbf{x}, y)}$, 先利用多层前馈神经网络将 ${\mathbf{x}}$ 映射到 ${\varphi(\mathbf{x})}$, 然 后再将 ${\varphi(\mathbf{x})}$ 输入到分类器 ${g(\cdot)}$ 。 $${ \hat{y}=g(\varphi(\mathbf{x}), \theta) }$$ 其中 ${g(\cdot)}$ 为线性或非线性的分类器, ${\theta}$ 为分类器 ${g(\cdot)}$ 的参数, ${\hat{y}}$ 为分类器的输出。 

特别地, 如果分类器 ${g(\cdot)}$ 为 #Logistic 回归分类器或 #softmax 回归分类器, 那 么 ${g(\cdot)}$ 也可以看成是网络的最后一层, 即神经网络直接输出不同类别的后验概率。
### 两类分类问题
对于两类分类问题 ${y \in\{0,1\}}$, 并采用Logistic 回归, 那么 Logistic 回归分类器可以看成神经网络的最后一层。也就是说, 网络的最后一层只用一个神经元, 并且其激活函数为Logistic函数。网络的输出可以直接可以作为类别 ${y=1}$ 的后验概率。 $${ p(y=1 | \mathbf{x})=a^{(L)} }$$ 其中 ${a^{(L)} \in \mathbb{R}}$ 为第 ${L}$ 层神经元的活性值。 
### 多类分类问题
对于多类分类问题 ${y \in\{1, \cdots, C\}}$, 如果使用 softmax 回归分类器, 相当于网络最后一层设置 ${C}$ 个神经元, 其激活函数为 softmax 函数。网络的输出可以作为每个类的后验概率。 $${ \hat{\mathbf{y}}=\operatorname{softmax}\left(\mathbf{z}^{(L)}\right), }$$
其中 ${\mathbf{z}^{(L)} \in \mathbb{R}}$ 为第 ${L}$ 层神经元的净输入; ${\hat{\mathbf{y}} \in \mathbb{R}^{C}}$ 为第 ${L}$ 层神经元的活性值, 分别是不同类别标签的预测后验概率。 
