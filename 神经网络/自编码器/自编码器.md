#自编码器 （ #Auto-Encoder ， #AE ）是通过无监督的方式来学习一组数据的有效编码（或表示）。

假设有一组 ${d}$ 维的样本 ${\mathbf{x}^{(n)} \in \mathbb{R}^{d}, 1 \leq n \leq N}$, 自编码器将这组数据映射到 特征空间得到每个样本的编码 ${\mathbf{z}^{(n)} \in \mathbb{R}^{p}, 1 \leq n \leq N}$, 并且希望这组编码可以 重构出原来的样本。 
自编码器的结构可分为两部分: #编码器 (encoder) ${ f: \mathbb{R}^{d} \rightarrow \mathbb{R}^{p} }$ 和 #解码器 (decoder) ${ g: \mathbb{R}^{p} \rightarrow \mathbb{R}^{d} . }$ 
自编码器的学习目标是 #最小化重构错误 (reconstruction errors) $${ \mathcal{L} =\sum_{n=1}^{N}\left\|\mathbf{x}^{(n)}-g\left(f\left(\mathbf{x}^{(n)}\right)\right)\right\|^{2} \\ =\sum_{n=1}^{N}\left\|\mathbf{x}^{(n)}-f \circ g\left(\mathbf{x}^{(n)}\right)\right\|^{2} }$$
如果特征空间的维度p小于原始空间的维度d，自编码器相当于是一种[[降维]]或特征抽取方法。如果p ≥ d，一定可以找到一组或多组解使得 f ◦ g 为 #单位函数 （Identity Function）(单位函数I(x) = x)，并使得重构错误为0。但是，这样的解并没有太多的意义。

但是如果再加上一些附加的约束，就可以得到一些有意义的解，比如编码的 稀疏性、取值范围，f 和g 的具体形式等。如果我们让编码只能取k 个不同的值(k < N)，那么自编码器就可以转换为一个k 类的聚类（Clustering）问题。

最简单的自编码器是如图9.2所示的两层神经网络。输入层到隐藏层用来编码，隐藏层到输出层用来解码，层与层之间互相全连接。
![[两层网络结构的自编码器.png|250]]

对于样本 ${\mathbf{x}}$, 中间隐藏层为编码 ${ \mathbf{z}=s\left(W^{(1)} \mathbf{x}+b^{(1)}\right) }$ ，输出为重构的数据 ${ \mathbf{x}{\prime}=s\left(W^{(2)} \mathbf{z}+b^{(2)}\right) }$ ，其中 ${W, b}$ 为网络参数, ${s(\cdot)}$ 为激活函数。
如果令 ${W^{(2)}}$ 等于 ${W^{(1)}}$ 的转置, 即 ${W^{(2)}=}$ ${W^{(1)^{\mathrm{T}}}}$, 称为 #捆绑权重 (tied weights)。 
给定一组样本 ${\mathbf{x}^{(n)} \in[0,1]^{d}, 1 \leq n \leq N}$, 其重构错误为$${ \left.\mathcal{L}=\sum_{n=1}^{N} \| \mathbf{x}^{(n)}-\mathbf{x}^{\prime(n)}\right)\left\|^{2}+\lambda\right\| W \|_{F}^{2} }$$ 其中 ${\lambda}$ 为正则化项系数。通过最小化重构错误，可以有效地学习刚络的参数。

我们使用自编码器是为了得到有效的数据表示，因此在训练结束后，我们一般去掉解码器，只保留编码器。编码器的输出可以直接作为后续机器学习模型的输入。

常用的自编码器模型有[[稀疏自编码器]]／[[堆叠自编码器]]／[[降噪自编码器]]。