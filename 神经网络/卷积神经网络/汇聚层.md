#汇聚层 （ #PoolingLayer ）也叫 #子采样层 （Subsampling Layer），其作用是进行 #特征选择 ，降低特征数量，并从而减少参数数量。

#卷积层 虽然可以显著减少网络中连接的数量，但特征映射组中的神经元个数并没有显著减少。如果后面接一个分类器，分类器的输入维数依然很高，很容易出现过拟合。为了解决这个问题，可以在卷积层之后加上一个汇聚层，从而降低特征维数，避免 #过拟合 。

假设汇聚层的输入特征映射组为 ${\mathbf{X} \in \mathbb{R}^{M \times N \times D}}$, 对于其中每一个特征映射 ${X^{d}}$, 将其划分为很多区域 ${R_{m, n}^{d}, 1 \leq m \leq M{\prime}, 1 \leq n \leq N{\prime}}$, 这些区域可以重叠, 也可以不重叠。 #汇聚 ( #Pooling) 是指对每个区域进行 #下采样 (Down Sampling) 得到一个值, 作为这个区域的概括。 

常用的汇聚函数有两种: 
1) #最大汇聚 （Maximum Pooling, #MaxPooling ）：一般是取一个区域内所有神经元的最大值。 $${ Y_{m, n}^{d}=\max _{i \in R_{m, n}^{d}} x_{i} }$$ 其中 ${x_{i}}$ 为区域 ${R_{k}^{d}}$ 内每个神经元的激活值。 
2) #平均汇聚 （Mean Pooling, #AvgPooling ）：一般是取区域内所有神经元的平均值。 $${ Y_{m, n}^{d}=\frac{1}{\left|R_{m, n}^{d}\right|} \sum_{i \in R_{m, n}^{d}} x_{i} . }$$ 对每一个输入特征映射 ${X^{d}}$ 的 ${M{\prime} \times N{\prime}}$ 个区域进行子采样, 得到汇聚层的输出特征映射 $${Y^{d}=\left\{Y_{m, n}^{d}\right\}, 1 \leq m \leq M{\prime}, 1 \leq n \leq N{\prime}}.$$
![[汇聚层中最大汇聚过程示例.png|600]]

目前主流的卷积网络中, 汇聚层仅包含下采样操作。但在早期的一些卷积网络 (比如 #LeNet-5) 中, 有时也会在汇聚层使用非线性激活函数, 比如 $${ Y^{\prime d}=f\left(w^{d} \cdot Y^{d}+b^{d}\right), }$$ 其中 ${Y^{\prime d}}$ 为汇聚层的输出, ${f(\cdot)}$ 为非线性激活函数, ${w^{d}}$ 和 ${b^{d}}$ 为可学习的标量权重和偏置。

典型的汇聚层是将每个特征映射划分为 2 × 2 大小的不重叠区域，然后使用最大汇聚的方式进行下采样．
汇聚层也可以看作一个特殊的卷积层，卷积核大小为 𝐾 × 𝐾，步长为 𝑆 × 𝑆，卷积核为max函数或 mean函数．过大的采样区域会急剧减少神经元的数量，也会造成过多的信息损失．