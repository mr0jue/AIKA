## 滑动步长和零填充
在 #卷积 的标准定义基础上，还可以引入滤波器的 #滑动步长 和零填充来增加卷积的多样性，可以更灵活地进行特征抽取。

滤波器的 #步长 （ #Stride ）是指滤波器在滑动时的时间间隔。图a给出了步长为2的卷积示例。
#零填充 （ #ZeroPadding ）是在输入向量两端进行补零。图b给出了输入的两端各补一个零后的卷积示例。
![[卷积的步长和零填充.png|250]]

假设卷积层的输入神经元个数为 ${n}$, 卷积大小为 ${m}$, 步长 (stride) 为 ${s}$, 输 入神经元两端各填补 ${p}$ 个零 (zero padding), 那么该卷积层的神经元数量为 ${(n-m+2 p) / s+1}$ 。通常可以通过选择合适的卷积大小以及步长来使得 ${(n-m+2 p) / s+1}$ 为整数。
一般常用的卷积有以下三类：
- #窄卷积 (Narrow Convolution): 步长 ${s=1}$, 两端不补零 ${p=0}$, 卷积后 输出长度为 ${n-m+1}$ 。 
- #宽卷积 (Wide Convolution) : 步长 ${s=1}$, 两端补零 ${p=m-1}$, 卷积后 输出长度 ${n+m-1}$ 。
- #等宽卷积 (Equal-Width Convolution) : 步长 ${s=1}$, 两端补零 ${p=(m-}$ ${1) / 2}$, 卷积后输出长度 ${n}$ 。图b就是一个等长卷积示例。

图给出了一些二维卷积的示例，其中m表示卷积核大小，p 表示零填充大小（zero-padding）， s 表示步长（stride）。
m = 3, p = 0 ,s = 1 ![[cnn-in_5_out_3_p_0_s_0.gif|250]]m = 3, p = 2 ,s = 2![[cnn-in_5_out_4_p_2_s_2.gif|250]] m = 3, p = 2 ,s = 1![[cnn-in_3_out_5.gif|250]]m=5, p = 2 ,s = 2![[cnn-in_9_out_5.gif|250]] 

## 转置卷积

我们一般可以通过卷积操作来实现高维特征到低维特征的转换．比如在一维卷积中，一个5维的输入特征，经过一个大小为3的卷积核，其输出为3维特征．如果设置步长大于1，可以进一步降低输出特征的维数．但在一些任务中，我们需要将低维特征映射到高维特征，并且依然希望通过卷积操作来实现．

假设有一个高维向量为 ${\boldsymbol{x} \in \mathbb{R}^{d}}$ 和一个低维向量为 ${\boldsymbol{z} \in \mathbb{R}^{p}, p<d}$. 如果用 #仿射变换 (Affine Transformation) 来实现高维到低维的映射, $${ z=W \boldsymbol{x} }$$ 其中 ${\boldsymbol{W} \in \mathbb{R}^{p \times d}}$ 为转换矩阵. 我们可以很容易地通过转置 ${\boldsymbol{W}}$ 来实现低维到高维的反向映射, 即 $${ \boldsymbol{x}=\boldsymbol{W}^{\top} \boldsymbol{z} }.$$ 需要说明的是,  ${ z=W \boldsymbol{x} }$ 和 ${ \boldsymbol{x}=\boldsymbol{W}^{\top} \boldsymbol{z} }$ 并不是逆运算, 两个映射只是形式上的转置关系. 

在全连接网络中, 忽略激活函数, 前向计算和反向传播就是一种转置关系. 比如前向计算时, 第 ${l+1}$ 层的净输入为 ${\boldsymbol{z}^{(l+1)}=\boldsymbol{W}^{(l+1)} \boldsymbol{z}^{(l)}}$, 反向传播时, 第 ${l}$ 层 的误差项为 $${\delta^{(l)}=\left(\boldsymbol{W}^{(l+1)}\right)^{\top} \delta^{(l+1)}}.$$
卷积操作也可以写为仿射变换的形式. 假设一个 5 维向量 ${\boldsymbol{x}}$, 经过大小为 3 的卷积核 ${\boldsymbol{w}=\left[w_{1}, w_{2}, w_{3}\right]^{\top}}$ 进行卷积, 得到 3 维向量 ${\boldsymbol{z}}$. 卷积操作可以写为 $${ \boldsymbol{z} =\boldsymbol{w} \boldsymbol{\otimes} \boldsymbol{x} \\
=\left[\begin{array}{ccccc}
w_{1} & w_{2} & w_{3} & 0 & 0 \\
0 & w_{1} & w_{2} & w_{3} & 0 \\
0 & 0 & w_{1} & w_{2} & w_{3} 
\end{array}\right] \boldsymbol{x} \\
=\boldsymbol{C} \boldsymbol{x} }$$ 其中 ${\boldsymbol{C}}$ 是一个稀疏矩阵,其非零元素来自于卷积核 ${\boldsymbol{w}}$ 中的元素.
如果要实现 3 维向量 ${\boldsymbol{z}}$ 到 5 维向量 ${\boldsymbol{x}}$ 的映射, 可以通过仿射矩阵的转置来实 现, 即 $${ \boldsymbol{x} =\boldsymbol{C}^{\top} \boldsymbol{z} \\
=\left[
\begin{array}{ccc}
w_{1}& 0& 0 \\
w_{2}& w_{1}& 0 \\
w_{3}& w_{2}& w_{1} \\
0& w_{3}& w_{2} \\
0& 0& w_{3}
\end{array}\right] \boldsymbol{z} \\
=\operatorname{rot} 180(\boldsymbol{w}) \tilde{\otimes} \boldsymbol{z} }$$其中 $rot\ 180( )$ 表示旋转 180 度. 

从公式可以看出, 从仿射变换的角度来看两个卷积操作 ${\boldsymbol{z}=\boldsymbol{w} \otimes \boldsymbol{x}}$ 和 ${\boldsymbol{x}=\operatorname{rot} 180(\boldsymbol{w}) \tilde{\otimes} \boldsymbol{z}}$ 也是形式上的转置关系. 因此, 我们将低维特征映射到高维特征的卷积操作称为 #转置卷积 ( Transposed Convolution ) [Dumoulin et al., 2016], 也称为 #反卷积 ( #Deconvolution ) [Zeiler et al., 2011]. 
将转置卷积称为反卷积（Deconvolution）并不太恰当，它不是指卷积的逆运算．

在卷积网络中, 卷积层的前向计算和反向传播也是一种转置关系. 
对一个 ${M}$ 维的向量 ${\boldsymbol{z}}$, 和大小为 ${K}$ 的卷积核, 如果希望通过卷积操作来映射到更高维的向量, 只需要对向量 ${\boldsymbol{z}}$ 进行两端补零 ${P=K-1}$, 然后进行卷积, 可以得到 ${M+K-1}$ 维的向量. 转置卷积同样适用于二维卷积. 

图给出了一个（m = 3, p = 0 ,s = 1）二维卷积和其对应的转置卷积的示例。其中 m表示卷积核大小，p 表示零填充大小（zero-padding）， s 表示步长（stride）。
![[cnn-no_padding_no_strides.gif|250]]
![[cnn-no_padding_no_strides_transposed.gif|250]]

## 微步卷积
我们可以通过增加卷积操作的步长 ${S>1}$ 来实现对输入特征的下采样操作, 大幅降低特征维数. 同样, 我们也可以通过减少转置卷积的步长 ${S<1}$ 来实现上采样操作, 大幅提高特征维数. 步长 ${S<1}$ 的 转置卷积 也称为 #微步卷积 ( Fractionally-Strided Convolution ) [Long et al., 2015]. 为了实现微步卷积, 我们可以在输入特征之间揷入 0 来间接地使得步长变小. 
如果卷积操作的步长为 ${S>1}$, 希望其对应的转置卷积的步长为 ${\frac{1}{S}}$, 需要在输入特征之间揷入 ${S-1}$ 个 0 来使得其移动的速度变慢. 
以一维转置卷积为例, 对一个 ${M}$ 维的向量 ${\boldsymbol{z}}$, 和大小为 ${K}$ 的卷积核, 通过对 向量 ${\boldsymbol{z}}$ 进行两端补零 ${P=K-1}$, 并且在每两个向量元素之间揷入 ${D}$ 个 0 , 然后进 行步长为 1 的卷积, 可以得到 ${(D+1) \times(M-1)+K}$ 维的向量. 

图给出了一个（m = 3,  p = 0 ,s = 2）二维卷积和其对应的转置卷积的示例。其中 m表示卷积核大小，p 表示零填充大小（zero-padding）， s 表示步长（stride）。
![[cnn-no_padding_strides.gif|250]]![[cnn-no_padding_strides_transposed.gif|250]]

## 空洞卷积

对于一个卷积层，如果希望增加输出单元的感受野，一般可以通过三种方式实现：
1）增加卷积核的大小；
2）增加层数，比如两层 3 × 3 的卷积可以近似一层 5 × 5卷积的效果；
3）在卷积之前进行汇聚操作．

前两种方式会增加参数数量，而第三种方式会丢失一些信息．
#空洞卷积 （Atrous Convolution）是一种不增加参数数量，同时增加输出单元感受野的一种方法，也称为 #膨胀卷积 （Dilated Convolution）[Chen et al., 2018; Yu et al., 2015]．

空洞卷积通过给卷积核插入“空洞”来变相地增加其大小．如果在卷积核的每两个元素之间揷入 ${D-1}$ 个空洞, 卷积核的有效大小为 ${ K{\prime}=K+(K-1) \times(D-1) }$ 其中 ${D}$ 称为膨胀率 (Dilation Rate ). 当 ${D=1}$ 时卷积核为普通的卷积核.

图给出了空洞卷积的示例．其中 m表示卷积核大小，p 表示零填充大小（zero-padding）， s 表示步长（stride），d表示膨胀率（dilation）。
m = 3, p = 0, s = 1，d=1 ![[cnn-dilation-in_7_out_3.gif|250]]m = 3, p = 0, s = 1，d=2![[cnn-dilation.gif|250]]


*在早期的文献中，卷积一般默认为窄卷积；而目前的文献中，卷积一般默认为等宽卷积。*
 *Atrous 一词来源于法语 à trous，意为“空洞，多孔”．*
图片修改自： Vincent Dumoulin, Francesco Visin - [A guide to convolution arithmetic
for deep learning](https://arxiv.org/abs/1603.07285)

<iframe 
	height=600
	width=700  
	src="file:///E:/ObisDian/AIKA/accessory/v/cnn-conv-2d.html"
	></iframe>
<iframe 
	height=600
	width=700  
	src="file:///E:/ObisDian/AIKA/accessory/v/cnn-conv-more.html"
	></iframe>
