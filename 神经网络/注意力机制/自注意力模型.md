当使用神经网络来处理一个变长的向量序列时，我们通常可以使用卷积网络或循环网络进行编码来得到一个相同长度的输出向量序列，如图所示。
![[基于卷积网络和循环网络的变长序列编码.png]]

基于卷积或循环网络的序列编码都是可以看做是一种局部的编码方式，只建模了输入信息的局部依赖关系。虽然循环网络理论上可以建立长距离依赖关系，但是由于信息传递的容量以及梯度消失问题，实际上也只能建立短距离依赖关系。

如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：一种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互另一种方法是使用全连接网络。全连接网络是一种非常直接的建模远距离依赖的模型，但是无法处理变长的输入序列。不同的输入长度，其连接权重的大小也是不同的。这时我们就可以利用注意力机制来“动态”地生成不同连接的权重，这就是自注意力模型（Self-Attention Model）。

自注意力也称为内部注意力（Intra-Attention）。

假设输入序列为 $X=\left[\mathbf{x}_{1}, \cdots, \mathbf{x}_{N}\right] \in \mathbb{R}^{d_{1} \times N}$, 输出序列为 $H=\left[\mathbf{h}_{1}, \cdots, \mathbf{h}_{N}\right] \in \mathbb{R}^{d_{2} \times N}$, 首先我们可以通过线性变换得到三组向量序列:
$$
\begin{aligned}
&Q=W_{Q} X \in \mathbb{R}^{d_{3} \times N} \\
&K=W_{K} X \in \mathbb{R}^{d_{3} \times N} \\
&V=W_{V} X \in \mathbb{R}^{d_{2} \times N}
\end{aligned}
$$
其中 $Q, K, V$ 分别为查询向量序列, 键向量序列和值向量序列, $W_{Q}, W_{K}, W_{V}$ 分 别为可学习的参数矩阵。
利用 #键值对注意力 可以得到输出向量 $\mathbf{h}_{i}$,
$$
\begin{aligned}
\mathbf{h}_{i} &=\operatorname{att}\left((K, V), \mathbf{q}_{i}\right) \\
&=\sum_{j=1}^{N} \alpha_{i j} \mathbf{v}_{j} \\
&=\sum_{j=1}^{N} \operatorname{softmax}\left(s\left(\mathbf{k}_{j}, \mathbf{q}_{i}\right)\right) \mathbf{v}_{j}
\end{aligned}
$$
其中 $i, j \in[1, N]$ 为输出和输入向量序列的位置, 连接权重 $\alpha_{i j}$ 由注意力机制动 态生成。
如果使用缩放点积来作为注意力打分函数, 输出向量序列可以写为
$$
H=V \operatorname{softmax}\left(\frac{K^{\mathrm{T}} Q}{\sqrt{d_{3}}}\right),
$$
其中 softmax 为按列进行归一化的函数。
图给出全连接模型和自注意力模型的对比，其中实线表示为可学习的权重，虚线表示动态生成的权重。由于自注意力模型的权重是动态生成的，因此可以处理变长的信息序列。
![[全连接模型和自注意力模型.png]]
自注意力模型可以作为神经网络中的一层来使用，既可以用来替换卷积层和循环层[Vaswani et al., 2017]，也可以和它们一起交替使用[Shen et al., 2018]（比如$X$可以是卷积层或循环层的输出）。自注意力模型计算的权重$α_{ij}$只依赖$q_i$和$k_j$的相关性，而忽略了输入信息的位置信息。因此在单独使用时，自注意力模型一般需要加入位置编码信息来进行修正[Vaswani et al., 2017]。