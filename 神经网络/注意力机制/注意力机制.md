根据 #通用近似定理 ，前馈网络和循环网络都有很强的能力。但由于优化算法和计算能力的限制，在实践中很难达到通用近似的能力。特别是在处理复杂任务时，比如需要处理大量的输入信息或者复杂的计算流程时，目前计算机的计算能力依然是限制神经网络发展的瓶颈。

当用神经网络来处理大量的输入信息时，也可以借鉴[[人脑的注意力机制]]，只选择一些关键的信息输入进行处理，来提高神经网络的效率。在目前的神经网络模型中，我们可以将 #最大汇聚 、 #门机制 来近似地看作是自下而上的基于显著性的注意力机制。除此之外，自上而下的会聚式注意力也是一种有效的信息选择方式。
以阅读理解任务为例，给定一篇很长的文章，然后就此文章的内容进行提问。提出的问题只和段落中的一两个句子相关，其余部分都是无关的。为了减小神经网络的计算负担，只需要把相关的片段挑选出来让后续的神经网络来处理，而不需要把所有文章内容都输入给神经网络。

用$X = [x_1, · · · , x_N ]$表示N 个输入信息，为了节省计算资源，不需要将所有的N 个输入信息都输入到神经网络进行计算，只需要从X 中选择一些和任务相关的信息输入给神经网络。注意力机制的计算可以分为两步：一是在所有输入信息上计算注意力分布，二是根据注意力分布来计算输入信息的加权平均。

## 注意力分布 
给定一个和任务相关的查询向量 $q$，我们用注意力变量 $z ∈ [1, N]$ 来表示被选择信息的索引位置，即 $z = i$ 表示选择了第 $i$ 个输入信息。为了方便计算，我们采用一种“软性”的信息选择机制，首先计算在给定 $q$ 和 $X$ 下，选择第i个输入信息的概率 $α_i$，
$$α_i = p(z = i|X, q)= softmax(s(x_i, q))=exp (s(x_i,q))) /\sum^N_{j=1}(exp (s(x_j ,q))$$,其中 $α_i$ 称为 #注意力分布 （Attention Distribution），$s(x_i,q)$为 #注意力打分函数 ，查询向量 $q$ 可以是动态生成的，也可以是可学习的参数。

注意力打分函数可以使用以下几种方式来计算：
加性模型 $\quad s\left(\mathbf{x}_{i}, \mathbf{q}\right)=\mathbf{v}^{\mathrm{T}} \tanh \left(W \mathbf{x}_{i}+U \mathbf{q}\right)$
点积模型    $s\left(\mathbf{x}_{i}, \mathbf{q}\right)=\mathbf{x}_{i}^{\mathrm{T}} \mathbf{q}$
缩放点积模型    $s\left(\mathbf{x}_{i}, \mathbf{q}\right)=\frac{\mathbf{x}_{i}^{\mathrm{T}} \mathbf{q}}{\sqrt{d}}$
双线性模型    $s\left(\mathbf{x}_{i}, \mathbf{q}\right)=\mathbf{x}_{i}^{\mathrm{T}} W \mathbf{q}$

其中$W, U, v$为可学习的网络参数，$d$为输入信息的维度。理论上，加性模型和点积模型的复杂度差不多，但是点积模型在实现上可以更好地利用矩阵乘积，从而计算效率更高。但当输入信息的维度$d$比较高，点积模型的值通常有比较大方差，从而导致softmax函数的梯度会比较小。因此，缩放点积模型可以较好地解决这个问题。双线性模型可以看做是一种泛化的点积模型。假设公式中 $W = U^TV$，双线性模型可以写为 $s\left(\mathbf{x}_{i}, \mathbf{q}\right)=\mathbf{x}_{i}^{\mathrm{T}} U^{\mathrm{T}} V \mathbf{q}=(U \mathbf{x})^{\mathrm{T}}(V \mathbf{q})$，即分别对 $x$ 和 $q$ 进行线性变换后计算点积。相比点积模型，双线性模型在计算相似度时引入了非对称性。

## 加权平均
注意力分布 $α_i$ 可以解释为在给定任务相关的查询$q$时，第$i$个信息受关注的程度。我们采用一种“软性”的信息选择机制对输入信息进行汇总，
$$\begin{aligned} \operatorname{att}(X, \mathbf{q}) &=\sum_{i=1}^{N} \alpha_{i} \mathbf{x}_{i} \\ &=\mathbb{E}_{z \sim p(z \mid X, \mathbf{q})}[\mathbf{x}] \end{aligned}$$
上述公式被称为 #软性注意力 机制（Soft Attention Mechanism）。
图给出软性注意力机制的示例。
![[软性注意力机制.png|300]]


#注意力机制 也可称为 #注意力模型 。注意力机制除了软性注意力机制的基本模式外，还存在一些[[注意力机制的变体]]。注意力机制一般可以用作一个神经网络中的组件，比如[[指针网络]]和[[自注意力模型]]。

注意力机制是一种（不严格的）受人类神经系统启发的信息处理机制。比如人视觉神经系统并不会一次性地处理所有接受到的视觉信息。

注意力机制最早在计算机视觉中提出。在神经网络中，Mnih et al. [2014]在循环神经网络模型上使用了注意力机制来进行图像分类。Bahdanau et al. [2014]使用注意力机制在机器翻译任务上将翻译和对齐同时进行。Xu et al. [2015]利用注意力机制来进行图片的内容描述。目前，注意力机制已经在语音识别、图像标题生成、阅读理解、文本分类、机器翻译等多个任务上取得了很好的效果，也变得越来越流行。注意力机制的一个重要应用是 #自注意力 。自注意力可以作为神经网络中的一层来使用，有效地建模 #长距离依赖问题[Vaswani et al., 2017]。
