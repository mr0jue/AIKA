除了 #软性注意力 机制的基本模式外，注意力机制还存在一些的变体。

## 硬性注意力
软性注意力其选择的信息是所有输入信息在注意力分布下的期望。此次，还有一种注意力是只关注到某一个位置上的信息，叫做 #硬性注意力 （Hard Attention）。硬性注意力有两种实现方式：
（1）一种是选取最高概率的输入信息，即
$$
\operatorname{att}(X, \mathbf{q})=\mathbf{x}_{j}
$$
其中 $j$ 为概率最大的输入信息的下标, 即 $j=\underset{i=1}{\arg \max } \alpha_{i}$ 。
（2）另一种硬性注意力可以通过在注意力分布式上随机采样的方式实现。
硬性注意力的一个缺点是基于最大采样或随机采样的方式来选择信息。因此最终的损失函数与注意力分布之间的函数关系不可导，因此无法使用在反向传播算法进行训练。为了使用反向传播算法，一般使用软性注意力来代替硬性注意力。
硬性注意力需要通过强化学习来进行训练。

## 键值对注意力
我们可以用键值对 (key-value pair) 格式来表示输入信息, 其 中“键” 用来计算注意力分布 $\alpha_{i}$, “值” 用来计算聚合信息。
用 $(K, V)=\left[\left(\mathbf{k}_{1}, \mathbf{v}_{1}\right), \cdots,\left(\mathbf{k}_{N}, \mathbf{v}_{N}\right)\right]$ 表示 $N$ 个输入信息, 给定任务相关的 查询向量 $\mathbf{q}$ 时, 注意力函数为
$$
\begin{aligned}
\operatorname{att}((K, V), \mathbf{q}) &=\sum_{i=1}^{N} \alpha_{i} \mathbf{v}_{i} \\
&=\sum_{i=1}^{N} \frac{\exp \left(s\left(\mathbf{k}_{i}, \mathbf{q}\right)\right)}{\sum_{j} \exp \left(s\left(\mathbf{k}_{j}, \mathbf{q}\right)\right)} \mathbf{v}_{i}
\end{aligned}
$$
其中 $s\left(\mathbf{k}_{i}, \mathbf{q}\right)$ 为打分函数。
图给出 #键值对注意力 机制的示例。当 $K=V$ 时, 键值对模式就等价于普通的注意力机制。
![[键值对模式.png|300]]


## 多头注意力
#多头注意力 ( #Multi-HeadAttention) 是利用多个查询 $Q=\left[\mathbf{q}_{1}, \cdots, \mathbf{q}_{M}\right]$, 来 平行地计算从输入信息中选取多个信息。每个注意力关注输入信息的不同部分。
$$
\operatorname{att}((K, V), Q)=\operatorname{att}\left((K, V), \mathbf{q}_{1}\right) \oplus \cdots \oplus \operatorname{att}\left((K, V), \mathbf{q}_{M}\right),
$$
其中 $\oplus$ 表示向量拼接。

## 结构化注意力
要从输入信息中选取出和任务相关的信息，主动注意力是在所有输入信息上的多项分布，是一种扁平（flflat）结构。如果输入信息本身具有层次（hierarchical）结构，比如文本可以分为词、句子、段落、篇章等不同粒度的层次，我们可以使用层次化的注意力来进行更好的信息选择[Yang et al., 2016]。此外，还可以假设注意力上下文相关的二项分布，用一种图模型来构建更复杂的 #结构化注意力 分布[Kim et al., 2017]。