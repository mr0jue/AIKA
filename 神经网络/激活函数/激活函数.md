#激活函数 （Activation Function）在神经元中非常重要的。为了增强网络的表示能力和学习能力，激活函数需要具备以下几点性质：
1. 连续并可导（允许少数点上不可导）的非线性函数。可导的激活函数可以直接利用数值优化的方法来学习网络参数。
2. 激活函数及其导函数要尽可能的简单，有利于提高网络计算效率。
3. 激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性。

下面介绍几种在神经网络中常用的激活函数。
- [[Sigmoid 型激活函数]]
	- #Logistic
	- #Tanh
	- #Hard-Logistic
	- #Hard-Tanh
- [[修正线性单元]]
	- #ReLU
	- #LeakyReLU
	- #PReLU
	- #ELU
- [[Softplus 函数]]
- [[Swish 函数]]
- [[Maxout 单元]]