#图灵机 （Turing Machine）是图灵在1936年提出的一种抽象数学模型，可以用来模拟任何可计算问题[Turing, 1937]。
#神经图灵机 （Neural Turing machine，NTM）[Graves et al., 2014]主要由两个部件构成: 控制器和外部记忆。外部记忆定义为矩阵  $M \in \mathbb{R}^{d \times N}$ , 这 里  $N$  是记忆片段的数量,  $d$  是每个记忆片段的大小。控制器为一个前馈或循环神经网络。神经图灵机中的外部记忆是可以可读写的，其结构如图所示。

![[神经图灵机示例.png|500]]

在每个时刻  t , 控制器接受当前时刻的输入  $\mathbf{x}_{t}$ , 上一时刻的输出  $\mathbf{h}_{t-1}$  和上 一时刻从外部记忆中读取的信息  $\mathbf{r}_{t-1}$ , 并产生输出  $\mathbf{h}_{t}$ , 同时生成和读写外部记 忆相关的三个向量: 查询向量  $\mathbf{q}_{t}$ , 删除向量  $\mathbf{e}_{t}$  和增加向量  $\mathbf{a}_{t}$  。然后对外部记忆  $\mathcal{M}_{t}$  进行读写操作, 生成读向量  $\mathbf{r}_{t}$ , 和新的外部记忆  $M_{t+1}$  。

# 读操作
在时刻 $t$, 外部记忆的内容记为 $M_{t}=\left[\mathbf{m}_{t, 1}, \cdots, \mathbf{m}_{t, n}\right]$, 读操作为从外部 记忆 $\mathcal{M}_{t}$ 中读取信息 $\mathbf{r}_{t} \in \mathbb{R}^{d}$ 。
首先通过注意力机制来进行基于内容的寻找, 即
$$
\alpha_{t, i}=\operatorname{softmax}\left(s\left(\mathbf{m}_{t, i}, \mathbf{q}_{t}\right)\right)
$$
其中 $\mathbf{q}_{t}$ 为控制器产生的查询向量, 用来进行基于内容的寻址。 $s(\cdot, \cdot)$ 为加性或乘 性的打分函数。注意力分布 $\alpha_{t, i}$ 是记忆片段 $\mathbf{m}_{t, i}$ 对应的权重, 并满足 $\sum_{i=1}^{n} \alpha_{t, i}=$ 1 。
根据注意力分布 $\alpha_{t}$, 可以计算读向量（read vector） $\mathbf{r}_{t}$ 作为下一个时刻控 制器的输入。
$$
\mathbf{r}_{t}=\sum_{i=1}^{n} \alpha_{i} \mathbf{m}_{t, i}
$$
# 写操作
外部记忆的写操作可以分解为两个子操作: 删除和增加。
首先, 控制器产生删除向量 (erase vector) $\mathbf{e}_{t}$ 和增加向量 (add vector) $\mathbf{a}_{t}$, 分别为要从外部记忆中删除的信息和要增加的信息。
删除操作是根据注意力分布来按比例地在每个记忆片段中删除 $\mathbf{e}_{t}$, 增加操 作根据注意力分布来进行按比例地给每个记忆片段加入 $\mathbf{a}_{t}$ 。
$$
\mathbf{m}_{t+1, i}=\mathbf{m}_{t, i}\left(\mathbf{1}-\alpha_{t, i} \mathbf{e}_{t}\right)+\alpha_{t, i} \mathbf{a}_{t}, \forall i \in[1, n] .
$$
通过写操作得到下一时刻的外部记忆 $\mathcal{M}_{t+1}$ 。

