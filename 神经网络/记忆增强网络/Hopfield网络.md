本书中之前介绍的神经网络都是作为一种机器学习模型的输入-输出映射函数，其参数学习方法是通过梯度下降方法来最小化损失函数。除了作为机器学习模型外，神经网络还可以作为一种记忆的存储和检索模型。

#Hopfield网络 （HopfieldNetwork）是一种循环神经网络模型，由一组相互连接的神经元组成。Hopfield网络也可以认为是所有神经元都相互连接的不分层的神经网络。每个神经元既是输入单元，又是输出单元，没有隐藏神经元。一个神经元和自身没有反馈相连，不同神经元之间连接权重是对称的。

图给出了Hopfield网络的结构示例。
![[四个节点的Hopfield网络.png|250]]

这里我们只介绍离散Hopfield网络，神经元状态为+1,−1两种。除此之外，还有连续Hopfield网络，即神经元状态为连续值。

假设一个Hopfield网络有${m}$个神经元,第${i}$个神经元的更新规则为
$$
s_{i}=\left\{\begin{aligned}
+1&\text{if}\sum_{j=1}^{m}w_{ij}s_{j}+b_{i}\geq0\\
-1&\text{otherwise}
\end{aligned}\right.
$$
其中${w_{ij}}$为神经元${i}$和${j}$之间的连接权重,${b_{i}}$为偏置。连接权重${w_{ij}}$有以下性质$${\begin{array}{ll}w_{ii}=0\forall i\in[1,m]\\w_{ij}=w_{ji}\forall i,j\in[1,m]\end{array}}$$Hopfield网络的更新可以分为异步和同步两种方式。异步更新是每次更新一个神经元。神经元的更新顺序可以是随机或事先固定的。同步更新是指一次更新所有的神经元，需要有一个时钟来进行同步。第${t}$时刻的神经元状态为${\mathrm{s}_{t}=}{\left[\mathbf{s}_{t,1},\mathbf{s}_{t,2},\cdots,\mathbf{s}_{t,m}\right]^{\mathrm{T}}}$，其更新规则为$${\mathbf{s}_{t}=f\left(W\mathbf{s}_{t-1}+\mathbf{b}\right)}$$其中${\mathbf{s}_{0}=\mathbf{x},W=\left[w_{ij}\right]_{m\times m}}$为连接权重,${\mathbf{b}=\left[b_{i}\right]_{m\times1}}$为偏置向量,${f(\cdot)}$为非线性阶跃函数。
# 能量函数
在Hopfield网络中,我们给每个不同的网络状态定义一个标量属性,称为“能量”。
$${\begin{align}E&=-\frac{1}{2}\sum_{i,j}w_{ij}s_{i}s_{j}-\sum_{i}b_{i}s_{i}\\&=-\frac{1}{2}\mathbf{s}^{\mathrm{T}}W\mathbf{s}-\mathbf{b}^{\mathrm{T}}\mathbf{s}\end{align}}$$
Hopfield网络是稳定的，即能量函数经过多次迭代后会达到收敛状态。权重对称是一个重要特征，因为它保证了能量函数在神经元激活时单调递减，而不对称的权重可能导致周期性震荡或者混乱。
给定一个外部输入，网络经过演化，会达到某个稳定状态。这些稳定状态称为吸引点（Attractor）。一个Hopfield网络中，通常有多个吸引点，每个吸引点为一个能量的局部最优点。
能量函数E是Hopfield网络的Lyapunov函数。Lyapunov定理是非线性动力系统中保证系统稳定性的充分条件。

图给出了Hopfield网络的能量函数。红线为网络能量的演化方向，蓝点为吸引点。
![[Hopfield网络的能量函数.png|500]]

# 联想记忆
Hopfield 网络存在有限的吸引点 (Attractor), 即能量函数的局部最小点。

每个吸引点 ${u}$ 都对应一个 “管辖” 区域 ${\mathcal{R}_{\mathrm{u}}}$, 如果输入向量 ${\mathbf{x}}$ 落入这个区 域, 网络最终会收玫到 ${u}$ 。因此, 吸引点可以看作是网络中存储的信息。将网 络输入 ${\mathrm{x}}$ 作为起始状态, 随时间收到吸引点 ${u}$ 上的过程作为检索过程。即使 输入向量 ${\mathrm{x}}$ 是有部分信息或有噪声, 只用其位于对应存储模式的 “吸引” 区域内，那么随着时间演化，网络最终会收敛到其对应的存储模式。因此，Hopfield的检索是基于内容寻址的检索，具有联想记忆能力。

# 信息存储
信息存储是指将一组向量 ${x_{1}, \cdots, x_{N}}$ 存储在网络中的过程。存储过程主要是调整神经元之间的连接权重, 因此可以看做是一种学习过程。Hopfield 网络的学习规则有很多种。一种最简单的学习方式为: 神经元 ${i}$ 和 ${j}$ 之间的连接权重
$${ w_{i j}=\frac{1}{N} \sum_{n=1}^{N} x_{i}^{(n)} x_{j}^{(n)} }$$ 其中 ${x_{i}^{(n)}}$ 是第 ${n}$ 个输入向量的第 ${i}$ 维特征。如果 ${x_{i}}$ 和 ${x_{j}}$ 在输入向量中相同的概 率越多, 则 ${w_{i j}}$ 越大。这种学习规则和人脑神经网络的学习方式十分类似。在人脑神经网络中, 如果两个神经元经常同时激活, 则它们之间的连接加强; 如果经常不同时激活, 则连接消失。这种学习方式称为 #Hebbian 法则。
# 存储容量
对于联想记忆模型来说，存储容量为其能够可靠地存储和检索模式的最大数量。对于数量为$m$的相互连接的二值神经元网络，其总状态数$2^m$，其中可以作为有效稳定点的状态数量就是其存储容量。模型容量一般与网络结构和学习方式有关。Hopfield最大的网络容量为0.14m，玻尔兹曼机的容量为0.6m，但是其学习效率比较低，需要非常长时间的演化才能达到均衡状态。通过改进学习算法，Hopfield网络的最大容量可以达到$O(m)$。如果允许高阶（阶数为K）连接，比如三个神经元连接关系，其稳定存储的最大容量为 $O(m^{K−1})$。Plate[1995]引入复数运算，有效地提高了网络容量。总体上讲，通过改进网络结构、学习方式以及引入更复杂的运算（比如复数、量子操作），联想记忆网络的容量可以有效改善。