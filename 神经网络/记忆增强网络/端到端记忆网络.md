#端到端记忆网络 （End-To-End Memory Network，MemN2N）[Sukhbaatar et al.，2015]采用一种可微的网络结构，可以多次从外部记忆中读取信息。在端到端记忆网络中，外部记忆单元是只读的。

给定一组需要存储的信息${m_{1:N}=\left\{m_{1},\cdots,m_{N}\right\}}$，首先将转换成两组记忆片段${A=\left[\mathbf{a}_{1},\cdots,\mathbf{a}_{N}\right]}$和${C=\left[\mathbf{c}_{1},\cdots,\mathbf{c}_{N}\right]}$，分别存放在两个外部记忆单元中，其中${A}$用来进行寻址，${C}$用来进行输出。
主网络根据输入${\mathrm{x}}$生成${\mathrm{q}}$，并使用注意力机制来从外部记忆中读取相关信息${r}$，
$${\mathbf{r}=\sum_{i=1}^{N}\operatorname{softmax}\left(\mathbf{a}_{i}^{\mathrm{T}}\mathbf{q}\right)\mathbf{c}_{i}}$$
并产生输出${\mathbf{y}=f(\mathbf{q}+\mathbf{r})}$. 其中${f(\cdot)}$为预测函数。当应用到分类任务时，${f(\cdot)}$可以设为softmax函数。

**多跳操作**
为了实现更新复杂的计算，我们可以让主网络和外部记忆进行多轮交互。

在第${k}$轮交互中，主网络根据上次从外部记忆中读取的信息${\mathbf{r}^{(k-1)}}$，产生新的查询向量
$${\mathbf{q}^{(k)}=\mathbf{r}^{(k-1)}+\mathbf{q}^{(k-1)}}$$
其中${\mathbf{q}^{(0)}}$为初始的查询向量，${\mathbf{r}^{(0)}=0}$。假设第${k}$轮交互的外部记忆为${A^{(k)}}$和${C^{(k)}}$，主网络从外部记忆读取信息为
$${\mathbf{r}^{(k)}=\sum_{i=1}^{N}\operatorname{softmax}\left(\left(\mathbf{a}_{i}^{(k)}\right)^{\mathrm{T}}\mathbf{q}^{(k)}\right)\mathbf{c}_{i}^{(k)}}$$
在$K$轮交互后，用$\mathbf{y}=f\left(\mathbf{q}^{(K)}+\mathbf{r}^{(K)}\right)$进行预测。这种多轮的交互方式也称为多跳(Multi-Hop)操作。多跳操作中的参数一般是共享的。为了简化起见，每轮交互的外部记忆也可以共享使用，比如$A^{(1)}=\cdots=A^{(K)}$和$C^{(1)}=\cdots=C^{(K)}$。
![[端到端记忆网络.png]]