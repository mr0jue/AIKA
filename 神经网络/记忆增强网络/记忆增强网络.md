借鉴[[人脑中的记忆]]( #工作记忆 )，可以在神经网络中引入辅助记忆单元，将一些信息保存辅助记忆中，在需要时再进行读取，这样可以有效地增加网络容量。这个引入辅助记忆单元一般称为 #外部记忆 （External Memory）（以区别与循环神经网络的内部记忆，即隐状态）。通过引入外部记忆，可以将神经网络的参数和记忆容量的“分离”，即在少量增加网络参数的条件下可以大幅增加网络容量。 #注意力机制 可以看做是一个接口，将信息的存储与计算分离。和之前介绍的 LSTM 中的记忆单元相比，外部记忆可以存储更多的信息，并且不直接参与计算，通过读写接口来进行操作。而LSTM模型中的记忆单元包含了信息存储和计算两种功能，不能存储太多的信息。*因此，LSTM中的记忆单元可以类比于计算机中寄存器，而外部记忆可以类比于计算机中的存储器：内存、磁带或硬盘等。* 

外部记忆的实现途径有两种：一种是结构化的记忆，这种记忆和计算机中的信息存储方法比较类似，可以分为多个记忆片段，并按照一定的结构来存储；另一种是基于神经动力学的联想记忆，这种记忆方式具有更好的生物学解释性。

## 记忆网络
为了增强网络容量，一种比较简单的方式是引入结构化的记忆模块，将和任务相关的短期记忆保存在记忆中，需要时再进行读取。这种装备外部记忆的神经网络称为 #记忆网络 （Memory Network，MN）也称为 #记忆增强神经网络 （Memory Augmented Neural Network，MANN）或 记忆增强网络 （Memory-Enhanced Networks）。外部记忆从记忆结构、读写方式等方面可以演变出很多模型。代表性模型有[[神经图灵机]][Graves et al., 2014]、[[端到端记忆网络]][Sukhbaataret al., 2015]、动态记忆网络[Kumar et al., 2016]等。

记忆网络结构如图所示，一般有以下几个模块构成：
![[记忆网络结构.png|400]]

1. 主网络 C：也称为控制器（Controller），负责信息处理，并与外界的交互（接受外界的输入信息并产生输出到外界）。主网络还同时通过读写模块和外部记忆进行交互。
2. 外部记忆单元M：外部记忆单元用来存储信息，一般可以分为很多记忆片段（Memory Segment），这些记忆片段按照一定的结构来进行组织。记忆片段一般用向量来表示，外部记忆单元可以用一组向量$\mathbf{m}_{1: N}=\left[\mathbf{m}_{1}, \cdots, \mathbf{m}_{N}\right]$来表示。这些向量的组织方式可以是集合、树、栈或队列等。大部分信息存储于外部记忆中，不需要全时参与主网络的运算。
3. 读取模块 $R$ : 根据主网络生成的查询向量 $\mathbf{q}_{r}$, 从外部记忆单元中读取相应 的信息 $\mathbf{r}=R\left(\mathbf{m}_{1: N}, \mathbf{q}_{r}\right)$ 。
4. 写入模块 $W$ : 根据主网络生成的查询向量 $\mathbf{q}_{w}$ 和要写入的信息 $\mathbf{a}$ 来更新外 部记忆 $\mathbf{m}_{1: N}=W\left(\mathbf{m}_{1: N}, \mathbf{q}_{w}, \mathbf{a}\right)$ 。

这种结构化的外部记忆是带有地址的, 即每个记忆片段都可以按地址读取和写入。要实现类似于人脑神经网络的联想记忆能力, 就需要按内容寻址的方式进行定位, 然后进行读取或写入操作。按内容寻址通常使用注意力机制来进行。通过注意力机制可以实现一种 “软性” 的寻址方式, 即计算一个在所有记忆片段上的分布, 而不是一个单一的绝对地址。比如读取模型 $R$ 的实现方式可以为:
$$
\begin{aligned}
\mathbf{r} &=\sum_{i=1}^{N} \alpha_{i} \mathbf{m}_{i} \\
\alpha_{i} &=\operatorname{softmax}\left(s\left(\mathbf{m}_{i}, \mathbf{q}_{r}\right)\right)
\end{aligned}
$$
其中$q_r$是主网络生成的查询向量，$s(·, ·)$为打分函数。类比于计算机的存储器读取，计算注意力分布的过程相当于是计算机的“寻址”过程，信息加权平均的过程相当于计算机的“内容读取”过程。因此，结构化的外部记忆也是一种联想记忆，只是其结构以及读写的操作方式更像是受计算机架构的启发。

## 联想记忆模型

此外，基于神经动力学的 #联想记忆 也可以作为一种外部记忆，引入到神经网络以增加网络容量，这种神经网络称为 #联想记忆模型 （Associative Memory Model）。结构化的外部记忆更多是受现代计算机架构的启发，将计算和存储功能进行分离。和结构化的外部记忆相比，联想记忆具有更好的生物学解释性。联想记忆模型可以看做是一种循环神经网络，基于神经动力学来实现按内容寻址的信息存储和检索。
*神经动力学（Neurodynamics）是将神经网络作为非线性动力系统，研究其随时间变化的规律以及稳定性等问题。*

联想记忆模型主要是通过神经网络的动态演化来进行联想，有两种应用场景：
1）输入的模式和输出的模式在同一空间，这种模型叫做 #自联想记忆模型 （Auto-Associative Model）。自联想模型可以通过前馈神经网络或者循环神经网络来实现，也经常称为 #自编码器 （Auto-Encoder）；
2）输入的模式和输出的模式不在同一空间，这种模型叫做 #异联想记忆模型 （Hetero-Associative Model）。从广义上讲，大部分模式识别问题都可以看作是异联想，因此异联想记忆模型可以作为分类器使用。

一个经典的联想记忆模型为[[Hopfifield网络]]：Hopfifield [1984]将能量函数的概念引入到神经网络模型中，提出了Hopfifield网络。Hopfifield网络在旅行商问题上获得当时最好结果，引起轰动。Danihelka et al. [2016]将一个联想记忆模型作为部件引入LSTM网络中，而从在不引入额外参数的情况下增加网络容量。Ba et al. [2016]将循环神经网络中的部分连接权重作为短期记忆，并通过一个联想记忆模型进行更新，而从提高网络性能。

在上述的网络中，联想记忆都是作为一个更大网络的组件，用来增加短期记忆的容量。联想记忆组件的参数可以使用 #Hebbian 方式来学习，也可以作为整个网络参数的一部分来进行学习。

## 后记
目前人工神经网络中的外部记忆模型结构还比较简单，需要借鉴神经科学的研究成果，提出更有效的记忆模型，增加网络容量。