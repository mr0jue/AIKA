在超参数优化中，每组超参数配置的评估代价比较高．如果我们可以在较早的阶段就估计出一组配置的效果会比较差，那么我们就可以中止这组配置的评估，将更多的资源留给其他配置．*动态资源分配*的关键是将有限的资源分配给更有可能带来收益的超参数组合．一种有效方法是逐次减半（Successive Halving）方法 \[Jamieson et al.,2016]

动态资源分配问题, 将超参数优化看作一种非随机的 #多臂赌博机问题 ．假设要尝试 𝑁 组超参数配置，总共可利用的资源预算（摇臂的次数）为𝐵，我们可以通过`𝑇 = ⌈log_2(𝑁)⌉ − 1`轮逐次减半的方法来选取最优的配置，

由于目前神经网络的优化方法一般都采取随机梯度下降，因此我们可以通过一组超参数的学习曲线来预估这组超参数配置是否有希望得到比较好的结果．如果一组超参数配置的学习曲线不收敛或者收敛比较差，我们可以应用早期停止（Early-Stopping）策略来中止当前的训练