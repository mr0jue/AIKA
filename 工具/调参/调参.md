在进行模型评估与选择时，除了要对适用学习算法进行选择，还需对算法参数([[超参数]])进行设定，这就是通常所说的“参数调节”或简称“调参”（parameter tuning).
调参和算法选择没什么本质区别:对每种参数配置都训练出模型，然后把对应最好模型的参数作为结果.

## 超参数的选取
学习算法的很多参数是在实数范围内取值，因此，对每种参数配置都训练出模型来是不可行的.
自动化调参,   或者说超参数优化（Hyperparameter Optimization很难通过[[优化算法]]来自动学习(见#超参数优化的困难)． 

因此，调参是机器学习的一个经验性很强的技术，通常是按照人的经验设定，或者通过搜索的方法对一组超参数组合进行不断试错调整．现实中常用的做法是使用[[网格搜索]], 即对每个参数选定一个范围和变化步长.显然，这样选定的参数值往往不是“最佳”值，但这是在计算开销和性能估计之间进行折中的结果，通过这个折中学习过程才变得可行.事实上，即便在进行这样的折中后，调参往往仍很困难.很多强大的学习算法有大量参数需设定，这将导致极大的调参工程量，以至于在不少应用任务中，参数调得好不好往往对最终模型性能有关键性影响.

超参数优化，常见的方法有[[网格搜索]]、[[贝叶斯优化]]、[[动态资源分配]]、神经架构搜索([[NAS]])．  除了NAS有点特殊, 上面的超参数优化方法都是在固定（或变化比较小）的超参数空间 𝒳中进行最优配置搜索，而最重要的神经网络架构一般还是需要由有经验的专家来进行设计．

## 超参数优化的困难
超参数优化主要存在两方面的困难：
1）超参数优化一般都是[[组合优化]]问题，无法像一般参数那样通过[[梯度下降法]]来优化，也没有一种通用有效的优化方法；
2）评估一组超参数配置（Confguration）的时间代价非常高， 从而导致一些优化方法, 比如 #演化算法 （Evolution Algorithm）, 在超参数优化中难以应用．
### PS.无法使用梯度下降等优化方法
假设一个神经网络中总共有 𝐾 个超参数，每个超参数配置表示为一个向量𝒙 ∈ 𝒳，𝒳 ⊂ ℝ^𝐾 是超参数配置的取值空间．超参数优化的目标函数定义为𝑓(𝒙) ∶ 𝒳 → ℝ，𝑓(𝒙) 是衡量一组超参数配置 𝒙 效果的函数，一般设置为开发集上的错误率．目标函数𝑓(𝒙)可以看作一个黑盒（black-box）函数，不需要知道其具体形式．虽然在神经网络的超参数优化中，𝑓(𝒙)的函数形式已知，但𝑓(𝒙)不是关于 𝒙 的连续函数，并且 𝒙 不同，𝑓(𝒙) 的函数形式也不同，因此无法使用梯度下降等优化方法．