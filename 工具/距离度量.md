对给定样本$𝔁_i=(x_{i1},x_{i2},...,x_{in})$与$𝔁_j=(x_{j1},x_{j2},...,x_{jn})$，若函数$dist(·,·)$是一个＂距离度量＂（distance measure），则需满足一些基本性质：
	非负性：$dist(x_i,x_j)≥0$;
	同一性：$dist(x_i,x_j)=0$当且仅当$x_i=x_j$;
	对称性：$dist(x_i,x_j)=dist(x_j，x_i)$;
 #直递性 ：$dist(x_i,x_j)≤dist(x_i,x_k)+dist(x_k,x_j)$.

*在讨论距离计算时, 相较于属性是否是离散属性，属性上是否定义了 “序”关系更为重要.* 

#  #有序属性 的距离度量
给定样本 ${\boldsymbol{x}_{i}=\left(x_{i 1} ; x_{i 2} ; \ldots ; x_{i n}\right)}$ 与 ${\boldsymbol{x}_{j}=\left(x_{j 1} ; x_{j 2} ; \ldots ; x_{j n}\right)}$, 最常用的是 “ #闵可夫斯基距离” ( #Minkowski distance) ，也即$\boldsymbol{x}_{i}-\boldsymbol{x}_{j}$的$\ell_{p}$ #范数 $\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{p} .$
$${ \begin{array}{l}\end{array} \operatorname{dist}_{\mathrm{mk}}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left(\sum_{u=1}^{n}\left|x_{i u}-x_{j u}\right|^{p}\right)^{\frac{1}{p}} . }$$

${p \geqslant 1}$ 时, 闵可夫斯基距离显然满足距离度量基本性质.  
${p=2}$ 时, 闵可夫斯基距离即 #欧氏距离(Euclidean distance)  $${ \operatorname{dist}_{\mathrm{ed}}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{2}=\sqrt{\sum_{u=1}^{n}\left|x_{i u}-x_{j u}\right|^{2}} }$$
 ${ p=1}$ 时, 闵可夫斯基距离即曼哈顿距离(Manhattan distance), 亦称 “街区距离” (city block distance).$${ \operatorname{dist}_{\operatorname{man}}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left\|\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\right\|_{1}=\sum_{u=1}^{n}\left|x_{i u}-x_{j u}\right| }$$
 $p \mapsto \infty$ 时, 则得到 #切比雪夫距离.

#  #无序属性 的距离度量
 无序属性的距离度量可采用 #VDM (Value Difference Metric) [Stanfill and Waltz, 1986]. 
 令 ${m_{u, a}}$ 表示在属性 ${u}$ 上取值为 ${a}$ 的样本数, ${m_{u, a, i}}$ 表示在第 ${i}$ 个样本簇中在属性 ${u}$ 上取值为 ${a}$ 的样本数, ${k}$ 为样本簇数（样本类别已知时 ${k}$ 通常设置为类别数.）, 则属性 ${u}$ 上两个离散值 ${a}$ 与 ${b}$  之间的 VDM 距离为 $${ \operatorname{VDM}_{p}(a, b)=\sum_{i=1}^{k}\left|\frac{m_{u, a, i}}{m_{u, a}}-\frac{m_{u, b, i}}{m_{u, b}}\right|^{p} }$$
 
#  混合属性的距离度量
将闵可夫斯基距离和 VDM 结合即可处理混合属性. 假定有 ${n_{c}}$ 个有 序属性、 ${n-n_{c}}$ 个无序属性, 不失一般性, 令有序属性排列在无序属性之前, 则 $${ \operatorname{MinkovDM}_{p}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left(\sum_{u=1}^{n_{c}}\left|x_{i u}-x_{j u}\right|^{p}+\sum_{u=n_{c}+1}^{n} \operatorname{VDM}_{p}\left(x_{i u}, x_{j u}\right)\right)^{\frac{1}{p}} }$$

# 加权距离
当样本空间中不同属性的重要性不同时, 可使用“ #加权距离” (weighted distance). 
以加权闵可夫斯基距离为例: $${ \operatorname{dist}_{\mathrm{wmk}}\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)=\left(w_{1} \cdot\left|x_{i 1}-x_{j 1}\right|^{p}+\ldots+w_{n} \cdot\left|x_{i n}-x_{j n}\right|^{p}\right)^{\frac{1}{p}} }$$
其中权重 ${w_{i} \geqslant 0(i=1,2, \ldots, n)}$ 表征不同属性的重要性, 通常 ${\sum_{i=1}^{n} w_{i}=1}$. 

# 非度量距离
通常我们是基于某种形式的距离来定义“相似度度量” (similarity measure), 距离越大相似度越小. 然而, 用于相似度度量的距离末必一定要满足距离度量的所有基本性质, 尤其是 #直递性. 例如在 某些任务中我们可能希望有这样的相似度度量：“人” “马”分别与 “人马” 相似, 但“人”与“马”很不相似; 要达到这个目的, 可以令“人” “马”与 “人马” 之间的距离都比较小, 但 “人”与“马” 之间的距离很大，此时该距离不再满足直递性。这样的距离称为 “ #非度量距离” (non-metric distance). 

# 其他
本节介绍的距离计算式都是事先定义好的, 但在不少现实任务中, 有必要基于数据样本来确定合适的距离计算式, 这可通过 “[[距离度量学习]]" (distance metric learning) 来实现.