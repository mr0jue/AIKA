AdaGrad 算法（Adaptive Gradient Algorithm）[Duchi et al., 2011]是借鉴ℓ2 正则化的思想，每次迭代时自适应地调整每个参数的学习率． 
在第 t 次迭代时，先计算每个参数梯度平方的累计值
### 环境感知
希望更新频率低的参数有较大的更新步幅, 更新频率高的参数有较小的更新步幅.
采用历史梯度平方和, 用来衡量不同参数的梯度和稀疏性, 取值越小表明越稀疏.
`λ=η/√[∑g²(k,i)+ε]`
`θ(t+1,i)=θ(t,i)-λg(t,i)`, 其中k∈[0,t], η是学习率, θ(t+1,i)是  (t+1)时刻参数向量θ(t+1)的第i个参数, g(k,i):  k时刻梯度向量g(k)的第i个维度.
学习速率λ中分母求和的形式实现了退火过程, 这是在很多优化过程中常见的策略. 随着时间推移, 保证算法的最终收敛.
问题是随着时间增加, 学习速率λ中分母单调递增, 对应的自适应学习速率的衰减速度过快.