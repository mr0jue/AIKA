梯度下降法（Gradient Descent Method），也叫作最速下降法（Steepest Descend Method），经常用来求解无约束优化的最小值问题．在机器学习中，最简单、常用的优化算法就是梯度下降法， 

在具体实现中为**批量梯度下降法**, 也称 #一阶法 , 梯度就是目标函数的一阶信息,
影响梯度下降法的主要因素有：批量大小(Batch Size)、[[学习率]]α(Learning Rate)、梯度估计方法.

在批量大小上的改进有: 随机梯度下降、小批量梯度下降
在学习率上的改进有: [[AdaGrad]]、[[RMSprop]]、[[AdaDelta]] 等．
在梯度估计方法上的改进有: [[Momentum]]方法、[[AdaGrad]]方法、[[Adam]]方法 等．

# 定义
对于函数𝑓(𝒙)，如果𝑓(𝒙)在点𝒙(𝑡) 附近是连续可微的，那么𝑓(𝒙)下降最快的方向是 𝑓(𝒙)在𝒙(𝑡) 点的梯度方向的反方向．
根据泰勒一阶展开公式，有
`𝑓(𝒙(𝑡+1)) = 𝑓(𝒙(𝑡) + Δ𝒙) ≈ 𝑓(𝒙(𝑡)) + Δ𝒙^T∇𝑓(𝒙(𝑡)). `
要使得 `𝑓(𝒙(𝑡+1)) < 𝑓(𝒙(𝑡))`，就得使 `Δ𝒙^T∇𝑓(𝒙(𝑡)) < 0`．我们取`Δ𝒙 = −𝛼∇𝑓(𝒙𝑡)`．
如果𝛼 > 0为一个够小数值时，那么 `𝑓(𝒙(𝑡+1)) < 𝑓(𝒙(𝑡))`成立．
这样我们就可以从一个初始值𝒙0 出发，通过迭代公式
`𝒙(𝑡+1) = 𝒙(𝑡) − 𝛼𝑡∇𝑓(𝒙(𝑡)), 𝑡 ≥ 0.`
生成序列 𝒙0,𝒙1,𝒙2,⋯ 使得`𝑓(𝒙0) ≥ 𝑓(𝒙1) ≥ 𝑓(𝒙2) ≥ ⋯` 
如果顺利的话，序列 (𝒙𝑛)收敛到局部最小解𝒙∗．
注意，每次迭代步长 𝛼 可以改变，但其取值必须合适，如果过大就不会收敛，如果过小则收敛速度太慢．

梯度下降法首先初始化参数𝜃_0，然后按下面的迭代公式来计算训练集𝒟 上风险函数的最小值： 
`𝜃𝑡+1 = 𝜃𝑡 − 𝛼(𝜕ℛ_𝒟(𝜃)/𝜕𝜃)= 𝜃𝑡 − 𝛼(1/𝑁)∑{𝑛=1;𝑁}𝜕ℒ(𝑦(𝑛), 𝑓(𝒙(𝑛); 𝜃))/𝜕𝜃` ,
其中𝜃𝑡 为第𝑡 次迭代时的参数值，𝛼为搜索步长．在机器学习中，𝛼一般称为学习率（Learning Rate）．

相反，如果我们要求解一个最大值问题，就需要向梯度正方向迭代进行搜索，逐渐接近函数的局部最大解，这个过程则被称为梯度上升法（Gradient Ascent Method）．

**批量梯度下降法** 使用有所得训练数据的平均损失来近似目标函数
对L(θt+δ)做一阶泰勒展开, `L(θt+δ)≈L(θt)+∇L(θt)Tδ`, 在δ较小时才比较准确, 因此一般加上L2正则项, `δ=arg min{ L(θt)+∇L(θt)ᵀδ+||δ||²/2α}``δ=-αL(θt)`
迭代公式`θ(t+1)=θ(t)-αL(θt)`, 其中 α为学习率
`L(θ)=[∑L(f(x,θ),y)]/M`,`∇L(θ)=[∑∇L(f(x,θ),y)]/M`, 其中 
M:样本数
目标函数:`L(θ)=[∑L(f(x,θ),y)]/m`
梯度:`∇L(θ)=[∑∇L(f(x,θ),y)]/m`


# 调优
为了更有效地训练深度神经网络，在标准的小批量梯度下降法的基础上，也经常使用一些改进方法以加快优化速度，比如如何选择批量大小、如何调整学习率以及如何修正梯度估计．这些改进的优化算法也同样可以应用在批量或随机梯度下降法上．

## 批量大小k的选择
在训练深度神经网络时， 训练数据的规模通常都比较大． 如果在梯度下降时， 每次迭代都要计算整个训练数据上的梯度， 每次更新模型参数时需要遍历所有训练数据．这就需要比较多的计算资源, 需要很大的计算量, 耗费很长计算时间,  另外大规模训练集中的数据通常会非常冗余， 也没有必要在整个训练集上计算梯度． 现实中基本不可行.这一问题可以通过小批量梯度下降法算法解决. 

小批量梯度下降法(Mini-Batch Gradient Descent), 用部分训练样本的损失来近似平均损失, 经常使用在训练深度神经网络时
随机梯度下降(Stochastic Gradient Descent，SGD), 用单个训练样本的损失来近似平均损失, 大大加快了收敛速率, 适用于在线场景.

在小批量梯度下降法中，批量大小（Batch Size）对网络优化的影响也非常大． 一般而言，批量大小不影响随机梯度的期望，但是会影响随机梯度的方差． 批量大小越大， 随机梯度的方差越小， 引入的噪声也越小， 训练也越稳定， 因此可以设置较大的学习率． 而批量大小较小时， 需要设置较小的学习率， 否则模型会不收敛． 

PS. 一般选取2的幂次时能充分利用高度优化的矩阵的运算操作。
PS. 遍历数据前先对所有数据进行随机排序， 避免数据的特定顺序对算法收敛的影响。
PS. 1 回合(Epoch) = (训练样本的数量N / 批量大小(Batch Size)K) × 迭代(Iteration).

## 学习率α的选取
常采用衰减学习率的方案, 加快收敛速率的同时提高求解精度. 具体的, 在算法开始时采用较大的学习率; 在误差曲线进入平台期, 减小学习率做更精细调整. 常用的手动学习率调整方法包括[[学习率衰减]]、[[学习率预热]]、[[周期性学习率调整]].
 在标准的梯度下降法中，每个参数在每次迭代时都使用相同的学习率． 由于每个参数的维度上收敛速度都不相同， 因此可以根据不同参数的收敛情况分别设置学习率.自适应学习率方法可以针对每个参数设置不同的学习率．比如 [[AdaGrad]]、[[RMSprop]]、[[AdaDelta]] 等．
 
 ### PS.学习率 与 批量大小
学习率通常要随着批量大小的增大而相应地增大． 一个简单有效的方法是线性缩放规则（Linear Scaling Rule）\[Goyal et al., 2017]：当批量大小增加m倍时，学习率也增加m倍． 
PS. 线性缩放规则往往在批量大小比较小时适用，当批量大小非常大时，线性缩放会使得训练不稳定．

## 梯度估计方法
由于每步接受信息有限, 对梯度估计常常出现偏差, 目标函数收敛得不稳定, 伴有剧烈波动甚至不收敛; 
优化过程中存在的陷阱, 除了局部最优点还有:“山谷”(山谷震荡)和“鞍点”(鞍点停滞).
使用带有动量的优化方法可以有效解决这个问题, 如[[Momentum]]、[[AdaGrad]]、[[Adam]]等．