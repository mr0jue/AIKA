在小批量梯度下降法中，当批量大小的设置比较大时，通常需要比较大的学习率． 但在刚开始训练时，由于参数是随机初始化的，梯度往往也比较大，再加上比较大的初始学习率，会使得训练不稳定．
为了提高训练稳定性， 我们可以在最初几轮迭代时， 采用比较小的学习率， 等梯度下降到一定程度后再恢复到初始的学习率， 这种方法称为学习率预热（Learning Rate Warmup）．
当预热过程结束，再选择一种学习率衰减方法来逐渐降低学习率．
一个常用的学习率预热方法是逐渐预热（Gradual Warmup）[Goyal et al., 2017]． 假设预热的迭代次数为 T′，初始学习率为 α0，在预热过程中，每次更新的学习率为`α′t = α0 × t/T′, 1≤t≤T′.`
