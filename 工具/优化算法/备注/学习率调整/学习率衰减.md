从经验上看， 学习率在一开始要保持大些来保证收敛速度， 在收敛到最优
点附近时要小些以避免来回振荡． 比较简单的学习率调整可以通过学习率衰减
（Learning Rate Decay）的方式来实现，也称为学习率退火（Learning Rate Annealing）．

#### 分段常数衰减（Piecewise Constant Decay）： 
即每经过 T1, T2, ⋯ ,Tm 次迭代将学习率衰减为原来的 β1, β2, ⋯ , βm 倍， 其中 Tm 和 βm < 1 为根据经验设置的超参数． 
分段常数衰减也称为阶梯衰减（Step Decay）．
#### 逆时衰减（Inverse Time Decay）：
`αt = α0 / (1 + β × t)`
其中 β 为衰减率．
#### 指数衰减（Exponential Decay）：
`αt = α0 × β^t`
其中 β < 1为衰减率．
#### 自然指数衰减（Natural Exponential Decay）：
`αt = α0 × exp(-β × t)`
其中 β 为衰减率．
#### 余弦衰减（Cosine Decay）：
`αt = α0 × (1 + cos(π × t/T)`
其中 T 为总的迭代次数．