为了使得梯度下降法能够逃离鞍点或尖锐最小值， 一种经验性的方式是在训练过程中周期性地增大学习率． 当参数处于尖锐最小值附近时，增大学习率有助于逃离尖锐最小值；当参数处于平坦最小值附近时，增大学习率依然有可能在该平坦最小值的*吸引域* （Basin of Attraction）内． 因此，周期性地增大学习率虽然可能短期内损害优化过程，使得网络收敛的稳定性变差，但从长期来看有助于找到更好的局部最优解.
两种常用的周期性调整学习率的方法： *循环学习率* 和*带热重启的随机梯度下降*．

一种简单的方法是使用循环学习率（Cyclic Learning Rate）[Goyal et al., 2017]，即让学习率在一个区间内周期性地增大和缩小． 通常可以使用线性缩放来调整学习率， 称为三角循环学习率（Triangular Cyclic Learning Rate）．
假设每个循环周期的长度相等都为 2Δ�， 其中前 Δ� 步为学习率线性增大阶段，
后 Δ� 步为学习率线性缩小阶段． 在第 � 次迭代时，其所在的循环周期数 �为

带热重启的随机梯度下降（Stochastic Gradient Descent with Warm Restarts，SGDR）[Loshchilov et al., 2017a] 是用热重启方式来替代学习率衰减的方法． 学习率每间隔一定周期后重新初始化为某个预先设定值， 然后逐渐衰减． 每次重启后模型参数不是从头开始优化， 而是从重启前的参数基础上继续优化．