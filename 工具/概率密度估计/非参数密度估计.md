
#非参数密度估计 （Nonparametric Density Estimation）是不假设数据服从某种分布，通过将样本空间划分为不同的区域并估计每个区域的概率来近似数据的概率密度函数

对于高维空间中的一个随机向量 ${\mathbf{x}}$，假设其服从一个末知分布 ${p(\mathbf{x})}$，则 ${\mathbf{x}}$ 落 入空间中的小区域 ${\mathcal{R}}$ 的概率为$${ P=\int_{\mathcal{R}} p(\mathbf{x}) d \mathbf{x} . }$$ 给定 ${N}$ 个训练样本 ${\mathcal{D}=\left\{\mathbf{x}^{(n)}\right\}_{n=1}^{N}}$，落入区域 ${\mathcal{R}}$ 的样本数量 ${K}$ 服从二项分布 $${ P_{K}=\left(\begin{array}{l} N \\ K \end{array}\right) P^{K}(1-P)^{1-K} }$$ 其中 ${K / N}$ 的期望为 ${\mathbb{E}[K / N]=P}$，方差为 ${\operatorname{var}(K / N)=P(1-P) / N}$ 。当 ${N}$ 非常大时，我们可以近似认为 ${ P \approx {K}/{N} }$。
假设区域 ${\mathcal{R}}$ 足够小，其内部的概率密度是相同的，则有 ${ P \approx p(\mathbf{x}) V }$ ，其中 ${V}$ 为区域 ${\mathcal{R}}$ 的体积。结合上述两个公式，得到 ${ p(\mathbf{x}) \approx {K}/{(N V)} . }$ 

根据公式 $p(\mathbf{x}) \approx {K}/{(N V)}$，要准确地估计 ${p(\mathbf{x})}$ 需要尽量使得样本数量 ${N}$ 足够大，区域体积 ${V}$ 尽可能地小。但在具体应用中，样本数量一般是有限的，过小的区域 会导致落入该区域的样本比较少，这样估计的概率密度就不太准确。因此，实 践中非参数密度估计通常使用两种方式:
1) 固定区域大小 ${V}$，统计落入不同区域的数量，这种方式包括 #直方图方法 和 #核方法 两种。
2) 改变区域大小 ${V}$，以使得落入每个区域的样本数量为 ${K}$，这种方式称为 #K近邻方法 。


# 直方图方法 
#直方图方法 （Histogram Method）是一种非常直观的估计连续变量密度函数的方法，可以表示为一种柱状图。
以一维随机变量为例, 首先将其取值范围分成 ${M}$ 个连续的、不重叠的区间 (bin), 每个区间的宽度为 ${\Delta_{m}}$ 。给定 ${N}$ 个训练样本 ${\mathcal{D}=\left\{x^{(n)}\right\}_{n=1}^{N}}$, 我们统计 这些样本落入每个区间的数量 ${K_{m}}$, 然后将它们归一化为密度函数。 $${ p_{m}=\frac{K_{m}}{N \Delta_{m}}, 1 \leq m \leq M }$$ 其中区间宽度 ${\Delta_{m}}$ 通常设为相同的值 ${\Delta}$ 。直方图方法的关键问题是如何选取一个合适的区间宽度 ${\Delta}$ 。如果 ${\Delta}$ 太小, 那么落入每个区间的样本数量会比较少, 其估计的区间密度也具有很大的随机性。如果 ${\Delta}$ 大大, 其估计的密度函数变得十分平滑, 很难反映出真实的数据分布。图给出了直方图密度估计的例子, 其中蓝线表示真实的密度函数, 红色的柱状图为直方图方法估计的密度。
![[直方图密度估计.png]]
直方图通常用来处理低维变量，可以非常快速地对数据的分布进行可视化，但其缺点是很难扩展到高维变量。假设一个$d$维的随机向量，如果每一维都划分为$M$个区间，那么整个空间的区间数量为$M^d$个。直方图方法需要的样本数量会随着维度$d$的增加而指数增长，从而导致 #维度灾难 问题。

# 核密度估计 
#核密度估计 (Kernel Density Estimation), 也叫 Parzen 窗方法, 是一种直方图方法的改进。 
假设 ${\mathcal{R}}$ 为 ${d}$ 维空间中的一个以点 ${\mathrm{x}}$ 为中心的 “超立方体”, 并定义核函数 $${ \phi\left(\frac{\mathbf{z}-\mathbf{x}}{h}\right)=\left\{\begin{array}{ll} 1 \text{ if }\left|z_{i}-x_{i}\right|<\frac{h}{2}, 1 \leq i \leq d \\ 0 \text{ else } \end{array}\right. }$$ 来表示一个样本 ${\mathbf{z}}$ 是否落入该超立方体中, 其中 ${h}$ 为超立方体的边长, 也称为核函数的宽度。 
给定 ${N}$ 个训练样本 ${\mathcal{D}=\left\{\mathbf{x}^{(n)}\right\}_{n=1}^{N}}$, 落入区域 ${\mathcal{R}}$ 的样本数量 ${K}$ 为 $${ K=\sum_{n=1}^{N} \phi\left(\frac{\mathbf{x}^{(n)}-\mathbf{x}}{h}\right) }$$ 则点 ${\mathrm{x}}$ 的密度估计为 $${ p(\mathbf{x})=\frac{K}{N h^{d}}=\frac{1}{N h^{d}} \sum_{n=1}^{N} \phi\left(\frac{\mathbf{x}^{(n)}-\mathbf{x}}{h}\right) }$$ 其中 ${h^{d}}$ 表示区域 ${\mathcal{R}}$ 的体积。 除了超立方体的核函数之外, 我们还可以选择更加平滑的核函数, 比如高斯核函数, $${ \phi\left(\frac{\mathbf{z}-\mathbf{x}}{h}\right)=\frac{1}{(2 \pi)^{1 / 2} h} \exp \left(-\frac{\|\mathbf{z}-\mathbf{x}\|^{2}}{2 h^{2}}\right) }$$ 其中 ${h^{2}}$ 可以看做是高斯核函数的方差。这样点 ${\mathbf{x}}$ 的密度估计为 $${ p(\mathbf{x})=\frac{1}{N} \sum_{n=1}^{N} \frac{1}{(2 \pi)^{1 / 2} h} \exp \left(-\frac{\|\mathbf{z}-\mathbf{x}\|^{2}}{2 h^{2}}\right) }$$

－K近邻方法（K-Nearest Neighbor Method）
核密度估计方法中的核宽度是固定的，因此同一个宽度可能对高密度的区域过大，而对低密度的区域过小． 一种更灵活的方式是设置一种可变宽度的区域，并使得落入每个区域中样本数量为固定的K． 要估计点x的密度，首先找到一个以x为中心的球体，使得落入球体的样本数量为K，然后根据公式(9.41)，就可以计算出点x的密度． 因为落入球体的样本也是离x最近的 K个样本，所以这种方法称为

# K近邻方法

核密度估计方法中的核宽度是固定的，因此同一个宽度可能对高密度的区域过大，而对低密度区域过小。一种更灵活的方式是设置一种可变宽度的区域，并使得落入每个区域中样本数量为固定的K。要估计点x的密度，首先找到一个以x为中心的球体，使得落入球体的样本数量为K，然后根据公式 $p(\mathbf{x}) \approx {K}/{(N V)}$，就可以计算出点x的密度。因为落入球体的样本也是离x最近的K 个样本，所以这种方法称为 #K近邻方法 （K-Nearest Neighbor）。

*K 近邻方法并不是一个严格的密度函数估计方法：K近邻方法估计的密度函数不是严格的概率密度函数，其在整个空间上的积分不等于1。*

在K近邻方法中，K 的选择也十分关键。如果K 太小，无法有效地估计密度函数，而K 太大也会使得局部的密度不准确，并且增加计算开销。
K近邻方法也经常用于分类问题，称为K近邻分类器。当K = 1也称为最近邻分类器。最近邻分类器的一个性质是，当N → ∞时，其分类错误率不超过最优分类器错误率的两倍[Cover and Hart, 1967]。
