神经网络的发展大致经过五个阶段。
## 第一阶段：模型提出
第一阶段：模型提出 第一阶段为1943年～1969年，是神经网络发展的第一个高潮期．在此期间，科学家们提出了许多神经元模型和学习规则．
1943 年，心理学家 Warren McCulloch 和数学家 Walter Pitts 最早提出了一种基于简单逻辑运算的人工神经网络，这种神经网络模型称为MP 模型，至此开启了人工神经网络研究的序幕．
1948 年，Alan Turing 提出了一种“B 型图灵机”．“B 型图灵机”可以基于Hebbian 法则来进行学习．
1951 年，McCulloch 和 Pitts 的学生 Marvin Minsky 建造了第一台神经网络机 SNARC．  
[Rosenblatt, 1958]提出了一种可以模拟人类感知能力的神经网络模型，称为感知器（Perceptron），并提出了一种接近于人类学习过程（迭代、试错）的学习算法．
在这一时期，神经网络以其独特的结构和处理信息的方法，在许多实际应用领域（自动控制、模式识别等）中取得了显著的成效．
## 第二阶段：冰河期
第二阶段为1969年～1983年，是神经网络发展的第一个低谷期．在此期间，神经网络的研究处于长年停滞及低潮状态．
1969 年，Marvin Minsky 出版《感知器》一书，指出了神经网络的两个关键缺陷：一是感知器无法处理“异或”回路问题；二是当时的计算机无法支持处理大型神经网络所需要的计算能力．这些论断使得人们对以感知器为代表的神经网络产生质疑，并导致神经网络的研究进入了十多年的“冰河期”．
但在这一时期，依然有不少学者提出了很多有用的模型或算法．1974 年，哈佛大学的 Paul Werbos 发明反向传播算法（BackPropagation，BP）[Werbos, 1974]，但当时未受到应有的重视．1980年，福岛邦彦提出了一种带卷积和子采样操作的多层神经网络：新知机（Neocognitron）[Fukushima, 1980]．新知机的提出是受到了动物初级视皮层简单细胞和复杂细胞的感受野的启发．但新知机并没有采用反向传播算法，而是采用了无监督学习的方式来训练，因此也没有引起足够的重视．
## 第三阶段：反向传播算法引起的复兴
第三阶段为1983年～1995年，是神经网络发展的第二个高潮期．这个时期中，反向传播算法重新激发了人们对神经网络的兴趣．
1983 年，物理学家 John Hopfield 提出了一种用于*联想记忆*（Associative Memory）的神经网络，称为*Hopfield 网络*．Hopfield 网络在旅行商问题上取得了当时最好结果，并引起了轰动． 
1984年，Geoffrey Hinton提出一种随机化版本的Hopfield网络，即*玻尔兹曼机*（Boltzmann Machine）．
真正引起神经网络第二次研究高潮的是反向传播算法．20 世纪 80 年代中期，一种连接主义模型开始流行，即*分布式并行处理*（Parallel Distributed Processing，PDP）模型[McClelland et al., 1986]．反向传播算法也逐渐成为PDP模型的主要学习算法．这时，神经网络才又开始引起人们的注意，并重新成为新的研究热点．
随后，[LeCun et al., 1989]将反向传播算法引入了卷积神经网络，并在手写体数字识别上取得了很大的成功[LeCun et al., 1998]．反向传播算法是迄今最为成功的神经网络学习算法．目前在深度学习中主要使用的*自动微分*可以看作反向传播算法的一种扩展．
然而，梯度消失问题（Vanishing Gradient Problem）阻碍神经网络的进一步发展，特别是循环神经网络．为了解决这个问题，[Schmidhuber, 1992]采用两步来训练一个多层的循环神经网络：1）通过无监督学习的方式来逐层训练每一层循环神经网络，即预测下一个输入；2）通过反向传播算法进行精调．
## 第四阶段：流行度降低
第四阶段为 1995 年～2006 年，在此期间，支持向量机和其他更简单的方法（例如线性分类器）在机器学习领域的流行度逐渐超过了神经网络．
虽然神经网络可以很容易地增加层数、神经元数量，从而构建复杂的网络，但其计算复杂性也会随之增长．当时的计算机性能和数据规模不足以支持训练大规模神经网络．在 20 世纪 90 年代中期，统计学习理论和以支持向量机为代表的机器学习模型开始兴起．相比之下，神经网络的理论基础不清晰、优化困难、可解释性差等缺点更加凸显，因此神经网络的研究又一次陷入低潮
## 第五阶段：深度学习的崛起
第五阶段为从 2006 年开始至今，在这一时期研究者逐渐掌握了训练深层神经网络的方法，使得神经网络重新崛起． [Hinton et al., 2006] 通过逐层预训练来学习一个深度信念网络，并将其权重作为一个多层前馈神经网络的初始化权重，再用反向传播算法进行精调．这 种“预训练 + 精调”的方式可以有效地解决深度神经网络难以训练的问题．随着深度神经网络在语音识别[Hinton et al., 2012]和图像分类[Krizhevsky et al., 2012]等任务上的巨大成功，以神经网络为基础的深度学习迅速崛起．近年来，随着大规模并行计算以及 GPU 设备的普及，计算机的计算能力得以大幅提高．此外，可供机器学习的数据规模也越来越大．在强大的计算能力和海量的数据规模支持下，计算机已经可以端到端地训练一个大规模神经网络，不再需要借助预训练的方式．各大科技公司都投入巨资研究深度学习，神经网络迎来第三次高潮．