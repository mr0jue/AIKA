# 概述
简单地讲，人工智能（Artificial Intelligence，AI）就是让机器具有人类的 #智能 ，这也是人们长期追求的目标．
人工智能是计算机科学的一个分支，主要研究、开发用于模拟、延伸和扩展人类智能的理论、方法、技术及应用系统等．和很多其他学科不同，人工智能这个学科的诞生有着明确的标志性事件，就是 1956 年的达特茅斯（Dartmouth）会 议．在这次会议上，“人工智能”被提出并作为本研究领域的名称．同时，人工智能研究的使命也得以确定．John McCarthy提出了人工智能的定义：人工智能就是要让机器的行为看起来就像是人所表现出的智能行为一样．

## 研究范畴
要实现人工智能的定义，使得计算机能通过 #图灵测试 ，计算机就必须具备理解语言、学习、记忆、推理、决策等能力．这样，人工智能就延伸出了很多不同的子学科，比如机器感知（ #计算机视觉 、 #语音信息处理 ）、学习（ #模式识别 、 #机器学习 、 #强化学习 ）、语言（ #自然语言处理 ）、记忆（知识表示）、决策（规划、 #数据挖掘 ）等．所有这些研究领域都可以看成是人工智能的研究范畴．
目前，人工智能的主要领域大体上可以分为以下几个方面：
1） 感知：模拟人的感知能力，对外部刺激信息（视觉和语音等）进行感知和加工．主要研究领域包括语音信息处理和计算机视觉等．
2） 学习：模拟人的学习能力，主要研究如何从样例或从与环境的交互中进行学习．主要研究领域包括 #监督学习 、 #无监督学习 和 #强化学习 等．
3） 认知：模拟人的认知能力，主要研究领域包括知识表示、自然语言理解、推理、规划、决策等．

[[人工智能流派]]
[[人工智能发展历史]]

# 人工智能算法
## 基础
数学基础：微积分/线性代数/概率论/数学优化/信息论
计算机基础：数据结构
## 理论
[[机器学习]]／[[深度学习]]／[[表示学习]]／[[强化学习]]
## 一般应用流程
1. 抽象成数学问题
2. 获取数据
3. 特征预处理与特征选择
4. 训练模型与调优
5. 模型诊断
6. 模型融合
7. 上线运行

*人工智能 这里关于什么是“智能”并没有一个很明确的定义，但一般认为智能（或特指人类智能）是知识和智力的总和，都和大脑的思维活动有关．人类大脑是经过了上亿年的进化才形成了如此复杂的结构，但我们至今仍然没有完全了解其工作机理．虽然随着神经科学、认知心理学等学科的发展，人们对大脑的结构有了一定程度的了解，但对大脑的智能究竟是怎么产生的还知道得很少．我们并不理解大脑的运作原理，以及如何产生意识、情感、记忆等功能．因此，通过“复制”人脑来实现人工智能在目前阶段是不切实际的．*

# 术语
### 完备性
如果p个基向量刚好可以支撑p维的欧氏空间，则这p个基向量是 #完备 的 。如果p个基向量可以支撑d维的欧氏空间，并且p > d，则这p个基向量是 #过完备 的，冗余的。
“ 过完备”基向量一般是指的是基向量个数远远大于其支撑空间维度。因此这些基向量一般是不具备独立、正交等性质。

---
### 稀疏性
对于一个向量, 其 #稀疏性 定义为非零元素的比例。如果一个向量只有很少的几个非零元素, 就说这个向量是稀疏的。严格的稀疏向量有时比较难以得到，因此如果一个向量只有少数几个远大于零的元素，其它元素都接近于0，我们也称这个向量为稀疏向量。

---
### 稀疏性衡量函数
对于一个向量 ${\mathbf{z} \in \mathbb{R}^{p}}$, #稀疏性衡量函数  ${\rho(\mathbf{z})}$ 是给向量 ${\mathbf{z}}$ 一个标量分数。 ${\mathbf{z}}$ 越稀疏, ${\rho(\mathbf{z})}$ 越小。 
稀疏性衡量函数有多种选择, 最直接的衡量向量 ${\mathrm{z}}$ 稀疏性的函数是
	${\ell_{0}}$ 范式       ${ \left.\rho(\mathbf{z})=\sum_{i=1}^{p} \mathbf{I}\left(\left|z_{i}\right|>0\right)\right) }$ 
但 ${\ell_{0}}$ 范数不满足连续可导, 因此很难进行优化。在实际中, 稀疏性衡量函数通常使用
	${\ell_{1}}$ 范数       ${ \rho(\mathbf{z})=\sum_{i=1}^{p}\left|z_{i}\right| }$ 
	对数函数    ${ \rho(\mathbf{z})=\sum_{i=1}^{p} \log \left(1+z_{i}^{2}\right) }$ 
	指数函数    ${ \rho(\mathbf{z})=\sum_{i=1}^{p}-\exp \left(-z_{i}^{2}\right) }$ 


---
### 饱和
#饱和 ：对于函数 ${f(x)}$, 若 ${x \rightarrow-\infty}$ 时, 其导数 ${f{\prime}(x) \rightarrow 0}$, 则称其为左饱和。若 ${x \rightarrow+\infty}$ 时, 其导数 ${f{\prime}(x) \rightarrow 0}$, 则称其为右饱和。当同时满足 左、右饱和时, 就称为两端饱和。

---
### 连续属性、离散属性、有序属性、无序属性
我们常将属性划分为“ #连续属性”（continuous attribute)和“ #离散属性”（categorical attribute),前者在定义域上有无穷多个可能的取值，后者在定义域上是有限个取值. “连续属性” 亦称“数值属性” (numerical attribute), “离散属性” 亦称 “列名属性” (nominal attribute).

例如定义域为{1,2,3}的离散属性与连续属性的性质更接近一些,能直接在属性值上计算距离：“1”与“2”比较接近、与“3”比较远，这样的属性称为“ #有序属性”(ordinalattribute);而定义域为{飞机，火车，轮船}这样的离散属性则不能直接在属性值上计算距离，称为“ #无序属性”（non-ordinalattribute)

---
### 维度灾难
#维度灾难 （Curse of Dimensionality）

---
### 过度参数
#过度参数 (Over-Parameterization）是指模型参数的数量远远大于训练数据的数量。

---
### 常见的向量
#全0向量 ：指所有元素都为0的向量，用0表示。全0向量为笛卡尔坐标系中的原点。
#全1向量 ：指所有值为1的向量，用1表示。
#one-hot 向量 为有且只有一个元素为1，其余元素都为0的向量。one-hot向量是在数字电路中的一种状态编码，指对任意给定的状态，状态寄存器中只有l位为1，其余位都为0。

---
### 局部表示、分布式表示 和 嵌入
 以颜色表示为例，我们一般有两种表示方法。
我们有很多词来形容颜色的词，除了基本的“红”、“蓝”、“绿”、“白”、“黑”等之外，有很多以地区或物品命名的，比如“中国红”、“天蓝色”、“咖啡色”、“琥珀色”等等。一种表示颜色的方式是以不同名字来命名不同的颜色，这种表示方式叫做 #局部表示 ，也称为离散表示或符号表示。局部表示通常可以表示为 #one-hot 向量的形式。假设所有颜色的名字构成一个词表V，词表大小为|V|。我们可以用一个|V| 维的one-hot 向量来表示每一种颜色。第i种颜色的one-hot 向量中，第i维的值为1，其它都为0。
局部表示有两个不足之处：
1）one-hot向量的维数很高，且不能扩展。如果有一种新的颜色，我们就需要增加一维来表示；
2）不同颜色之间的相似度都为0，即我们无法知道“红色”和“中国红”的相似度要比“红色”和“黑色”的相似度要高。
另一种表示颜色的方式是用RGB值来表示颜色，不同颜色对应到R、G、B三维空间中一个点，这种表示方式叫做分布式表示。分布式表示通常可以表示为低维的稠密向量。
相比与局部表示，分布式表示的表示能力要比局部表示强很多，分布式表示的向量维度一般都比较低。我们只需要用一个三维的稠密向量就可以表示所有颜色。并且分布式表示也很容易表示新的颜色名。此外，不同颜色之间的相似度也很容易计算。
我们可以使用神经网络来将高维的局部表示空间 ${\mathbb{R}^{|\mathcal{V}|}}$ 映射到一个非常低维的分布式表示空间 ${\mathbb{R}^{d}, d \ll|\mathcal{V}|}$ 。在这个低维空间中, 每个特征不在是坐标轴上的点，而是分散在整个低维空间中。在机器学习中，这个过程也称为 #嵌入 （ #embedding ）。嵌入通常指将一个度量空间中的一些对象映射到另一个低维的度量空间中，并尽可能保持不同对象之间的拓扑关系。比如自然语言中词的分布式表示，也经常叫做词嵌入。
图展示了一个3维one-hot向量空间和一个2维嵌入空间的对比。在one-hot向量空间中，每个特征都位于坐标轴上，每个坐标轴上一个特征。而在低维的嵌入空间中，每个特征都不在坐标轴上，特征之间可以计算相似度。
![[One-hot向量空间与嵌入空间.png|300]]


---
### 动力系统
#动力系统 （Dynamical System）是一个数学上的概念，指系统状态按照一定的规律随时间变化的系统。具体地讲，动力系统是使用一个函数来描述一个给定空间（如某个物理系统的状态空间）中所有点随时间的变化情况。

---
### 通用近似定理
#通用近似定理 (Universal Approximation Theorem)  [Cybenko, 1989, Hornik et al., 1989]: 
令  $\varphi(\cdot)$  是一个非常数、有界、单调递增的连续函数,  $\mathcal{I}_{d}$  是一个  $d$  维的单位超立方体  $[0,1]^{d}$ ,  $C\left(\mathcal{I}_{d}\right)$  是定义在 $\mathcal{I}_{d}$  上的连续函数集合。对于任何一个函数  $f \in   C\left(\mathcal{I}_{d}\right)$ , 存在一个整数  m , 和一组实数  $v_{i}, b_{i} \in \mathbb{R}$  以及实数向量  $\mathbf{w}_{i} \in \mathbb{R}^{d}, i=1, \cdots, m$, 以至于我们可以定义函数$$F(\mathbf{x})=\sum_{i=1}^{m} v_{i} \varphi\left(\mathbf{w}_{i}^{\mathrm{T}} \mathbf{x}+b_{i}\right)$$作为函数  $f$  的近似实现, 即$$|F(\mathbf{x})-f(\mathbf{x})|<\epsilon, \forall \mathbf{x} \in \mathcal{I}_{d}$$
其中 $\epsilon>0$  是一个很小的正数。
通用近似定理在实数空间 $\mathbb{R}^{d}$ 中的有界闭集上依然成立。

---
### Borel可测函数
定义在实数空间 $\mathbb{R}^{d}$ 中的有界闭集上的任意连续函数，也称为 #Borel可测函数 。

---
### 过拟合/欠拟合
根据大数定理可知，当训练集大小$|D|$趋向于无穷大时， #经验风险 就趋向于 #期望风险 。然而通常情况下，我们无法获取无限的训练样本，并且训练样本往往是真实数据的一个很小的子集或者包含一定的噪声数据，不能很好地反映全部数据的真实分布。 #经验风险最小化 准则很容易导致模型在训练集上错误率很低，但是在未知数据上错误率很高。这就是所谓的 #过拟合 （ #Overfifitting ）。
**定义**： 给定一个假设空间$F$，一个假设$f$ 属于$F$，如果存在其他的假设 $f′$ 也属于 $F$, 使得在训练集上 $f$ 的损失比 $f′$ 小，但在整个样本空间上$f′$ 比$f$ 的损失小，那么就说假设$f$过度拟合训练数据[Mitchell, 1997]。

和过拟合相反的一个概念是 #欠拟合 （underfifitting），即模型不能很好地拟合训练数据，在训练集的错误率比较高。欠拟合一般是由于模型能力不足造成的。 

图给出了欠拟合和过拟合的示例。
![[欠拟合和过拟合示例.png]]

---
### Frobenius范数
#Frobenius范数： $${ \|W\|_{F}^{2}=\sum_{l=1}^{L} \sum_{i=1}^{m^{(l)}} \sum_{j=1}^{m^{(l-1)}}\left(W_{i j}^{(l)}\right)^{2} . }$$

---
### 人类大脑
随着神经科学、认知科学的发展，我们逐渐知道人类的智能行为都和大脑活动有关。 #人类大脑 是一个可以产生意识、思想和情感的器官．人类大脑是人体最复杂的器官，由神经元、神经胶质细胞、神经干细胞和血管组成．其中，神经元（Neuron），也叫神经细胞（Nerve Cell），是携带和传输信息的细胞，是人脑神经系统中最基本的单元． 人脑神经系统是一个非常复杂的组织，包含近860亿个神经元 [Azevedo et al., 2009]，每个神经元有上千个突触和其他神经元相连接．这些神经元和它们之间的连接形成巨大的复杂网络，其中神经连接的总长度可达数千公里．

---
### 生物神经元
生物学家在20世纪初就发现了生物神经元的结构。一个 #生物神经元 通常具有多个树突和一条轴突。树突用来接受信息，轴突用来发送信息。当神经元所获得的输入信号的积累超过某个阈值时，它就处于兴奋状态，产生电脉冲。轴突尾端有许多末梢可以给其他个神经元的树突产生连接（突触），并将电脉冲信号传递给其它神经元。

---
### MP神经元
1943年，心理学家McCulloch和数学家Pitts根据生物神经元的结构，提出了一种非常简单的神经元模型， #MP神经元[McCulloch and Pitts, 1943]。现代神经网络中的神经元和M-P神经元的结构并无太多变化。不同的是，MP神经元中的激活函数$f$为0或1的阶跃函数，而现代神经元中的激活函数通常要求是连续可导的函数。

---
### 人工智能低谷
#人工智能低谷 也叫人工智能冬天（AI Winter），指人工智能史上研究资金及学术界研究兴趣都大幅减少的时期．人工智能领域经历过好几次低谷期．每次狂热高潮之后，紧接着是失望、批评以及研究资金断绝，然后在几十年后又重燃研究兴趣．1974～1980 年及1987～1993年是两个主要的低谷时期，其他还有几个较小的低谷.

---
### 学习
对于某类任务 T 和性能度量 P ，一个计算机程序被认为可以从经验 E 中 #学习 ，是指计算机程序通过经验 E 改进后，在任务 T 上由性能度量 P 衡量的性能有所提升。

---
### 智能
#智能 （Intelligence）是现代生活中很常见的一个词，比如智能手机、智能家居、智能驾驶等。在不同使用场合中，智能的含义也不太一样。比如“智能手机”中 的“智能”一般是指由计算机控制并具有某种智能行为。这里的“计算机控制”+ “智能行为”隐含了对人工智能的简单定义。“智能”可以理解为“智力”和“能力”。前者是智能的基础，后者是指获取和运用知识求解的能力。

---
### 模式识别
#模式识别 （Pattern Recognition，PR）更偏向于具体的应用任务，比如光学字符识别、语音识别、人脸识别等． 这些任务的特点是，对于我们人类而言，这些任务很容易完成，但我们不知道自己是如何做到的，因此也很难人工设计一个计算机程序来完成这些任务．一个可行的方法是设计一个算法可以让计算机自己从有标注的样本上学习其中的规律， 并用来完成各种识别任务． 随着机器学习技术的应用越来越广， 现在机器学习的概念逐渐替代模式识别，成为这一类问题及其解决方法的统称．

---
### 图灵机
#图灵机 （Turing Machine）是图灵在1936年提出的一种抽象数学模型，可以用来模拟任何可计算问题[Turing, 1937]。图灵机有以下几个组件构成：
1. 一条无限长的纸带：纸带上有一个个方格组成，每个方格可以存储一个符号；
2. 一个符号表：纸带上可能出现的所有符号的集合，包含一个特殊的空白符。
3. 一个读写头：指向纸带上某个方格的指针，每次可以向左或右移动一个位置，并可以读取、擦除、写入当前方格中的内容；
4. 一个状态寄存器：用来保存图灵机当前所处的状态，其中包含两个特殊的状态：起始状态和终止状态；
5. 一套控制规则：根据当前机器所处的状态以及当前读写头所指的方格上的符号来确定读写头下一步的动作，令机器进入一个新的状态。

图灵机的结构如图所示，其中控制器包括状态寄存器、控制规则。
![[图灵机结构示例.png|300]]

---
### 图灵测试
1950 年，阿兰·图灵（Alan Turing）发表了一篇有着重要影响力的论文《Computing Machinery and Intelligence》，讨论了创造一种“智能机器”的可能性．由于“ #智能 ”一词比较难以定义，他提出了著名的 #图灵测试 ：“一个人在不接触对方的情况下，通过一种特殊的方式和对方进行一系列的问答．如果在相当长时间内，他无法根据这些问题判断对方是人还是计算机，那么就可以认为这个计算机是智能的”．图灵测试是促使人工智能从哲学探讨到科学研究的一个重要因素，引导了人工智能的很多研究方向．

---
### 图灵完备
#图灵完备 （Turing Completeness）是指一种数据操作规则，比如一种计算机编程语言，可以实现 #图灵机 （Turing Machine）的所有功能，解决所有的可计算问题。目前主流的编程语言（比如C++，Java，Python等）都是图灵完备的。

---
### 语义鸿沟
#语义鸿沟 （Semantic Gap）问题是指输入数据的底层特征和高层语义信息之间的不一致性和差异性。比如给定一些关于“车”的图片，由于图片中每辆车的颜色和形状等属性都不尽相同，不同图片在像素级别上的表示（即底层特征）差异性也会非常大。但是我们人理解这些图片是建立在比较抽象的高层语义概念上的。

---
### 手写体数字识别
![[手写体数字识别示例.png|500]]
（图片来源[LeCun et al., 1998]）
#手写数字识别 是一个经典的机器学习任务，对人来说很简单，但对计算机来说却十分困难。我们很难总结每个数字的手写体特征，或者区分不同数字的规则，因此设计一套识别算法几乎是一项几乎不可能的任务。在现实生活中，很多问题都类似于手写体数字识别这类问题，比如物体识别、语音识别等。对于这类问题，我们不知道如何设计一个计算机程序来解决，即使可以通过一些启发式规则来实现，其过程也是极其复杂的。因此，人们开始尝试采用另一种思路，即让计算机“看”大量的样本，并从中学习到一些经验，然后用这些经验来识别新的样本。要识别手写体数字，首先通过人工标注大量的手写体数字图像（即每张图像都通过人工标记了它是什么数字），这些图像作为训练数据，然后通过学习算法自动生成一套模型，并依靠它来识别新的手写体数字。这和人类学习过程也比较类似，我们教小孩子识别数字也是这样的过程。这种通过数据来学习的方法就称为机器学习的方法。

---
### 感受野 
 #感受野 （Receptive Field）主要是指听觉、视觉等神经系统中一些神经元的特性，即神经元只接受其所支配的刺激区域内的信号。在视觉神经系统中，视觉皮层中的神经细胞的输出依赖于视网膜上的光感受器。视网膜上的光感受器受刺激兴奋时，将神经冲动信号传到视觉皮层，但不是所有视觉皮层中的神经元都会接受这些信号。一个神经元的感受野是指视网膜上的特定区域，只有这个区域内的刺激才能够激活该神经元。
 
---
### 尺度不变特征变换
#尺度不变特征变换 （Scale Invariant Feature Transform， #SIFT ）

---
### 带通性
#带通性 是指不同尺度下空间结构的敏感性．比如， #带通滤波器 （Bandpass Filter）是指容许某个频率范围的信号通过，同时屏蔽其他频段的设备．