# #风险最小化准则
一个好的模型  $f(\mathbf{x}, \theta)$  应当有一个比较小的期望错误, 但由于不知道真实的数据分布和映射函数, 实际上无法计算 #期望风险  $\mathcal{R}(\theta ; \mathbf{x}, y)$ 。给定一个训练集  $\mathcal{D}=\left\{\left(\mathbf{x}^{(n)}, y^{(n)}\right)\right\}_{n=1}^{N}$ , 我们可以计算的是 #经验风险 (Empirical Risk) , 即在训练集上的平均损失。$$\mathcal{R}_{\mathcal{D}}^{e m p}(\theta)=\frac{1}{N} \sum_{n=1}^{N} \mathcal{L}\left(y^{(n)}, f\left(x^{(n)}, \theta\right)\right)$$因此, 一个切实可行的学习准则是找到一组参数  $\theta^{*}$  使得经验风险最小,$$\theta^{*}=\underset{\theta}{\arg \min } \mathcal{R}_{\mathcal{D}}^{e m p}(\theta),$$这就是 #经验风险最小化 （Empirical Risk Minimization, ERM）准则。

经验风险最小化原则很容易导致模型在训练集上错误率很低，但是在未知数据上错误率很高。这就是所谓的 #过拟合 。过拟合问题往往是由于训练数据少和噪声以及模型能力强等原因造成的。为了解决过拟合问题，一般在经验风险最小化的基础上再引入参数的 [[正则化]] ，来限制模型能力，使其不要过度地最小化经验风险。这种准则就是 #结构风险最小化 （Structure Risk Minimization，SRM）准则。
$$\begin{aligned}
\theta^{*} &=\underset{\theta}{\arg \min } \mathcal{R}_{\mathcal{D}}^{\text {struct }}(\theta) \\
&=\underset{\theta}{\arg \min } \mathcal{R}_{\mathcal{D}}^{e m p}(\theta)+\frac{1}{2} \lambda\|\theta\|^{2} \\
&=\underset{\theta}{\arg \min } \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}\left(y^{(n)}, f\left(x^{(n)}, \theta\right)\right)+\frac{1}{2} \lambda\|\theta\|^{2},
\end{aligned}$$
其中  $\|\theta\|$ 是 $\ell_{2}$ 范数的正则化项, 用来减少参数空间, 避免过拟合;  $\lambda$用来控制正则化的强度。正则化项也可以使用其它函数，比如ℓ1 范数。
在贝叶斯学习的角度来讲，正则化是假设了参数的先验分布( #增加先验)，不完全依赖训练数据。

总之，机器学习中的学习准则并不仅仅是拟合训练集上的数据，同时也要使得泛化错误最低。给定一个训练集，机器学习的目标是从假设空间中找到一个泛化错误较低的“理想”模型，以便更好地对未知的样本进行预测，特别是不在训练集中出现的样本。因此，机器学习可以看作是一个从有限、高维、有噪声的数据上得到更一般性规律的泛化问题。
